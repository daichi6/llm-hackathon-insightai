{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sNiKUkfkZwPG"
   },
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community\n",
    "# !pip install sentence-transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install --upgrade langchain\n",
    "# !pip install fitz\n",
    "# !pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s1lfUClCZhuo"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZoheG32fx0o"
   },
   "source": [
    "### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "cmPA1v6bfgTg"
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# Load Sample PDF\n",
    "pdf_path = \"attention.pdf\"\n",
    "\n",
    "text = extract_text(pdf_path)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "LTbfdWp9II1P"
   },
   "outputs": [],
   "source": [
    "# text to document\n",
    "doc = Document(page_content=text)\n",
    "docs = [doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TunwL3riOeQt"
   },
   "source": [
    "### Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "I2Uytk6VN2Ya"
   },
   "outputs": [],
   "source": [
    "def split_documents_into_chunks(docs, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Splits the given documents into chunks of specified size with overlap.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of documents to split.\n",
    "        chunk_size (int): Size of each chunk. Default is 500 characters.\n",
    "        chunk_overlap (int): Overlap size between chunks. Default is 100 characters.\n",
    "\n",
    "    Returns:\n",
    "        list: List of split documents with chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "6mdMWt5-N7Yx"
   },
   "outputs": [],
   "source": [
    "def add_chunk_numbers_to_metadata(doc_splits):\n",
    "    \"\"\"\n",
    "    Adds chunk numbers to the metadata of each split document.\n",
    "\n",
    "    Args:\n",
    "        doc_splits (list): List of split documents.\n",
    "\n",
    "    Returns:\n",
    "        list: List of split documents with updated metadata.\n",
    "    \"\"\"\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"chunk\"] = idx\n",
    "    return doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "clHnXEjoOFms"
   },
   "outputs": [],
   "source": [
    "# Split the documents into chunks\n",
    "doc_splits = split_documents_into_chunks(docs)\n",
    "# Add chunk number to metadata\n",
    "doc_splits = add_chunk_numbers_to_metadata(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjNeMyT5P3VQ",
    "outputId": "a523b13c-e640-4239-87b1-ad46bb57cf3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 102 splits\n"
     ]
    }
   ],
   "source": [
    "print(f'Created {len(doc_splits):,} splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_W_6851dM5M",
    "outputId": "a0eee6a5-d1ae-4850-e3e6-ee728cb91e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Page Content---\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "Metadata:\n",
      "{'chunk': 0}\n",
      "\n",
      "---Page Content---\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "Metadata:\n",
      "{'chunk': 1}\n",
      "\n",
      "---Page Content---\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "Metadata:\n",
      "{'chunk': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in doc_splits[:3]:\n",
    "    print(f'---Page Content---\\n{split.page_content}')\n",
    "    print(f'Metadata:\\n{split.metadata}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ila_HXSxdOIb"
   },
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358,
     "referenced_widgets": [
      "6eb13d45b34149c087d691ecf023bdb6",
      "8425e17b886644faaac946195a975a52",
      "5235c7fde8d14c15bfe40ba2c6ad3df7",
      "634329a3ba5749bc92e28cc9349f96ec",
      "48f7b6925b5845eaa338cdcfabffa533",
      "8f3cd45a0f494741925f070cf7982dbf",
      "c6fc96efd3f843c4a248082278d85f60",
      "302aa94d588448cdaf93d0c8e3c83d7b",
      "f045e69e1a224341a90490a3e33c32ac",
      "b8cdee6b124a4f3dbafe4853530b8070",
      "e8c3e430fb364ff78edcc0b7bbb37827",
      "cebe7d8b9b80498291aa9aaa47b76bbd",
      "8b4e60d3937e4303a83339d47bb23ec7",
      "2cf46ed90b534dfb84c1a0a86cd37cea",
      "f7892a4e8e1b4440ad82f675a2fd9db3",
      "ec80c509488540f090236b45d37ff218",
      "5a953f60bba448f092a145757b0be464",
      "4ef614a8f174408cba72974ddb0cfa99",
      "f128bc400dbb4c5293aa007b948ac957",
      "5d20e0b51b7c4ac8af29ca296e2554f0",
      "09bd83a20a6e4c6881da65c8c82b37b0",
      "0cc674826142494bbf9205ac0583f14b",
      "2879e77e3d82444db0ba7e3187931ac9",
      "95df531bcf854fb0b80b3d50d31adde3",
      "cc7b7a9af09446efb91aa9e590150e8c",
      "9a1a23324c4e48e5bad4d04c5e6aedf5",
      "f6fd82678d3f4a8191418b5681a3ffe0",
      "99446ad02b86405e9cde439de840e505",
      "5fc77181c38943ffb208eed889d9d52e",
      "66c93a3adbfa46e2a5a73baea6e1bf42",
      "253414e97e99406db7f58a556539eafb",
      "c1d90486e6744eedb1921ad2d1619e21",
      "8118e0ddd091477bba719ad41b886dfe"
     ]
    },
    "id": "i9IFXIy2ZsPT",
    "outputId": "920b5b00-5288-48e8-ea30-a86236680759"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb13d45b34149c087d691ecf023bdb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebe7d8b9b80498291aa9aaa47b76bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2879e77e3d82444db0ba7e3187931ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embedding model - Stronger model can be considered\n",
    "\n",
    "# SciBERT(Allen Institute for AI) - for academic(science) paper including computer science - maximum 512 tokens\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # small model(microsoft)\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"roberta-large\") # RoBERTa - large (facebook)　- Longer context\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"roberta-base\") # RoBERTa - base(facebook)　- Longer context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwVMT3Tdd2-m"
   },
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "sFILjzeod616"
   },
   "outputs": [],
   "source": [
    "def configure_faiss_vector_store(doc_splits, embeddings):\n",
    "    \"\"\"\n",
    "    Configures FAISS as the vector store using the provided document splits and embeddings.\n",
    "\n",
    "    Args:\n",
    "        doc_splits (list): List of split documents.\n",
    "        embeddings (Embeddings): Embeddings to be used for FAISS.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: Configured FAISS vector store.\n",
    "    \"\"\"\n",
    "    return FAISS.from_documents(doc_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KGA2967RJCF",
    "outputId": "901db274-fe56-4d78-cd5d-faaab7468d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.9 s, sys: 2.87 s, total: 53.8 s\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "# Configure FAISS as Vector Store\n",
    "%%time\n",
    "vector_db = configure_faiss_vector_store(doc_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFmu05HlQ2AQ",
    "outputId": "b90e2d1b-0a40-424c-920d-abc69b84cd7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the FAISS index: 102\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents in the FAISS index:\", vector_db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "XaIWe3NKj3Kn"
   },
   "outputs": [],
   "source": [
    "def create_retriever(vector_store, search_type=\"similarity\", k=5):\n",
    "    \"\"\"\n",
    "    Exposes the vector store index to a retriever.\n",
    "\n",
    "    Args:\n",
    "        vector_store: The vector store (e.g., FAISS, Annoy, etc.).\n",
    "        search_type (str): The type of search to perform. Default is \"similarity\".\n",
    "        k (int): The number of documents to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        retriever: The configured retriever.\n",
    "    \"\"\"\n",
    "    return vector_store.as_retriever(\n",
    "        search_type=search_type, search_kwargs={\"k\": k}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "y2AsKeLkUzPG"
   },
   "outputs": [],
   "source": [
    " # Expose index to the retriever\n",
    "retriever = create_retriever(vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWRMcD9FZ4I0"
   },
   "source": [
    "### Retreive contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "6wfFIy4NptWn"
   },
   "outputs": [],
   "source": [
    "def process_query(query: str, retriever):\n",
    "    \"\"\"\n",
    "    Processes the query using the provided retriever to retrieve relevant document chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for relevant documents.\n",
    "        retriever: The retriever object configured to use the vector store for document retrieval.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the formatted content and metadata of the retrieved document chunks.\n",
    "    \"\"\"\n",
    "    # Retrieve chunks based on the query\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Initialize an empty string to collect all outputs\n",
    "    full_output = \"\"\n",
    "\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        chunk_output = f\"-----Chunk {i}------\\n\"\n",
    "        chunk_output += f\"Content: {doc.page_content}...\\n\"\n",
    "        chunk_output += f\"Metadata {doc.metadata}\\n\\n\"\n",
    "\n",
    "        # Append the chunk output to the full output\n",
    "        full_output += chunk_output\n",
    "\n",
    "    return full_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "9Bpi0z79V5BE"
   },
   "outputs": [],
   "source": [
    "# Sample Query\n",
    "\n",
    "# query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
    "# query = \"What is the regularization addressed in the academic article?\"\n",
    "# query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
    "query = \"What is the Attention addressed in the academic article?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVknk8M3WCu1",
    "outputId": "204e38d9-6161-40ff-c6f1-443719ecdaaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Chunk 1------\n",
      "Content: opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15...\n",
      "Metadata {'chunk': 101}\n",
      "\n",
      "-----Chunk 2------\n",
      "Content: opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be...\n",
      "Metadata {'chunk': 99}\n",
      "\n",
      "-----Chunk 3------\n",
      "Content: the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13\n",
      "Input-Input Layer5\n",
      "The...\n",
      "Metadata {'chunk': 97}\n",
      "\n",
      "-----Chunk 4------\n",
      "Content: the approach we take in our model.\n",
      "As side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5\n",
      "Training\n",
      "This section describes the training regime for our models.\n",
      "5.1\n",
      "Training Data and Batching...\n",
      "Metadata {'chunk': 50}\n",
      "\n",
      "-----Chunk 5------\n",
      "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
      "Metadata {'chunk': 5}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve chunks\n",
    "retrieved_output = process_query(query, retriever)\n",
    "\n",
    "# Print chunks\n",
    "print(retrieved_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzkVKKDnV5u9"
   },
   "source": [
    "### Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Vu0c9G5iV5V3"
   },
   "outputs": [],
   "source": [
    "# pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "bNGW60wSZTIX"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "def get_groq_response(client, prompt, model=\"llama3-70b-8192\", max_tokens=2048, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Generates a response using the provided client, model, prompt, and specified parameters.\n",
    "\n",
    "    Args:\n",
    "        client: The client object to interact with the API.\n",
    "        prompt (str): The prompt to generate a response for.\n",
    "        model (str, optional): The model identifier to use for generating the response. Default is \"llama3-70b-8192\".\n",
    "        max_tokens (int, optional): The maximum number of tokens for the generated response. Default is 2048.\n",
    "        temperature (float, optional): The temperature setting for the response generation. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The generated response content and usage statistics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content, chat_completion.usage\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "mCONlYHIaGef"
   },
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUyHPJ5lWDi0",
    "outputId": "e6c2027c-cf5e-4525-ad81-71638733a3ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello\"\n",
    "response = get_groq_response(client, prompt)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT_skBCedxYk"
   },
   "source": [
    "### RAG - HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "zHlgNqUCoNiz"
   },
   "outputs": [],
   "source": [
    "# prompt for RAG - HyDE\n",
    "instruction_hyde = \"\"\"\n",
    "### Instructions ###\n",
    "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" below seems relevant to \"Users' query\", please refer to it.\n",
    "\n",
    "### User’s query ###\n",
    "{USER_QUERY}\n",
    "\n",
    "### Context ###\n",
    "{CONTEXT_HYDE}\n",
    "\n",
    "### Output ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "fIbjqv0-aH75"
   },
   "outputs": [],
   "source": [
    "def generate_prompt_hyde(instruction, user_query, context_hyde):\n",
    "    \"\"\"\n",
    "    Generates a prompt for HyDE by replacing placeholders in the instruction template with the user's query and context.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The template instruction containing placeholders.\n",
    "        user_query (str): The user's query to be inserted into the instruction.\n",
    "        context_hyde (str): The context for creating a hypothetical answer to be inserted into the instruction.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated instruction with the placeholders replaced by the user's query and context.\n",
    "    \"\"\"\n",
    "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
    "    instruction = instruction.replace(\"{CONTEXT_HYDE}\", context_hyde)\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "1euLNgPeaXSp"
   },
   "outputs": [],
   "source": [
    "# user_query = \"What is the regularization addressed in the academic article?\"\n",
    "user_query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
    "# user_query  = \"What is the Attention addressed in the academic article?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "grkR3S8ee5Lj"
   },
   "outputs": [],
   "source": [
    "# Add abstract of the academic paper as a sample, but summary may be better as it can cover a wider range of document(especially for slides case, there is no abstract)\n",
    "# >>>>>>>>>Need to get appropriate paper and summary by using extraction function<<<<<<<<<\n",
    "\n",
    "context_hyde = \"\"\"\n",
    "Abstract\n",
    "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPoASCIMbOXG",
    "outputId": "30b6a4d4-6ea1-4469-dacd-a6019a5e221e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instructions ###\n",
      "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" below seems relevant to your response, please refer to it.\n",
      "\n",
      "### User’s query ###\n",
      "What is the main hypothesis or research question addressed in the first academic article?\n",
      "\n",
      "### Context ###\n",
      "\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "\n",
      "### Output ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create prompt for HyDE\n",
    "prompt_hyde = generate_prompt_hyde(instruction_hyde, user_query, context_hyde)\n",
    "print(prompt_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BRDQsnScEGn",
    "outputId": "ae2c750e-83e9-48dc-fa59-51994ec8d247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main hypothesis or research question addressed in the first academic article is: Can a sequence transduction model based solely on attention mechanisms, without recurrence and convolutions, achieve superior performance and efficiency in machine translation tasks compared to traditional models that rely on complex recurrent or convolutional neural networks?\n"
     ]
    }
   ],
   "source": [
    "# Get a hypothetical answer\n",
    "response = get_groq_response(client, prompt_hyde)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4aUE2DlcPcs",
    "outputId": "9cf602e7-fd9d-4e6e-e8f1-2434d95bdc32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Chunk 1------\n",
      "Content: Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to...\n",
      "Metadata {'chunk': 1}\n",
      "\n",
      "-----Chunk 2------\n",
      "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
      "Metadata {'chunk': 10}\n",
      "\n",
      "-----Chunk 3------\n",
      "Content: textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying...\n",
      "Metadata {'chunk': 15}\n",
      "\n",
      "-----Chunk 4------\n",
      "Content: To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3\n",
      "Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]....\n",
      "Metadata {'chunk': 16}\n",
      "\n",
      "-----Chunk 5------\n",
      "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2\n",
      "Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
      "Metadata {'chunk': 11}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find chunks based on similarity\n",
    "print(process_query(response[0], retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjelMdOQlZqA"
   },
   "source": [
    "### Extract thesis/figure/table numbers from user's query and search descriptions based on numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "p5Wq-zGomha3"
   },
   "outputs": [],
   "source": [
    "# Please refer to another jupyter notebook for the detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "zNAmmZ-XnrON"
   },
   "outputs": [],
   "source": [
    "# sample figure/image description\n",
    "context_figure_table = ['thesis1 description: --']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ARxr5weFqdG"
   },
   "source": [
    "### Provide a response given retreived contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Rfi5cWwBF22n"
   },
   "outputs": [],
   "source": [
    "instruction_final = \"\"\"\n",
    "### Instructions ###\n",
    "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
    "If the information in the \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them.\n",
    "\"Text Context\" includes several chunks from different parts of an academic paper. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper.\n",
    "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response.\n",
    "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
    "If you determine that the previous conversation history is relevant, please also refer to that information to answer the user's query.　\n",
    "Additionally, if the conversation is continuing from the previous session and no additional information is needed, you may refer to the previous conversation history and might not need to use the contexts below. (e.g., User's query: Please make your response brief).\n",
    "If the contexts and the previous conversation history do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
    "\n",
    "##### User’s query #####\n",
    "{USER_QUERY}\n",
    "\n",
    "\n",
    "##### Figure/Table Context #####\n",
    "{CONTEXT_FIGURE_TABLE}\n",
    "\n",
    "##### Text Context #####\n",
    "{CONTEXT_RAG_HYDE}\n",
    "\n",
    "{CONTEXT_RAG_GENERAL}\n",
    "\n",
    "\n",
    "##### Output #####\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "KhZEVdrUHd0T"
   },
   "outputs": [],
   "source": [
    "def generate_prompt_final(instruction, user_query, context_figure_table, context_rag_hyde, context_rag_general):\n",
    "    \"\"\"\n",
    "    Generates a final prompt by replacing placeholders in the instruction template with the user's query and various contexts.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The template instruction containing placeholders.\n",
    "        user_query (str): The user's query to be inserted into the instruction.\n",
    "        context_figure_table (str): The context(description) related to figure and table to be inserted into the instruction.\n",
    "        context_rag_hyde (str): The context retreived from RAG HyDE to be inserted into the instruction.\n",
    "        context_rag_general (str): The general context retreived from RAG to be inserted into the instruction.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated instruction with the placeholders replaced by the user's query and contexts.\n",
    "    \"\"\"\n",
    "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
    "    instruction = instruction.replace(\"{CONTEXT_FIGURE_TABLE}\", context_figure_table)\n",
    "    instruction = instruction.replace(\"{CONTEXT_RAG_HYDE}\", context_rag_hyde)\n",
    "    instruction = instruction.replace(\"{CONTEXT_RAG_GENERAL}\", context_rag_general)\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLBGJk3sKTfN",
    "outputId": "284c1d50-71c4-40de-981e-374a513deedf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Chunk 1------\n",
      "Content: opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15...\n",
      "Metadata {'chunk': 101}\n",
      "\n",
      "-----Chunk 2------\n",
      "Content: length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
      "6\n",
      "the input sequence centered around the respective output position. This would increase the maximum...\n",
      "Metadata {'chunk': 47}\n",
      "\n",
      "-----Chunk 3------\n",
      "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
      "Metadata {'chunk': 5}\n",
      "\n",
      "-----Chunk 4------\n",
      "Content: opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be...\n",
      "Metadata {'chunk': 99}\n",
      "\n",
      "-----Chunk 5------\n",
      "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
      "Metadata {'chunk': 10}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create contexts - RAG(General)\n",
    "context_rag_general = process_query(user_query, retriever)\n",
    "print(context_rag_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AQip0tgIjh3",
    "outputId": "9efc2729-5771-451a-fe9b-b299a48512ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Chunk 1------\n",
      "Content: Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to...\n",
      "Metadata {'chunk': 1}\n",
      "\n",
      "-----Chunk 2------\n",
      "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
      "Metadata {'chunk': 10}\n",
      "\n",
      "-----Chunk 3------\n",
      "Content: textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying...\n",
      "Metadata {'chunk': 15}\n",
      "\n",
      "-----Chunk 4------\n",
      "Content: To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3\n",
      "Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]....\n",
      "Metadata {'chunk': 16}\n",
      "\n",
      "-----Chunk 5------\n",
      "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2\n",
      "Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
      "Metadata {'chunk': 11}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create contexts - RAG(Hyde)\n",
    "context_rag_hyde = process_query(response[0], retriever)\n",
    "print(context_rag_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2ZqJDG5Ifz_",
    "outputId": "c747f34d-463f-4396-d28c-f609df79ea88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instructions ###\n",
      "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
      "If the information in the \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them. \n",
      "\"Text Context\" includes several chunks from different parts of an academic paper. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper. \n",
      "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response.\n",
      "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
      "If the contexts do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
      "\n",
      "\n",
      "##### User’s query #####\n",
      "What is the main hypothesis or research question addressed in the first academic article?\n",
      "\n",
      "\n",
      "##### Figure/Table Context #####\n",
      "['thesis1 description: --']\n",
      "\n",
      "##### Text Context #####\n",
      "-----Chunk 1------\n",
      "Content: Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to...\n",
      "Metadata {'chunk': 1}\n",
      "\n",
      "-----Chunk 2------\n",
      "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
      "Metadata {'chunk': 10}\n",
      "\n",
      "-----Chunk 3------\n",
      "Content: textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying...\n",
      "Metadata {'chunk': 15}\n",
      "\n",
      "-----Chunk 4------\n",
      "Content: To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3\n",
      "Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]....\n",
      "Metadata {'chunk': 16}\n",
      "\n",
      "-----Chunk 5------\n",
      "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2\n",
      "Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
      "Metadata {'chunk': 11}\n",
      "\n",
      "\n",
      "\n",
      "-----Chunk 1------\n",
      "Content: opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15...\n",
      "Metadata {'chunk': 101}\n",
      "\n",
      "-----Chunk 2------\n",
      "Content: length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
      "6\n",
      "the input sequence centered around the respective output position. This would increase the maximum...\n",
      "Metadata {'chunk': 47}\n",
      "\n",
      "-----Chunk 3------\n",
      "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
      "Metadata {'chunk': 5}\n",
      "\n",
      "-----Chunk 4------\n",
      "Content: opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be...\n",
      "Metadata {'chunk': 99}\n",
      "\n",
      "-----Chunk 5------\n",
      "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
      "Metadata {'chunk': 10}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### Output #####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create prompt for a final response\n",
    "prompt_final = generate_prompt_final(instruction_final, user_query, str(context_figure_table), context_rag_hyde, context_rag_general)\n",
    "print(prompt_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMxrgB4xJSVx",
    "outputId": "b4248402-6dbe-4ba0-df88-e2ca19aebc9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main hypothesis or research question addressed in the first academic article is: Can a sequence transduction model relying solely on attention mechanisms, without using recurrence or convolution, achieve state-of-the-art results in machine translation tasks?\n"
     ]
    }
   ],
   "source": [
    "# Get final response\n",
    "response = get_groq_response(client, prompt_final)\n",
    "print(response[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09bd83a20a6e4c6881da65c8c82b37b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cc674826142494bbf9205ac0583f14b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "253414e97e99406db7f58a556539eafb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2879e77e3d82444db0ba7e3187931ac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_95df531bcf854fb0b80b3d50d31adde3",
       "IPY_MODEL_cc7b7a9af09446efb91aa9e590150e8c",
       "IPY_MODEL_9a1a23324c4e48e5bad4d04c5e6aedf5"
      ],
      "layout": "IPY_MODEL_f6fd82678d3f4a8191418b5681a3ffe0"
     }
    },
    "2cf46ed90b534dfb84c1a0a86cd37cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f128bc400dbb4c5293aa007b948ac957",
      "max": 442221694,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d20e0b51b7c4ac8af29ca296e2554f0",
      "value": 442221694
     }
    },
    "302aa94d588448cdaf93d0c8e3c83d7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48f7b6925b5845eaa338cdcfabffa533": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ef614a8f174408cba72974ddb0cfa99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5235c7fde8d14c15bfe40ba2c6ad3df7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_302aa94d588448cdaf93d0c8e3c83d7b",
      "max": 385,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f045e69e1a224341a90490a3e33c32ac",
      "value": 385
     }
    },
    "5a953f60bba448f092a145757b0be464": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d20e0b51b7c4ac8af29ca296e2554f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5fc77181c38943ffb208eed889d9d52e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "634329a3ba5749bc92e28cc9349f96ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8cdee6b124a4f3dbafe4853530b8070",
      "placeholder": "​",
      "style": "IPY_MODEL_e8c3e430fb364ff78edcc0b7bbb37827",
      "value": " 385/385 [00:00&lt;00:00, 21.3kB/s]"
     }
    },
    "66c93a3adbfa46e2a5a73baea6e1bf42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6eb13d45b34149c087d691ecf023bdb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8425e17b886644faaac946195a975a52",
       "IPY_MODEL_5235c7fde8d14c15bfe40ba2c6ad3df7",
       "IPY_MODEL_634329a3ba5749bc92e28cc9349f96ec"
      ],
      "layout": "IPY_MODEL_48f7b6925b5845eaa338cdcfabffa533"
     }
    },
    "8118e0ddd091477bba719ad41b886dfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8425e17b886644faaac946195a975a52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f3cd45a0f494741925f070cf7982dbf",
      "placeholder": "​",
      "style": "IPY_MODEL_c6fc96efd3f843c4a248082278d85f60",
      "value": "config.json: 100%"
     }
    },
    "8b4e60d3937e4303a83339d47bb23ec7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a953f60bba448f092a145757b0be464",
      "placeholder": "​",
      "style": "IPY_MODEL_4ef614a8f174408cba72974ddb0cfa99",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "8f3cd45a0f494741925f070cf7982dbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95df531bcf854fb0b80b3d50d31adde3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99446ad02b86405e9cde439de840e505",
      "placeholder": "​",
      "style": "IPY_MODEL_5fc77181c38943ffb208eed889d9d52e",
      "value": "vocab.txt: 100%"
     }
    },
    "99446ad02b86405e9cde439de840e505": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a1a23324c4e48e5bad4d04c5e6aedf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1d90486e6744eedb1921ad2d1619e21",
      "placeholder": "​",
      "style": "IPY_MODEL_8118e0ddd091477bba719ad41b886dfe",
      "value": " 228k/228k [00:00&lt;00:00, 3.05MB/s]"
     }
    },
    "b8cdee6b124a4f3dbafe4853530b8070": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1d90486e6744eedb1921ad2d1619e21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6fc96efd3f843c4a248082278d85f60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc7b7a9af09446efb91aa9e590150e8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66c93a3adbfa46e2a5a73baea6e1bf42",
      "max": 227845,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_253414e97e99406db7f58a556539eafb",
      "value": 227845
     }
    },
    "cebe7d8b9b80498291aa9aaa47b76bbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8b4e60d3937e4303a83339d47bb23ec7",
       "IPY_MODEL_2cf46ed90b534dfb84c1a0a86cd37cea",
       "IPY_MODEL_f7892a4e8e1b4440ad82f675a2fd9db3"
      ],
      "layout": "IPY_MODEL_ec80c509488540f090236b45d37ff218"
     }
    },
    "e8c3e430fb364ff78edcc0b7bbb37827": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec80c509488540f090236b45d37ff218": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f045e69e1a224341a90490a3e33c32ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f128bc400dbb4c5293aa007b948ac957": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6fd82678d3f4a8191418b5681a3ffe0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7892a4e8e1b4440ad82f675a2fd9db3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09bd83a20a6e4c6881da65c8c82b37b0",
      "placeholder": "​",
      "style": "IPY_MODEL_0cc674826142494bbf9205ac0583f14b",
      "value": " 442M/442M [00:06&lt;00:00, 72.4MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
