{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sNiKUkfkZwPG"
      },
      "outputs": [],
      "source": [
        "# !pip install -U langchain-community\n",
        "# !pip install sentence-transformers\n",
        "# !pip install faiss-cpu\n",
        "# !pip install --upgrade langchain\n",
        "# !pip install fitz\n",
        "# !pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s1lfUClCZhuo"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZoheG32fx0o"
      },
      "source": [
        "### Load PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cmPA1v6bfgTg"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from a single PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "def extract_texts_from_pdfs(pdf_paths):\n",
        "    \"\"\"\n",
        "    Extract text from each PDF file in the list and create Document objects.\n",
        "\n",
        "    Args:\n",
        "        pdf_paths (list of str): List of paths to PDF files.\n",
        "\n",
        "    Returns:\n",
        "        list of Document: List of Document objects containing the extracted text.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        text = extract_text(pdf_path)\n",
        "        doc = Document(page_content=text, metadata={\"source\": pdf_path})\n",
        "        docs.append(doc)\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LTbfdWp9II1P"
      },
      "outputs": [],
      "source": [
        "### List of PDF files(All files) ###\n",
        "pdf_paths = [\"attention.pdf\", \"Multimodal.pdf\"]\n",
        "\n",
        "# Extract text from each PDF and create Document objects\n",
        "docs = extract_texts_from_pdfs(pdf_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TunwL3riOeQt"
      },
      "source": [
        "### Chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "I2Uytk6VN2Ya"
      },
      "outputs": [],
      "source": [
        "def split_documents_into_chunks(docs, chunk_size=500, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Splits the given documents into chunks of specified size with overlap.\n",
        "\n",
        "    Args:\n",
        "        docs (list): List of documents to split.\n",
        "        chunk_size (int): Size of each chunk. Default is 500 characters.\n",
        "        chunk_overlap (int): Overlap size between chunks. Default is 100 characters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of lists containing split documents with chunks per original document.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "    doc_chunks = {}\n",
        "    for doc in docs:\n",
        "        doc_chunks[doc.metadata[\"source\"]] = text_splitter.split_documents([doc])\n",
        "    return doc_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6mdMWt5-N7Yx"
      },
      "outputs": [],
      "source": [
        "def add_chunk_numbers_to_metadata(doc_chunks):\n",
        "    \"\"\"\n",
        "    Adds chunk numbers to the metadata of each split document.\n",
        "\n",
        "    Args:\n",
        "        doc_chunks (dict): Dictionary of lists containing split documents.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of lists containing split documents with updated metadata.\n",
        "    \"\"\"\n",
        "    for chunks in doc_chunks.values():\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            chunk.metadata[\"chunk\"] = idx\n",
        "    return doc_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "clHnXEjoOFms"
      },
      "outputs": [],
      "source": [
        "# Split the documents into chunks\n",
        "doc_splits = split_documents_into_chunks(docs)\n",
        "# Add chunk number to metadata\n",
        "doc_splits = add_chunk_numbers_to_metadata(doc_splits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of splits per document\n",
        "for doc_source, chunks in doc_splits.items():\n",
        "    print(f'Document {doc_source} has {len(chunks):,} splits')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqvZ4Mv9IfVH",
        "outputId": "053e606a-2c1e-47e0-cbac-8e97322f13e1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document attention.pdf has 102 splits\n",
            "Document Multimodal.pdf has 232 splits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first 3 chunks for each document\n",
        "for doc_source, chunks in doc_splits.items():\n",
        "    print(f'--- Document: {doc_source} ---')\n",
        "    for split in chunks[:3]:\n",
        "        print(f'---Page Content---\\n{split.page_content}')\n",
        "        print(f'Metadata:\\n{split.metadata}')\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeLgqFdQLTGl",
        "outputId": "c49deba9-f760-45be-db0c-61486e3c79f8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Document: attention.pdf ---\n",
            "---Page Content---\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.com\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "nikip@google.com\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.com\n",
            "Aidan N. Gomez∗†\n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "Łukasz Kaiser∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "Metadata:\n",
            "{'source': 'attention.pdf', 'chunk': 0}\n",
            "\n",
            "---Page Content---\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "Metadata:\n",
            "{'source': 'attention.pdf', 'chunk': 1}\n",
            "\n",
            "---Page Content---\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
            "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
            "to-German translation task, improving over the existing best results, including\n",
            "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
            "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
            "Metadata:\n",
            "{'source': 'attention.pdf', 'chunk': 2}\n",
            "\n",
            "--- Document: Multimodal.pdf ---\n",
            "---Page Content---\n",
            "Visual Instruction Tuning\n",
            "Haotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yong Jae Lee1\n",
            "1University of Wisconsin–Madison\n",
            "2Microsoft Research\n",
            "3Columbia University\n",
            "https://llava-vl.github.io\n",
            "Abstract\n",
            "Instruction tuning large language models (LLMs) using machine-generated\n",
            "instruction-following data has been shown to improve zero-shot capabilities on\n",
            "new tasks, but the idea is less explored in the multimodal field. We present the\n",
            "Metadata:\n",
            "{'source': 'Multimodal.pdf', 'chunk': 0}\n",
            "\n",
            "---Page Content---\n",
            "new tasks, but the idea is less explored in the multimodal field. We present the\n",
            "first attempt to use language-only GPT-4 to generate multimodal language-image\n",
            "instruction-following data. By instruction tuning on such generated data, we in-\n",
            "troduce LLaVA: Large Language and Vision Assistant, an end-to-end trained\n",
            "large multimodal model that connects a vision encoder and an LLM for general-\n",
            "purpose visual and language understanding. To facilitate future research on visual\n",
            "Metadata:\n",
            "{'source': 'Multimodal.pdf', 'chunk': 1}\n",
            "\n",
            "---Page Content---\n",
            "purpose visual and language understanding. To facilitate future research on visual\n",
            "instruction following, we construct two evaluation benchmarks with diverse and\n",
            "challenging application-oriented tasks. Our experiments show that LLaVA demon-\n",
            "strates impressive multimodal chat abilities, sometimes exhibiting the behaviors\n",
            "of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela-\n",
            "tive score compared with GPT-4 on a synthetic multimodal instruction-following\n",
            "Metadata:\n",
            "{'source': 'Multimodal.pdf', 'chunk': 2}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ila_HXSxdOIb"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "8549cf49fc51465b9f41b1c979269e87",
            "fa48e2c90011415e98d3ab398843399b",
            "80fccae24b2d421080e7dc814aada95c",
            "c730e960939a4abe8f6f27b322832284",
            "e92cd14bba9d4551bcf47b6ee954ceb1",
            "1c507e74b3bc481e93e5b5e4865b70d7",
            "62ba8070bfd14b128ac853632acf0c11",
            "e120db3337ed4a40982b4282dbba142f",
            "64542834ce5347e0a4ed2ea702e538eb",
            "bda293ca6bd24e8486840224c840046f",
            "d447cb041ef14ad78c88ea269a81d5ef",
            "0644802eb1504a0892d369f887ae5cc7",
            "682d11dde371405391bdb3f182881167",
            "b228b63d09c345f79561eb3b6291f0d0",
            "e75177c5582b45c093522ed54ae402d3",
            "e28dd9593bec4dd290c2e905aa465c3b",
            "44bc6a663e1c47669614e34d8dbf1465",
            "1fa80b2c74b74fe0928a53cefbf777dd",
            "9eef4ee36a114554ac75857526d5a152",
            "dc400afd2ebd413c8dda12f8077a5022",
            "d233a2bcd6b64d799106de30ebb3fbd4",
            "d220084b84944177bd9472840b285104",
            "bf30e2cddfd140e5ac0e4ceb0efd7a80",
            "81d202f3e20049058eb179a442e501b5",
            "53ed714d28b8451bb9acd68747a5ddf4",
            "9f00d52dcac5403da3c0d6a11b0ef2a5",
            "0a557373db07443e9b6405b049bfbba9",
            "9beb04b3b2fc469ebea95f5799a9b7a6",
            "b1d3957a6bf645a29b92ee23423fa5e4",
            "58e528ae9300458d908ecb38cb70996c",
            "e9395f7d58bc4d9bb445e4e0900ae79b",
            "d73a9542c639464693e36f514a098d36",
            "c5ec62c5edbb40c18dc2c952ea7499c6"
          ]
        },
        "id": "i9IFXIy2ZsPT",
        "outputId": "c6d01ac5-941c-4f9b-cc29-60e0b876ce57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8549cf49fc51465b9f41b1c979269e87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0644802eb1504a0892d369f887ae5cc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf30e2cddfd140e5ac0e4ceb0efd7a80"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# embedding model - Stronger model can be considered\n",
        "\n",
        "# SciBERT(Allen Institute for AI) - for academic(science) paper including computer science - maximum 512 tokens\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # small model(microsoft)\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"roberta-large\") # RoBERTa - large (facebook)　- Longer context\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"roberta-base\") # RoBERTa - base(facebook)　- Longer context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwVMT3Tdd2-m"
      },
      "source": [
        "### Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "sFILjzeod616"
      },
      "outputs": [],
      "source": [
        "def configure_faiss_vector_store(doc_splits, embeddings):\n",
        "    \"\"\"\n",
        "    Configures FAISS as the vector store using the provided document splits and embeddings.\n",
        "\n",
        "    Args:\n",
        "        doc_splits (dict): Dictionary of lists containing split documents.\n",
        "        embeddings (Embeddings): Embeddings to be used for FAISS.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of FAISS vector stores per document.\n",
        "    \"\"\"\n",
        "    vector_stores = {}\n",
        "    for doc_source, chunks in doc_splits.items():\n",
        "        vector_stores[doc_source] = FAISS.from_documents(chunks, embeddings)\n",
        "    return vector_stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KGA2967RJCF",
        "outputId": "6350e190-4196-48a4-86a4-4850a0ea4e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 22s, sys: 13.5 s, total: 4min 36s\n",
            "Wall time: 4min 38s\n"
          ]
        }
      ],
      "source": [
        "# Configure FAISS as Vector Store\n",
        "%%time\n",
        "vector_db = configure_faiss_vector_store(doc_splits, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5pHQZiJPFAj",
        "outputId": "ad6a3764-0bea-4b9e-f4f3-0fdffa9d882c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention.pdf': <langchain_community.vectorstores.faiss.FAISS at 0x7945eaaa9750>,\n",
              " 'Multimodal.pdf': <langchain_community.vectorstores.faiss.FAISS at 0x7945eaaa9780>}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFmu05HlQ2AQ",
        "outputId": "10b7678c-67ab-43a8-c424-07eb653610b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents in the FAISS index for attention.pdf: 102\n",
            "Number of documents in the FAISS index for Multimodal.pdf: 232\n"
          ]
        }
      ],
      "source": [
        "# print(\"Number of documents in the FAISS index:\", vector_db.index.ntotal)\n",
        "# Print number of documents in the FAISS index for each document\n",
        "for doc_source, faiss_index in vector_db.items():\n",
        "    print(f\"Number of documents in the FAISS index for {doc_source}: {faiss_index.index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XaIWe3NKj3Kn"
      },
      "outputs": [],
      "source": [
        "def create_retrievers(vector_stores, search_type=\"similarity\", k=5):\n",
        "    \"\"\"\n",
        "    Exposes the vector store index to retrievers for multiple documents.\n",
        "\n",
        "    Args:\n",
        "        vector_stores (dict): Dictionary of FAISS vector stores per document.\n",
        "        search_type (str): The type of search to perform. Default is \"similarity\".\n",
        "        k (int): The number of documents to return. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of retrievers per document.\n",
        "    \"\"\"\n",
        "    retrievers = {}\n",
        "    for doc_source, vector_store in vector_stores.items():\n",
        "        retrievers[doc_source] = vector_store.as_retriever(\n",
        "            search_type=search_type, search_kwargs={\"k\": k}\n",
        "        )\n",
        "    return retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "y2AsKeLkUzPG"
      },
      "outputs": [],
      "source": [
        "# Create retrievers for each document and store them in a dictionary\n",
        "retrievers = create_retrievers(vector_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWRMcD9FZ4I0"
      },
      "source": [
        "### Retreive contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6wfFIy4NptWn"
      },
      "outputs": [],
      "source": [
        "def process_query(query: str, retriever):\n",
        "    \"\"\"\n",
        "    Processes the query using the provided retriever to retrieve relevant document chunks.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string to search for relevant documents.\n",
        "        retriever: The retriever object configured to use the vector store for document retrieval.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the formatted content and metadata of the retrieved document chunks.\n",
        "    \"\"\"\n",
        "    # Retrieve chunks based on the query\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "    # Initialize an empty string to collect all outputs\n",
        "    full_output = \"\"\n",
        "\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        chunk_output = f\"-----Chunk {i}------\\n\"\n",
        "        chunk_output += f\"Content: {doc.page_content}...\\n\"\n",
        "        chunk_output += f\"Metadata {doc.metadata}\\n\\n\"\n",
        "\n",
        "        # Append the chunk output to the full output\n",
        "        full_output += chunk_output\n",
        "\n",
        "    return full_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "9Bpi0z79V5BE"
      },
      "outputs": [],
      "source": [
        "# Sample Query\n",
        "\n",
        "# query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
        "# query = \"What is the regularization addressed in the academic article?\"\n",
        "# query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
        "query = \"What is the Attention addressed in the academic article?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVknk8M3WCu1",
        "outputId": "65cb053f-cfbd-412f-8bc1-1286bf48f5e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Chunk 1------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
            "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
            "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "15...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 101}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
            "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
            "and 6. Note that the attentions are very sharp for this word.\n",
            "14\n",
            "Input-Input Layer5\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 99}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: the\n",
            "registration\n",
            "or\n",
            "voting\n",
            "process\n",
            "more\n",
            "difficult\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
            "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
            "the verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\n",
            "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
            "13\n",
            "Input-Input Layer5\n",
            "The...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 97}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: the approach we take in our model.\n",
            "As side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\n",
            "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
            "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
            "and semantic structure of the sentences.\n",
            "5\n",
            "Training\n",
            "This section describes the training regime for our models.\n",
            "5.1\n",
            "Training Data and Batching...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 50}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 5}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Retrieve chunks　from the first document\n",
        "# retriever = retrievers[\"attention.pdf\"]\n",
        "retriever = retrievers[pdf_paths[0]]\n",
        "retrieved_output = process_query(query, retriever)\n",
        "\n",
        "# Print chunks\n",
        "print(retrieved_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzkVKKDnV5u9"
      },
      "source": [
        "### Setup LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Vu0c9G5iV5V3"
      },
      "outputs": [],
      "source": [
        "# pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "bNGW60wSZTIX"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "\n",
        "def get_groq_response(client, prompt, model=\"llama3-70b-8192\", max_tokens=2048, temperature=0.0):\n",
        "    \"\"\"\n",
        "    Generates a response using the provided client, model, prompt, and specified parameters.\n",
        "\n",
        "    Args:\n",
        "        client: The client object to interact with the API.\n",
        "        prompt (str): The prompt to generate a response for.\n",
        "        model (str, optional): The model identifier to use for generating the response. Default is \"llama3-70b-8192\".\n",
        "        max_tokens (int, optional): The maximum number of tokens for the generated response. Default is 2048.\n",
        "        temperature (float, optional): The temperature setting for the response generation. Default is 0.0.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The generated response content and usage statistics.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ],\n",
        "            model=model,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content, chat_completion.usage\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "mCONlYHIaGef"
      },
      "outputs": [],
      "source": [
        "client = Groq(\n",
        "    api_key=\"YOUR_API_KEY\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUyHPJ5lWDi0",
        "outputId": "e6c2027c-cf5e-4525-ad81-71638733a3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello\"\n",
        "response = get_groq_response(client, prompt)\n",
        "print(response[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT_skBCedxYk"
      },
      "source": [
        "### RAG - HyDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "zHlgNqUCoNiz"
      },
      "outputs": [],
      "source": [
        "# prompt for RAG - HyDE\n",
        "instruction_hyde = \"\"\"\n",
        "### Instructions ###\n",
        "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" below seems relevant to \"Users' query\", please refer to it.\n",
        "\n",
        "### User’s query ###\n",
        "{USER_QUERY}\n",
        "\n",
        "### Context ###\n",
        "{CONTEXT_HYDE}\n",
        "\n",
        "### Output ###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fIbjqv0-aH75"
      },
      "outputs": [],
      "source": [
        "def generate_prompt_hyde(instruction, user_query, context_hyde):\n",
        "    \"\"\"\n",
        "    Generates a prompt for HyDE by replacing placeholders in the instruction template with the user's query and context.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing placeholders.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "        context_hyde (str): The context for creating a hypothetical answer to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholders replaced by the user's query and context.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    instruction = instruction.replace(\"{CONTEXT_HYDE}\", context_hyde)\n",
        "    return instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "1euLNgPeaXSp"
      },
      "outputs": [],
      "source": [
        "# user_query = \"What is the regularization addressed in the academic article?\"\n",
        "user_query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
        "# user_query  = \"What is the Attention addressed in the academic article?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "grkR3S8ee5Lj"
      },
      "outputs": [],
      "source": [
        "# Add abstract of the academic paper as a sample (but summary may be better as it can cover a wider range of document(especially for slides case, there is no abstract))\n",
        "\n",
        "context_hyde = \"\"\"\n",
        "Abstract\n",
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPoASCIMbOXG",
        "outputId": "dae95699-90ae-4b29-fbd9-6bd9f1386a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Instructions ###\n",
            "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" below seems relevant to \"Users' query\", please refer to it.\n",
            "\n",
            "### User’s query ###\n",
            "What is the main hypothesis or research question addressed in the first academic article?\n",
            "\n",
            "### Context ###\n",
            "\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
            "\n",
            "\n",
            "### Output ###\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create prompt for HyDE\n",
        "prompt_hyde = generate_prompt_hyde(instruction_hyde, user_query, context_hyde)\n",
        "print(prompt_hyde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BRDQsnScEGn",
        "outputId": "2283868a-3f1b-4a87-9759-f4edddf82619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main hypothesis or research question addressed in the first academic article is: Can a sequence transduction model based solely on attention mechanisms, without recurrence and convolutions, achieve superior performance and efficiency in machine translation tasks compared to traditional models based on complex recurrent or convolutional neural networks?\n"
          ]
        }
      ],
      "source": [
        "# Get a hypothetical answer\n",
        "response = get_groq_response(client, prompt_hyde)\n",
        "print(response[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4aUE2DlcPcs",
        "outputId": "7c8e51d4-aca9-407f-8022-23895a11d908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Chunk 1------\n",
            "Content: Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 1}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
            "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
            "language modeling tasks [34].\n",
            "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 15}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
            "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
            "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
            "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "2\n",
            "Background\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 11}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
            "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
            "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
            "the number of operations required to relate signals from two arbitrary input or output positions grows...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 12}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find chunks based on similarity\n",
        "print(process_query(response[0], retrievers[pdf_paths[0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjelMdOQlZqA"
      },
      "source": [
        "### Extract thesis/figure/table numbers from user's query and search descriptions based on numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to another jupyter notebook for the detail  \n",
        "https://github.com/daichi6/llm-hackathon-insightai/blob/main/notebooks/extract_query.ipynb"
      ],
      "metadata": {
        "id": "OkVpGhqn5OVB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "zNAmmZ-XnrON"
      },
      "outputs": [],
      "source": [
        "# sample figure/image description\n",
        "context_figure_table = ['thesis1 description: --']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ARxr5weFqdG"
      },
      "source": [
        "### Provide a response given retreived contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Rfi5cWwBF22n"
      },
      "outputs": [],
      "source": [
        "instruction_final = \"\"\"\n",
        "### Instructions ###\n",
        "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
        "If the information in the \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them.\n",
        "\"Text Context\" includes several chunks from different parts of an academic paper. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper.\n",
        "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response.\n",
        "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
        "If you determine that the previous conversation history is relevant, please also refer to that information to answer the user's query.　Especially when the the contexts below are empty, please answer the user's most recent query　based on the conversation history(the user's previous queries and your responses).\n",
        "If the conversation is continuing from the previous session and no additional information is needed, you may refer to the previous conversation history and might not need to use the contexts below. (e.g., User's query: Please make your response brief).\n",
        "If the contexts and the previous conversation history do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
        "\n",
        "##### User’s query #####\n",
        "{USER_QUERY}\n",
        "\n",
        "\n",
        "##### Figure/Table Context #####\n",
        "{CONTEXT_FIGURE_TABLE}\n",
        "\n",
        "##### Text Context #####\n",
        "{CONTEXT_RAG_HYDE}\n",
        "\n",
        "{CONTEXT_RAG_GENERAL}\n",
        "\n",
        "\n",
        "##### Output #####\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "KhZEVdrUHd0T"
      },
      "outputs": [],
      "source": [
        "def generate_prompt_final(instruction, user_query, context_figure_table, context_rag_hyde, context_rag_general):\n",
        "    \"\"\"\n",
        "    Generates a final prompt by replacing placeholders in the instruction template with the user's query and various contexts.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing placeholders.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "        context_figure_table (str): The context(description) related to figure and table to be inserted into the instruction.\n",
        "        context_rag_hyde (str): The context retreived from RAG HyDE to be inserted into the instruction.\n",
        "        context_rag_general (str): The general context retreived from RAG to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholders replaced by the user's query and contexts.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    instruction = instruction.replace(\"{CONTEXT_FIGURE_TABLE}\", context_figure_table)\n",
        "    instruction = instruction.replace(\"{CONTEXT_RAG_HYDE}\", context_rag_hyde)\n",
        "    instruction = instruction.replace(\"{CONTEXT_RAG_GENERAL}\", context_rag_general)\n",
        "    return instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLBGJk3sKTfN",
        "outputId": "6d96dd7e-7789-4ec8-bfa5-09bbd1e1d0d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Chunk 1------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
            "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
            "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "15...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 101}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: length n is smaller than the representation dimensionality d, which is most often the case with\n",
            "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
            "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
            "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
            "6\n",
            "the input sequence centered around the respective output position. This would increase the maximum...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 47}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 5}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
            "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
            "and 6. Note that the attentions are very sharp for this word.\n",
            "14\n",
            "Input-Input Layer5\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 99}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create contexts - RAG(General)\n",
        "context_rag_general = process_query(user_query, retrievers[pdf_paths[0]])\n",
        "print(context_rag_general)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AQip0tgIjh3",
        "outputId": "c8d8edcd-8ec4-46af-f0dd-3a20effbc89a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Chunk 1------\n",
            "Content: Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 1}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
            "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
            "language modeling tasks [34].\n",
            "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 15}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
            "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
            "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
            "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "2\n",
            "Background\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 11}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
            "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
            "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
            "the number of operations required to relate signals from two arbitrary input or output positions grows...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 12}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create contexts - RAG(Hyde)\n",
        "context_rag_hyde = process_query(response[0], retrievers[pdf_paths[0]])\n",
        "print(context_rag_hyde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ZqJDG5Ifz_",
        "outputId": "71a0fe89-f54d-4b5f-c214-f7943ce5497f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Instructions ###\n",
            "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
            "If the information in the \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them.\n",
            "\"Text Context\" includes several chunks from different parts of an academic paper. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper.\n",
            "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response.\n",
            "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
            "If you determine that the previous conversation history is relevant, please also refer to that information to answer the user's query.　\n",
            "Additionally, if the conversation is continuing from the previous session and no additional information is needed, you may refer to the previous conversation history and might not need to use the contexts below. (e.g., User's query: Please make your response brief).\n",
            "If the contexts and the previous conversation history do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
            "\n",
            "##### User’s query #####\n",
            "What is the main hypothesis or research question addressed in the first academic article?\n",
            "\n",
            "\n",
            "##### Figure/Table Context #####\n",
            "['thesis1 description: --']\n",
            "\n",
            "##### Text Context #####\n",
            "-----Chunk 1------\n",
            "Content: Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 1}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
            "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
            "language modeling tasks [34].\n",
            "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 15}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
            "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
            "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
            "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "2\n",
            "Background\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 11}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
            "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
            "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
            "the number of operations required to relate signals from two arbitrary input or output positions grows...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 12}\n",
            "\n",
            "\n",
            "\n",
            "-----Chunk 1------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
            "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
            "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "15...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 101}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: length n is smaller than the representation dimensionality d, which is most often the case with\n",
            "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
            "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
            "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
            "6\n",
            "the input sequence centered around the respective output position. This would increase the maximum...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 47}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 5}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
            "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
            "and 6. Note that the attentions are very sharp for this word.\n",
            "14\n",
            "Input-Input Layer5\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 99}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "##### Output #####\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create prompt for a final response\n",
        "prompt_final = generate_prompt_final(instruction_final, user_query, str(context_figure_table), context_rag_hyde, context_rag_general)\n",
        "print(prompt_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMxrgB4xJSVx",
        "outputId": "5b27d439-d7c2-478c-f71e-aac9d82c3d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main hypothesis or research question addressed in the first academic article is: Can a sequence transduction model based solely on attention mechanisms, without recurrence and convolutions, achieve state-of-the-art results in machine translation tasks?\n"
          ]
        }
      ],
      "source": [
        "# Get final response\n",
        "response = get_groq_response(client, prompt_final)\n",
        "print(response[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8549cf49fc51465b9f41b1c979269e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa48e2c90011415e98d3ab398843399b",
              "IPY_MODEL_80fccae24b2d421080e7dc814aada95c",
              "IPY_MODEL_c730e960939a4abe8f6f27b322832284"
            ],
            "layout": "IPY_MODEL_e92cd14bba9d4551bcf47b6ee954ceb1"
          }
        },
        "fa48e2c90011415e98d3ab398843399b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c507e74b3bc481e93e5b5e4865b70d7",
            "placeholder": "​",
            "style": "IPY_MODEL_62ba8070bfd14b128ac853632acf0c11",
            "value": "config.json: 100%"
          }
        },
        "80fccae24b2d421080e7dc814aada95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e120db3337ed4a40982b4282dbba142f",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64542834ce5347e0a4ed2ea702e538eb",
            "value": 385
          }
        },
        "c730e960939a4abe8f6f27b322832284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bda293ca6bd24e8486840224c840046f",
            "placeholder": "​",
            "style": "IPY_MODEL_d447cb041ef14ad78c88ea269a81d5ef",
            "value": " 385/385 [00:00&lt;00:00, 15.6kB/s]"
          }
        },
        "e92cd14bba9d4551bcf47b6ee954ceb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c507e74b3bc481e93e5b5e4865b70d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ba8070bfd14b128ac853632acf0c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e120db3337ed4a40982b4282dbba142f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64542834ce5347e0a4ed2ea702e538eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bda293ca6bd24e8486840224c840046f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d447cb041ef14ad78c88ea269a81d5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0644802eb1504a0892d369f887ae5cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_682d11dde371405391bdb3f182881167",
              "IPY_MODEL_b228b63d09c345f79561eb3b6291f0d0",
              "IPY_MODEL_e75177c5582b45c093522ed54ae402d3"
            ],
            "layout": "IPY_MODEL_e28dd9593bec4dd290c2e905aa465c3b"
          }
        },
        "682d11dde371405391bdb3f182881167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44bc6a663e1c47669614e34d8dbf1465",
            "placeholder": "​",
            "style": "IPY_MODEL_1fa80b2c74b74fe0928a53cefbf777dd",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "b228b63d09c345f79561eb3b6291f0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eef4ee36a114554ac75857526d5a152",
            "max": 442221694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc400afd2ebd413c8dda12f8077a5022",
            "value": 442221694
          }
        },
        "e75177c5582b45c093522ed54ae402d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d233a2bcd6b64d799106de30ebb3fbd4",
            "placeholder": "​",
            "style": "IPY_MODEL_d220084b84944177bd9472840b285104",
            "value": " 442M/442M [00:11&lt;00:00, 40.2MB/s]"
          }
        },
        "e28dd9593bec4dd290c2e905aa465c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44bc6a663e1c47669614e34d8dbf1465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fa80b2c74b74fe0928a53cefbf777dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eef4ee36a114554ac75857526d5a152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc400afd2ebd413c8dda12f8077a5022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d233a2bcd6b64d799106de30ebb3fbd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d220084b84944177bd9472840b285104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf30e2cddfd140e5ac0e4ceb0efd7a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81d202f3e20049058eb179a442e501b5",
              "IPY_MODEL_53ed714d28b8451bb9acd68747a5ddf4",
              "IPY_MODEL_9f00d52dcac5403da3c0d6a11b0ef2a5"
            ],
            "layout": "IPY_MODEL_0a557373db07443e9b6405b049bfbba9"
          }
        },
        "81d202f3e20049058eb179a442e501b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9beb04b3b2fc469ebea95f5799a9b7a6",
            "placeholder": "​",
            "style": "IPY_MODEL_b1d3957a6bf645a29b92ee23423fa5e4",
            "value": "vocab.txt: 100%"
          }
        },
        "53ed714d28b8451bb9acd68747a5ddf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58e528ae9300458d908ecb38cb70996c",
            "max": 227845,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9395f7d58bc4d9bb445e4e0900ae79b",
            "value": 227845
          }
        },
        "9f00d52dcac5403da3c0d6a11b0ef2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73a9542c639464693e36f514a098d36",
            "placeholder": "​",
            "style": "IPY_MODEL_c5ec62c5edbb40c18dc2c952ea7499c6",
            "value": " 228k/228k [00:00&lt;00:00, 4.03MB/s]"
          }
        },
        "0a557373db07443e9b6405b049bfbba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9beb04b3b2fc469ebea95f5799a9b7a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d3957a6bf645a29b92ee23423fa5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58e528ae9300458d908ecb38cb70996c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9395f7d58bc4d9bb445e4e0900ae79b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d73a9542c639464693e36f514a098d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5ec62c5edbb40c18dc2c952ea7499c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}