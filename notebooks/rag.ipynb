{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sNiKUkfkZwPG"
      },
      "outputs": [],
      "source": [
        "# !pip install -U langchain-community\n",
        "# !pip install sentence-transformers\n",
        "# !pip install faiss-cpu\n",
        "# !pip install --upgrade langchain\n",
        "# !pip install fitz\n",
        "# !pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s1lfUClCZhuo"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZoheG32fx0o"
      },
      "source": [
        "### Load PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cmPA1v6bfgTg"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from a single PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "def extract_texts_from_pdfs(pdf_paths):\n",
        "    \"\"\"\n",
        "    Extract text from each PDF file in the list and create Document objects.\n",
        "\n",
        "    Args:\n",
        "        pdf_paths (list of str): List of paths to PDF files.\n",
        "\n",
        "    Returns:\n",
        "        list of Document: List of Document objects containing the extracted text.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        text = extract_text(pdf_path)\n",
        "        doc = Document(page_content=text, metadata={\"source\": pdf_path})\n",
        "        docs.append(doc)\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LTbfdWp9II1P"
      },
      "outputs": [],
      "source": [
        "### List of PDF files(All files) ###\n",
        "pdf_paths = [\"attention.pdf\", \"Multimodal.pdf\"]\n",
        "\n",
        "# Extract text from each PDF and create Document objects\n",
        "docs = extract_texts_from_pdfs(pdf_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TunwL3riOeQt"
      },
      "source": [
        "### Chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I2Uytk6VN2Ya"
      },
      "outputs": [],
      "source": [
        "def split_documents_into_chunks(docs, chunk_size=500, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Splits the given documents into chunks of specified size with overlap.\n",
        "\n",
        "    Args:\n",
        "        docs (list): List of documents to split.\n",
        "        chunk_size (int): Size of each chunk. Default is 500 characters.\n",
        "        chunk_overlap (int): Overlap size between chunks. Default is 100 characters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of lists containing split documents with chunks per original document.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "    doc_chunks = {}\n",
        "    for doc in docs:\n",
        "        doc_chunks[doc.metadata[\"source\"]] = text_splitter.split_documents([doc])\n",
        "    return doc_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6mdMWt5-N7Yx"
      },
      "outputs": [],
      "source": [
        "def add_chunk_numbers_to_metadata(doc_chunks):\n",
        "    \"\"\"\n",
        "    Adds chunk numbers to the metadata of each split document.\n",
        "\n",
        "    Args:\n",
        "        doc_chunks (dict): Dictionary of lists containing split documents.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of lists containing split documents with updated metadata.\n",
        "    \"\"\"\n",
        "    for chunks in doc_chunks.values():\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            chunk.metadata[\"chunk\"] = idx\n",
        "    return doc_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "clHnXEjoOFms"
      },
      "outputs": [],
      "source": [
        "# Split the documents into chunks\n",
        "doc_splits = split_documents_into_chunks(docs)\n",
        "# Add chunk number to metadata\n",
        "doc_splits = add_chunk_numbers_to_metadata(doc_splits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of splits per document\n",
        "for doc_source, chunks in doc_splits.items():\n",
        "    print(f'Document {doc_source} has {len(chunks):,} splits')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqvZ4Mv9IfVH",
        "outputId": "dc9f39b7-bf0a-432f-8457-161388c2d5e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document attention.pdf has 102 splits\n",
            "Document Multimodal.pdf has 232 splits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first 3 chunks for each document\n",
        "for doc_source, chunks in doc_splits.items():\n",
        "    print(f'--- Document: {doc_source} ---')\n",
        "    for split in chunks[:3]:\n",
        "        print(f'---Page Content---\\n{split.page_content}')\n",
        "        print(f'Metadata:\\n{split.metadata}')\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeLgqFdQLTGl",
        "outputId": "e0d1f211-b71d-41c0-e932-695d94fe83dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Document: attention.pdf ---\n",
            "---Page Content---\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.com\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "nikip@google.com\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.com\n",
            "Aidan N. Gomez∗†\n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "Łukasz Kaiser∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "Metadata:\n",
            "{'source': 'attention.pdf', 'chunk': 0}\n",
            "\n",
            "---Page Content---\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "Metadata:\n",
            "{'source': 'attention.pdf', 'chunk': 1}\n",
            "\n",
            "---Page Content---\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
            "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
            "to-German translation task, improving over the existing best results, including\n",
            "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
            "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
            "Metadata:\n",
            "{'source': 'attention.pdf', 'chunk': 2}\n",
            "\n",
            "--- Document: Multimodal.pdf ---\n",
            "---Page Content---\n",
            "Visual Instruction Tuning\n",
            "Haotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yong Jae Lee1\n",
            "1University of Wisconsin–Madison\n",
            "2Microsoft Research\n",
            "3Columbia University\n",
            "https://llava-vl.github.io\n",
            "Abstract\n",
            "Instruction tuning large language models (LLMs) using machine-generated\n",
            "instruction-following data has been shown to improve zero-shot capabilities on\n",
            "new tasks, but the idea is less explored in the multimodal field. We present the\n",
            "Metadata:\n",
            "{'source': 'Multimodal.pdf', 'chunk': 0}\n",
            "\n",
            "---Page Content---\n",
            "new tasks, but the idea is less explored in the multimodal field. We present the\n",
            "first attempt to use language-only GPT-4 to generate multimodal language-image\n",
            "instruction-following data. By instruction tuning on such generated data, we in-\n",
            "troduce LLaVA: Large Language and Vision Assistant, an end-to-end trained\n",
            "large multimodal model that connects a vision encoder and an LLM for general-\n",
            "purpose visual and language understanding. To facilitate future research on visual\n",
            "Metadata:\n",
            "{'source': 'Multimodal.pdf', 'chunk': 1}\n",
            "\n",
            "---Page Content---\n",
            "purpose visual and language understanding. To facilitate future research on visual\n",
            "instruction following, we construct two evaluation benchmarks with diverse and\n",
            "challenging application-oriented tasks. Our experiments show that LLaVA demon-\n",
            "strates impressive multimodal chat abilities, sometimes exhibiting the behaviors\n",
            "of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela-\n",
            "tive score compared with GPT-4 on a synthetic multimodal instruction-following\n",
            "Metadata:\n",
            "{'source': 'Multimodal.pdf', 'chunk': 2}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ila_HXSxdOIb"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "ccb73b4931cf4efd9a2358dc5cd7e916",
            "d378f437666741e295a9b4e62b5ceb46",
            "21e6e431eef44720a2abecaccd835a44",
            "5380a0aa338143a5b852d26bd1363b0a",
            "f1411e7803eb441294d14ef487a21dcf",
            "46a87ed03b7549d68be7caf91c64c9ad",
            "76b15f9ec74545388b02853a1b79f0f8",
            "570a7f1304d5440385bbece3b1b711a3",
            "b2cace10eda446d79b53f746ffc501d1",
            "79ac7d3aa3124cce9882b7e7fbedaa0d",
            "cad815d959d84636894c1fdb0ce04288",
            "7fecde8075c24b29a129410533b6203b",
            "b5a3aa82f5784664b1e1edefba2e77e5",
            "0288c06b424b461c82052fb7e44ade62",
            "92a67c43246c48d4a7d6a4e91aad56ab",
            "575e8d160c174501acacff3d83b0fc16",
            "d415b4246fbe473fa8c857347be0d34d",
            "6e8c9637c57247b7a1875574d52e9ff6",
            "8a45626f1ad140c8a4d7a536985222ea",
            "0398ae2a1e984b4a85211863c7686572",
            "dec299b8ce1546a1be1e395a48866a2b",
            "8f6ab19d7bb24e06ab3c8a47048a9db7",
            "89da5fea3b8b4003987155cdd160176e",
            "34700ace27ed4be9aafab09a048c0718",
            "9d7d37baadc4406e8f9c2d25b67c6995",
            "ab2aef1cc583411a918766ce4c485318",
            "0aad5d8efac54d908a6cf1aa94a1e869",
            "aadaeeda0b414cb48554d5c3417a5031",
            "770070842ad348138f5db2ca1b345554",
            "3e057398912648a5b609141e403bdf7a",
            "c06a04de0b5f43c6bd5d7283502cdd18",
            "a3326b9e3fab42d8997b39ebf4bf9898",
            "e25056997aae4d1482d030a9a0222e33"
          ]
        },
        "id": "i9IFXIy2ZsPT",
        "outputId": "5cdc5f44-1ac9-4d52-c503-c6606a84f5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccb73b4931cf4efd9a2358dc5cd7e916"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fecde8075c24b29a129410533b6203b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89da5fea3b8b4003987155cdd160176e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# embedding model - Stronger model can be considered\n",
        "\n",
        "# SciBERT(Allen Institute for AI) - for academic(science) paper including computer science - maximum 512 tokens\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # small model(microsoft)\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"roberta-large\") # RoBERTa - large (facebook)　- Longer context\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"roberta-base\") # RoBERTa - base(facebook)　- Longer context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwVMT3Tdd2-m"
      },
      "source": [
        "### Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sFILjzeod616"
      },
      "outputs": [],
      "source": [
        "def configure_faiss_vector_store(doc_splits, embeddings):\n",
        "    \"\"\"\n",
        "    Configures FAISS as the vector store using the provided document splits and embeddings.\n",
        "\n",
        "    Args:\n",
        "        doc_splits (dict): Dictionary of lists containing split documents.\n",
        "        embeddings (Embeddings): Embeddings to be used for FAISS.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of FAISS vector stores per document.\n",
        "    \"\"\"\n",
        "    vector_stores = {}\n",
        "    for doc_source, chunks in doc_splits.items():\n",
        "        vector_stores[doc_source] = FAISS.from_documents(chunks, embeddings)\n",
        "    return vector_stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KGA2967RJCF",
        "outputId": "832cf86c-e232-44b7-b5c7-81c428865bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4min 13s, sys: 13.6 s, total: 4min 26s\n",
            "Wall time: 4min 29s\n"
          ]
        }
      ],
      "source": [
        "# Configure FAISS as Vector Store\n",
        "%%time\n",
        "vector_db = configure_faiss_vector_store(doc_splits, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5pHQZiJPFAj",
        "outputId": "6893b90f-d7db-4c54-b632-7e2db4da08df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention.pdf': <langchain_community.vectorstores.faiss.FAISS at 0x7fbf517e2620>,\n",
              " 'Multimodal.pdf': <langchain_community.vectorstores.faiss.FAISS at 0x7fbf517e3a90>}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFmu05HlQ2AQ",
        "outputId": "73d4b21b-48d9-4cb0-90a1-396e30953c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents in the FAISS index for attention.pdf: 102\n",
            "Number of documents in the FAISS index for Multimodal.pdf: 232\n"
          ]
        }
      ],
      "source": [
        "# print(\"Number of documents in the FAISS index:\", vector_db.index.ntotal)\n",
        "# Print number of documents in the FAISS index for each document\n",
        "for doc_source, faiss_index in vector_db.items():\n",
        "    print(f\"Number of documents in the FAISS index for {doc_source}: {faiss_index.index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XaIWe3NKj3Kn"
      },
      "outputs": [],
      "source": [
        "def create_retrievers(vector_stores, search_type=\"similarity\", k=5):\n",
        "    \"\"\"\n",
        "    Exposes the vector store index to retrievers for multiple documents.\n",
        "\n",
        "    Args:\n",
        "        vector_stores (dict): Dictionary of FAISS vector stores per document.\n",
        "        search_type (str): The type of search to perform. Default is \"similarity\".\n",
        "        k (int): The number of documents to return. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of retrievers per document.\n",
        "    \"\"\"\n",
        "    retrievers = {}\n",
        "    for doc_source, vector_store in vector_stores.items():\n",
        "        retrievers[doc_source] = vector_store.as_retriever(\n",
        "            search_type=search_type, search_kwargs={\"k\": k}\n",
        "        )\n",
        "    return retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "y2AsKeLkUzPG"
      },
      "outputs": [],
      "source": [
        "# Create retrievers for each document and store them in a dictionary\n",
        "retrievers = create_retrievers(vector_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWRMcD9FZ4I0"
      },
      "source": [
        "### Retreive contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6wfFIy4NptWn"
      },
      "outputs": [],
      "source": [
        "def process_query(query: str, retriever):\n",
        "    \"\"\"\n",
        "    Processes the query using the provided retriever to retrieve relevant document chunks.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string to search for relevant documents.\n",
        "        retriever: The retriever object configured to use the vector store for document retrieval.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the formatted content and metadata of the retrieved document chunks.\n",
        "    \"\"\"\n",
        "    # Retrieve chunks based on the query\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "    # Initialize an empty string to collect all outputs\n",
        "    full_output = \"\"\n",
        "\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        chunk_output = f\"-----Chunk {i}------\\n\"\n",
        "        chunk_output += f\"Content: {doc.page_content}...\\n\"\n",
        "        chunk_output += f\"Metadata {doc.metadata}\\n\\n\"\n",
        "\n",
        "        # Append the chunk output to the full output\n",
        "        full_output += chunk_output\n",
        "\n",
        "    return full_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9Bpi0z79V5BE"
      },
      "outputs": [],
      "source": [
        "# Sample Query\n",
        "\n",
        "# query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
        "# query = \"What is the regularization addressed in the academic article?\"\n",
        "query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
        "# query = \"What is the Attention addressed in the academic article?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVknk8M3WCu1",
        "outputId": "970dc9b7-4a51-4871-fc45-d694c442cafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Chunk 1------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
            "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
            "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "15...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 101}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: length n is smaller than the representation dimensionality d, which is most often the case with\n",
            "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
            "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
            "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
            "6\n",
            "the input sequence centered around the respective output position. This would increase the maximum...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 47}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 5}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
            "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
            "and 6. Note that the attentions are very sharp for this word.\n",
            "14\n",
            "Input-Input Layer5\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 99}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Retrieve chunks　from the first document\n",
        "# retriever = retrievers[\"attention.pdf\"]\n",
        "retriever = retrievers[pdf_paths[0]]\n",
        "retrieved_output = process_query(query, retriever)\n",
        "\n",
        "# Print chunks\n",
        "print(retrieved_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzkVKKDnV5u9"
      },
      "source": [
        "### Setup LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Vu0c9G5iV5V3"
      },
      "outputs": [],
      "source": [
        "# pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bNGW60wSZTIX"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "\n",
        "def get_groq_response(client, prompt, model=\"llama3-70b-8192\", max_tokens=2048, temperature=0.0):\n",
        "    \"\"\"\n",
        "    Generates a response using the provided client, model, prompt, and specified parameters.\n",
        "\n",
        "    Args:\n",
        "        client: The client object to interact with the API.\n",
        "        prompt (str): The prompt to generate a response for.\n",
        "        model (str, optional): The model identifier to use for generating the response. Default is \"llama3-70b-8192\".\n",
        "        max_tokens (int, optional): The maximum number of tokens for the generated response. Default is 2048.\n",
        "        temperature (float, optional): The temperature setting for the response generation. Default is 0.0.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The generated response content and usage statistics.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ],\n",
        "            model=model,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content, chat_completion.usage\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mCONlYHIaGef"
      },
      "outputs": [],
      "source": [
        "client = Groq(\n",
        "    api_key=\"YOUR_API_KEY\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUyHPJ5lWDi0",
        "outputId": "29e90fa0-b05c-4f03-cced-dfe6be5f132d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello\"\n",
        "response = get_groq_response(client, prompt)\n",
        "print(response[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT_skBCedxYk"
      },
      "source": [
        "### RAG - HyDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zHlgNqUCoNiz"
      },
      "outputs": [],
      "source": [
        "# prompt for RAG - HyDE\n",
        "instruction_hyde = \"\"\"\n",
        "### Instructions ###\n",
        "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" below seems relevant to \"Users' query\", please refer to it.\n",
        "\n",
        "### User’s query ###\n",
        "{USER_QUERY}\n",
        "\n",
        "### Context ###\n",
        "{CONTEXT_HYDE}\n",
        "\n",
        "### Output ###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fIbjqv0-aH75"
      },
      "outputs": [],
      "source": [
        "def generate_prompt_hyde(instruction, user_query, context_hyde):\n",
        "    \"\"\"\n",
        "    Generates a prompt for HyDE by replacing placeholders in the instruction template with the user's query and context.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing placeholders.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "        context_hyde (str): The context for creating a hypothetical answer to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholders replaced by the user's query and context.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    instruction = instruction.replace(\"{CONTEXT_HYDE}\", context_hyde)\n",
        "    return instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1euLNgPeaXSp"
      },
      "outputs": [],
      "source": [
        "# user_query = \"What is the regularization addressed in the academic article?\"\n",
        "user_query = \"What is the main hypothesis or research question addressed in the first academic article?\"\n",
        "# user_query  = \"What is the Attention addressed in the academic article?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "grkR3S8ee5Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef44c91-4d96-478c-f5e4-5513cbd1aff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper \"Attention Is All You Need\" presents a new neural network architecture called the Transformer, designed for sequence transduction tasks like machine translation. The key innovation of the Transformer is that it relies entirely on attention mechanisms to draw global dependencies between input and output, eschewing the recurrence and convolution operations used in previous state-of-the-art models.\n",
            "\n",
            "The authors motivate the use of self-attention by highlighting three desirable characteristics: 1) reduced sequential computation, allowing more parallelization, 2) shorter path lengths between long-range dependencies in the network, and 3) more interpretable attention patterns. \n",
            "\n",
            "The Transformer architecture consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The attention mechanism used is \"scaled dot-product attention\", which the authors found to be faster and more space-efficient than additive attention. They also introduce \"multi-head attention\", which allows the model to jointly attend to information from different representation subspaces.\n",
            "\n",
            "To enable the model to make use of the order of the sequence, the authors add \"positional encodings\" to the input embeddings. They experimented with both learned and fixed sinusoidal positional encodings, finding similar results.\n",
            "\n",
            "The Transformer achieves state-of-the-art results on the WMT 2014 English-to-German and English-to-French translation tasks, outperforming previous models including recurrent and convolutional architectures. Notably, it reaches these results with significantly reduced training time - the base model can be trained on 8 GPUs in about 12 hours.\n",
            "\n",
            "The authors conduct ablation studies to analyze the impact of various components, finding that both the multi-head attention mechanism and the use of very large inner-layer dimensionality in the position-wise feed-forward networks are important for performance.\n",
            "\n",
            "To demonstrate the Transformer's ability to generalize to other tasks, the authors also apply it to English constituency parsing. With minimal task-specific tuning, it outperforms several baseline models on this task.\n",
            "\n",
            "The paper concludes by highlighting the Transformer's potential for efficiently handling large inputs and outputs, as well as the possibility of making sequence generation less sequential. The authors express excitement about future applications of attention-based models to other modalities beyond text.\n",
            "\n",
            "Overall, the Transformer represents a significant advance in sequence transduction models, demonstrating that attention mechanisms alone can replace the recurrent and convolutional layers commonly used in previous architectures, while providing benefits in terms of training efficiency and performance.\n"
          ]
        }
      ],
      "source": [
        "# read sample summary table(for attention.pdf)\n",
        "import pandas as pd\n",
        "table_summary = pd.read_csv('summaries.csv')\n",
        "context_hyde = table_summary['description'].iloc[0]\n",
        "\n",
        "print(context_hyde)\n",
        "# sample - abstract\n",
        "# context_hyde = \"\"\"\n",
        "# Abstract\n",
        "# The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPoASCIMbOXG",
        "outputId": "4c71cb74-d9f0-4d20-f48e-27f41b63334f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Instructions ###\n",
            "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" below seems relevant to \"Users' query\", please refer to it.\n",
            "\n",
            "### User’s query ###\n",
            "What is the main hypothesis or research question addressed in the first academic article?\n",
            "\n",
            "### Context ###\n",
            "The paper \"Attention Is All You Need\" presents a new neural network architecture called the Transformer, designed for sequence transduction tasks like machine translation. The key innovation of the Transformer is that it relies entirely on attention mechanisms to draw global dependencies between input and output, eschewing the recurrence and convolution operations used in previous state-of-the-art models.\n",
            "\n",
            "The authors motivate the use of self-attention by highlighting three desirable characteristics: 1) reduced sequential computation, allowing more parallelization, 2) shorter path lengths between long-range dependencies in the network, and 3) more interpretable attention patterns. \n",
            "\n",
            "The Transformer architecture consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The attention mechanism used is \"scaled dot-product attention\", which the authors found to be faster and more space-efficient than additive attention. They also introduce \"multi-head attention\", which allows the model to jointly attend to information from different representation subspaces.\n",
            "\n",
            "To enable the model to make use of the order of the sequence, the authors add \"positional encodings\" to the input embeddings. They experimented with both learned and fixed sinusoidal positional encodings, finding similar results.\n",
            "\n",
            "The Transformer achieves state-of-the-art results on the WMT 2014 English-to-German and English-to-French translation tasks, outperforming previous models including recurrent and convolutional architectures. Notably, it reaches these results with significantly reduced training time - the base model can be trained on 8 GPUs in about 12 hours.\n",
            "\n",
            "The authors conduct ablation studies to analyze the impact of various components, finding that both the multi-head attention mechanism and the use of very large inner-layer dimensionality in the position-wise feed-forward networks are important for performance.\n",
            "\n",
            "To demonstrate the Transformer's ability to generalize to other tasks, the authors also apply it to English constituency parsing. With minimal task-specific tuning, it outperforms several baseline models on this task.\n",
            "\n",
            "The paper concludes by highlighting the Transformer's potential for efficiently handling large inputs and outputs, as well as the possibility of making sequence generation less sequential. The authors express excitement about future applications of attention-based models to other modalities beyond text.\n",
            "\n",
            "Overall, the Transformer represents a significant advance in sequence transduction models, demonstrating that attention mechanisms alone can replace the recurrent and convolutional layers commonly used in previous architectures, while providing benefits in terms of training efficiency and performance.\n",
            "\n",
            "### Output ###\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create prompt for HyDE\n",
        "prompt_hyde = generate_prompt_hyde(instruction_hyde, user_query, context_hyde)\n",
        "print(prompt_hyde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BRDQsnScEGn",
        "outputId": "c366f755-82fb-468e-d7ca-03d7e31fe01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main hypothesis or research question addressed in the first academic article \"Attention Is All You Need\" is whether attention mechanisms alone can replace the recurrent and convolutional layers commonly used in previous sequence transduction models, and if so, what benefits this approach would bring in terms of training efficiency and performance.\n"
          ]
        }
      ],
      "source": [
        "# Get a hypothetical answer\n",
        "response = get_groq_response(client, prompt_hyde)\n",
        "print(response[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4aUE2DlcPcs",
        "outputId": "409ef858-dfaf-4b4e-c848-79eaa1681028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Chunk 1------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
            "entirely on self-attention to compute representations of its input and output without using sequence-\n",
            "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
            "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "3\n",
            "Model Architecture\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]....\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 16}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
            "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
            "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
            "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "2\n",
            "Background\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 11}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
            "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
            "during training.\n",
            "4\n",
            "Why Self-Attention\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
            "tional layers commonly used for mapping one variable-length sequence of symbol representations...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 42}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3\n",
            "Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 31}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find chunks based on similarity\n",
        "print(process_query(response[0], retrievers[pdf_paths[0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjelMdOQlZqA"
      },
      "source": [
        "### Extract thesis/figure/table numbers from user's query and search descriptions based on numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to another jupyter notebook for the detail  \n",
        "https://github.com/daichi6/llm-hackathon-insightai/blob/main/notebooks/extract_query.ipynb"
      ],
      "metadata": {
        "id": "OkVpGhqn5OVB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zNAmmZ-XnrON"
      },
      "outputs": [],
      "source": [
        "# sample figure/image description\n",
        "context_figure_table = ['description: --']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ARxr5weFqdG"
      },
      "source": [
        "### Provide a response given retreived contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Rfi5cWwBF22n"
      },
      "outputs": [],
      "source": [
        "instruction_final = \"\"\"\n",
        "### Instructions ###\n",
        "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
        "If the information in the \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them.\n",
        "\"Text Context\" includes several chunks from different parts of an academic paper. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper.\n",
        "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response.\n",
        "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
        "If you determine that the previous conversation history is relevant, please also refer to that information to answer the user's query.　Especially when the the contexts below are empty, please answer the user's most recent query　based on the conversation history(the user's previous queries and your responses).\n",
        "If the conversation is continuing from the previous session and no additional information is needed, you may refer to the previous conversation history and might not need to use the contexts below. (e.g., User's query: Please make your response brief).\n",
        "If the contexts and the previous conversation history do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
        "\n",
        "##### User’s query #####\n",
        "{USER_QUERY}\n",
        "\n",
        "\n",
        "##### Figure/Table Context #####\n",
        "{CONTEXT_FIGURE_TABLE}\n",
        "\n",
        "##### Text Context #####\n",
        "{CONTEXT_RAG_HYDE}\n",
        "\n",
        "{CONTEXT_RAG_GENERAL}\n",
        "\n",
        "\n",
        "##### Output #####\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KhZEVdrUHd0T"
      },
      "outputs": [],
      "source": [
        "def generate_prompt_final(instruction, user_query, context_figure_table, context_rag_hyde, context_rag_general):\n",
        "    \"\"\"\n",
        "    Generates a final prompt by replacing placeholders in the instruction template with the user's query and various contexts.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing placeholders.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "        context_figure_table (str): The context(description) related to figure and table to be inserted into the instruction.\n",
        "        context_rag_hyde (str): The context retreived from RAG HyDE to be inserted into the instruction.\n",
        "        context_rag_general (str): The general context retreived from RAG to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholders replaced by the user's query and contexts.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    instruction = instruction.replace(\"{CONTEXT_FIGURE_TABLE}\", context_figure_table)\n",
        "    instruction = instruction.replace(\"{CONTEXT_RAG_HYDE}\", context_rag_hyde)\n",
        "    instruction = instruction.replace(\"{CONTEXT_RAG_GENERAL}\", context_rag_general)\n",
        "    return instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLBGJk3sKTfN",
        "outputId": "d4292a82-48bc-42a2-d4ab-53cda62f2fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Chunk 1------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
            "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
            "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "15...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 101}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: length n is smaller than the representation dimensionality d, which is most often the case with\n",
            "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
            "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
            "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
            "6\n",
            "the input sequence centered around the respective output position. This would increase the maximum...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 47}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 5}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
            "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
            "and 6. Note that the attentions are very sharp for this word.\n",
            "14\n",
            "Input-Input Layer5\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 99}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create contexts - RAG(General)\n",
        "context_rag_general = process_query(user_query, retrievers[pdf_paths[0]])\n",
        "print(context_rag_general)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AQip0tgIjh3",
        "outputId": "2fdc1b71-6a05-474f-da14-5a49f7848c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Chunk 1------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
            "entirely on self-attention to compute representations of its input and output without using sequence-\n",
            "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
            "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "3\n",
            "Model Architecture\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]....\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 16}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
            "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
            "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
            "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "2\n",
            "Background\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 11}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
            "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
            "during training.\n",
            "4\n",
            "Why Self-Attention\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
            "tional layers commonly used for mapping one variable-length sequence of symbol representations...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 42}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3\n",
            "Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 31}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create contexts - RAG(Hyde)\n",
        "context_rag_hyde = process_query(response[0], retrievers[pdf_paths[0]])\n",
        "print(context_rag_hyde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ZqJDG5Ifz_",
        "outputId": "7977dfa5-15a6-4d81-a76b-d2a4c4e186e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Instructions ###\n",
            "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
            "If the information in the \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them.\n",
            "\"Text Context\" includes several chunks from different parts of an academic paper. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper.\n",
            "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response.\n",
            "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
            "If you determine that the previous conversation history is relevant, please also refer to that information to answer the user's query.　Especially when the the contexts below are empty, please answer the user's most recent query　based on the conversation history(the user's previous queries and your responses).\n",
            "If the conversation is continuing from the previous session and no additional information is needed, you may refer to the previous conversation history and might not need to use the contexts below. (e.g., User's query: Please make your response brief).\n",
            "If the contexts and the previous conversation history do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
            "\n",
            "##### User’s query #####\n",
            "What is the main hypothesis or research question addressed in the first academic article?\n",
            "\n",
            "\n",
            "##### Figure/Table Context #####\n",
            "['description: --']\n",
            "\n",
            "##### Text Context #####\n",
            "-----Chunk 1------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
            "entirely on self-attention to compute representations of its input and output without using sequence-\n",
            "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
            "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "3\n",
            "Model Architecture\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]....\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 16}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
            "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
            "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
            "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "2\n",
            "Background\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 11}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
            "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
            "during training.\n",
            "4\n",
            "Why Self-Attention\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
            "tional layers commonly used for mapping one variable-length sequence of symbol representations...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 42}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3\n",
            "Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 31}\n",
            "\n",
            "\n",
            "\n",
            "-----Chunk 1------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
            "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
            "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "15...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 101}\n",
            "\n",
            "-----Chunk 2------\n",
            "Content: length n is smaller than the representation dimensionality d, which is most often the case with\n",
            "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
            "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
            "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
            "6\n",
            "the input sequence centered around the respective output position. This would increase the maximum...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 47}\n",
            "\n",
            "-----Chunk 3------\n",
            "Content: has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 5}\n",
            "\n",
            "-----Chunk 4------\n",
            "Content: opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be\n",
            "just\n",
            "-\n",
            "this\n",
            "is\n",
            "what\n",
            "we\n",
            "are\n",
            "missing\n",
            ",\n",
            "in\n",
            "my\n",
            "opinion\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
            "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
            "and 6. Note that the attentions are very sharp for this word.\n",
            "14\n",
            "Input-Input Layer5\n",
            "The\n",
            "Law\n",
            "will\n",
            "never\n",
            "be\n",
            "perfect\n",
            ",\n",
            "but\n",
            "its\n",
            "application\n",
            "should\n",
            "be...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 99}\n",
            "\n",
            "-----Chunk 5------\n",
            "Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead...\n",
            "Metadata {'source': 'attention.pdf', 'chunk': 10}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "##### Output #####\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create prompt for a final response\n",
        "prompt_final = generate_prompt_final(instruction_final, user_query, str(context_figure_table), context_rag_hyde, context_rag_general)\n",
        "print(prompt_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMxrgB4xJSVx",
        "outputId": "5dea8f28-c7c5-4cc6-a9ef-a67bb5613274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main hypothesis or research question addressed in the first academic article is: Can a model architecture that relies entirely on self-attention mechanisms, without using recurrent networks or convolution, be effective for sequence modeling and transduction tasks?\n"
          ]
        }
      ],
      "source": [
        "# Get final response\n",
        "response = get_groq_response(client, prompt_final)\n",
        "print(response[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ccb73b4931cf4efd9a2358dc5cd7e916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d378f437666741e295a9b4e62b5ceb46",
              "IPY_MODEL_21e6e431eef44720a2abecaccd835a44",
              "IPY_MODEL_5380a0aa338143a5b852d26bd1363b0a"
            ],
            "layout": "IPY_MODEL_f1411e7803eb441294d14ef487a21dcf"
          }
        },
        "d378f437666741e295a9b4e62b5ceb46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a87ed03b7549d68be7caf91c64c9ad",
            "placeholder": "​",
            "style": "IPY_MODEL_76b15f9ec74545388b02853a1b79f0f8",
            "value": "config.json: 100%"
          }
        },
        "21e6e431eef44720a2abecaccd835a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570a7f1304d5440385bbece3b1b711a3",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2cace10eda446d79b53f746ffc501d1",
            "value": 385
          }
        },
        "5380a0aa338143a5b852d26bd1363b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79ac7d3aa3124cce9882b7e7fbedaa0d",
            "placeholder": "​",
            "style": "IPY_MODEL_cad815d959d84636894c1fdb0ce04288",
            "value": " 385/385 [00:00&lt;00:00, 9.80kB/s]"
          }
        },
        "f1411e7803eb441294d14ef487a21dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a87ed03b7549d68be7caf91c64c9ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76b15f9ec74545388b02853a1b79f0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "570a7f1304d5440385bbece3b1b711a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2cace10eda446d79b53f746ffc501d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79ac7d3aa3124cce9882b7e7fbedaa0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad815d959d84636894c1fdb0ce04288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fecde8075c24b29a129410533b6203b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5a3aa82f5784664b1e1edefba2e77e5",
              "IPY_MODEL_0288c06b424b461c82052fb7e44ade62",
              "IPY_MODEL_92a67c43246c48d4a7d6a4e91aad56ab"
            ],
            "layout": "IPY_MODEL_575e8d160c174501acacff3d83b0fc16"
          }
        },
        "b5a3aa82f5784664b1e1edefba2e77e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d415b4246fbe473fa8c857347be0d34d",
            "placeholder": "​",
            "style": "IPY_MODEL_6e8c9637c57247b7a1875574d52e9ff6",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "0288c06b424b461c82052fb7e44ade62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a45626f1ad140c8a4d7a536985222ea",
            "max": 442221694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0398ae2a1e984b4a85211863c7686572",
            "value": 442221694
          }
        },
        "92a67c43246c48d4a7d6a4e91aad56ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dec299b8ce1546a1be1e395a48866a2b",
            "placeholder": "​",
            "style": "IPY_MODEL_8f6ab19d7bb24e06ab3c8a47048a9db7",
            "value": " 442M/442M [00:02&lt;00:00, 140MB/s]"
          }
        },
        "575e8d160c174501acacff3d83b0fc16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d415b4246fbe473fa8c857347be0d34d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8c9637c57247b7a1875574d52e9ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a45626f1ad140c8a4d7a536985222ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0398ae2a1e984b4a85211863c7686572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dec299b8ce1546a1be1e395a48866a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f6ab19d7bb24e06ab3c8a47048a9db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89da5fea3b8b4003987155cdd160176e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34700ace27ed4be9aafab09a048c0718",
              "IPY_MODEL_9d7d37baadc4406e8f9c2d25b67c6995",
              "IPY_MODEL_ab2aef1cc583411a918766ce4c485318"
            ],
            "layout": "IPY_MODEL_0aad5d8efac54d908a6cf1aa94a1e869"
          }
        },
        "34700ace27ed4be9aafab09a048c0718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aadaeeda0b414cb48554d5c3417a5031",
            "placeholder": "​",
            "style": "IPY_MODEL_770070842ad348138f5db2ca1b345554",
            "value": "vocab.txt: 100%"
          }
        },
        "9d7d37baadc4406e8f9c2d25b67c6995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e057398912648a5b609141e403bdf7a",
            "max": 227845,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c06a04de0b5f43c6bd5d7283502cdd18",
            "value": 227845
          }
        },
        "ab2aef1cc583411a918766ce4c485318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3326b9e3fab42d8997b39ebf4bf9898",
            "placeholder": "​",
            "style": "IPY_MODEL_e25056997aae4d1482d030a9a0222e33",
            "value": " 228k/228k [00:00&lt;00:00, 4.75MB/s]"
          }
        },
        "0aad5d8efac54d908a6cf1aa94a1e869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aadaeeda0b414cb48554d5c3417a5031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "770070842ad348138f5db2ca1b345554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e057398912648a5b609141e403bdf7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c06a04de0b5f43c6bd5d7283502cdd18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3326b9e3fab42d8997b39ebf4bf9898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e25056997aae4d1482d030a9a0222e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}