{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain-community\n",
        "# !pip install sentence-transformers\n",
        "# !pip install faiss-cpu\n",
        "# !pip install --upgrade langchain\n",
        "# !pip install fitz\n",
        "# !pip install PyMuPDF\n",
        "# !pip install groq"
      ],
      "metadata": {
        "id": "mw01kLe7RkCO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "import fitz\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from groq import Groq"
      ],
      "metadata": {
        "id": "rt-p2YyLRqBJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "def split_documents_into_chunks(docs, chunk_size=500, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Splits the given documents into chunks of specified size with overlap.\n",
        "\n",
        "    Args:\n",
        "        docs (list): List of documents to split.\n",
        "        chunk_size (int): Size of each chunk. Default is 500 characters.\n",
        "        chunk_overlap (int): Overlap size between chunks. Default is 100 characters.\n",
        "\n",
        "    Returns:\n",
        "        list: List of split documents with chunks.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "    return text_splitter.split_documents(docs)\n",
        "\n",
        "def add_chunk_numbers_to_metadata(doc_splits):\n",
        "    \"\"\"\n",
        "    Adds chunk numbers to the metadata of each split document.\n",
        "\n",
        "    Args:\n",
        "        doc_splits (list): List of split documents.\n",
        "\n",
        "    Returns:\n",
        "        list: List of split documents with updated metadata.\n",
        "    \"\"\"\n",
        "    for idx, split in enumerate(doc_splits):\n",
        "        split.metadata[\"chunk\"] = idx\n",
        "    return doc_splits\n",
        "\n",
        "def configure_faiss_vector_store(doc_splits, embeddings):\n",
        "    \"\"\"\n",
        "    Configures FAISS as the vector store using the provided document splits and embeddings.\n",
        "\n",
        "    Args:\n",
        "        doc_splits (list): List of split documents.\n",
        "        embeddings (Embeddings): Embeddings to be used for FAISS.\n",
        "\n",
        "    Returns:\n",
        "        FAISS: Configured FAISS vector store.\n",
        "    \"\"\"\n",
        "    return FAISS.from_documents(doc_splits, embeddings)\n",
        "\n",
        "def create_retriever(vector_store, search_type=\"similarity\", k=5):\n",
        "    \"\"\"\n",
        "    Exposes the vector store index to a retriever.\n",
        "\n",
        "    Args:\n",
        "        vector_store: The vector store (e.g., FAISS, Annoy, etc.).\n",
        "        search_type (str): The type of search to perform. Default is \"similarity\".\n",
        "        k (int): The number of documents to return. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        retriever: The configured retriever.\n",
        "    \"\"\"\n",
        "    return vector_store.as_retriever(\n",
        "        search_type=search_type, search_kwargs={\"k\": k}\n",
        "    )\n",
        "\n",
        "def process_query(query: str, retriever):\n",
        "    \"\"\"\n",
        "    Processes the query using the provided retriever to retrieve relevant document chunks.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string to search for relevant documents.\n",
        "        retriever: The retriever object configured to use the vector store for document retrieval.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the formatted content and metadata of the retrieved document chunks.\n",
        "    \"\"\"\n",
        "    # Retrieve chunks based on the query\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "    # Initialize an empty string to collect all outputs\n",
        "    full_output = \"\"\n",
        "\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        chunk_output = f\"-----Chunk {i}------\\n\"\n",
        "        chunk_output += f\"Content: {doc.page_content}...\\n\"\n",
        "        chunk_output += f\"Metadata {doc.metadata}\\n\\n\"\n",
        "\n",
        "        # Append the chunk output to the full output\n",
        "        full_output += chunk_output\n",
        "\n",
        "    return full_output\n",
        "\n",
        "def get_groq_response(client, prompt, model=\"llama3-70b-8192\", max_tokens=2048, temperature=0.0):\n",
        "    \"\"\"\n",
        "    Generates a response using the provided client, model, prompt, and specified parameters.\n",
        "\n",
        "    Args:\n",
        "        client: The client object to interact with the API.\n",
        "        prompt (str): The prompt to generate a response for.\n",
        "        model (str, optional): The model identifier to use for generating the response. Default is \"llama3-70b-8192\".\n",
        "        max_tokens (int, optional): The maximum number of tokens for the generated response. Default is 2048.\n",
        "        temperature (float, optional): The temperature setting for the response generation. Default is 0.0.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The generated response content and usage statistics.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ],\n",
        "            model=model,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content, chat_completion.usage\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def generate_prompt_hyde(instruction, user_query, context_hyde):\n",
        "    \"\"\"\n",
        "    Generates a prompt for HyDE by replacing placeholders in the instruction template with the user's query and context.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing placeholders.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "        context_hyde (str): The context for creating a hypothetical answer to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholders replaced by the user's query and context.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    instruction = instruction.replace(\"{CONTEXT_HYDE}\", context_hyde)\n",
        "    return instruction\n",
        "\n",
        "def generate_prompt_final(instruction, user_query, context_figure_table, context_rag_hyde, context_rag_general):\n",
        "    \"\"\"\n",
        "    Generates a final prompt by replacing placeholders in the instruction template with the user's query and various contexts.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing placeholders.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "        context_figure_table (str): The context(description) related to figure and table to be inserted into the instruction.\n",
        "        context_rag_hyde (str): The context retreived from RAG HyDE to be inserted into the instruction.\n",
        "        context_rag_general (str): The general context retreived from RAG to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholders replaced by the user's query and contexts.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    instruction = instruction.replace(\"{CONTEXT_FIGURE_TABLE}\", context_figure_table)\n",
        "    instruction = instruction.replace(\"{CONTEXT_RAG_HYDE}\", context_rag_hyde)\n",
        "    instruction = instruction.replace(\"{CONTEXT_RAG_GENERAL}\", context_rag_general)\n",
        "    return instruction\n",
        "\n",
        "def generate_prompt_extract_query(instruction, user_query):\n",
        "    \"\"\"\n",
        "    Generates a prompt for extracting keys from the user's query by replacing placeholders in the instruction template.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing a placeholder for the user's query.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholder replaced by the user's query.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    return instruction\n",
        "\n",
        "def parse_and_convert_keys(json_string):\n",
        "    \"\"\"\n",
        "    Parse the JSON string and convert the string values in the keys list to their appropriate types.\n",
        "\n",
        "    Args:\n",
        "    json_string (str): A JSON string representing a list of dictionaries with string values for 'thesis', 'figure', and 'table'.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of dictionaries with 'thesis' as int, and 'figure' and 'table' as int or None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        keys = json.loads(json_string)\n",
        "        if not keys:\n",
        "            return []\n",
        "\n",
        "        converted_keys = []\n",
        "        for key in keys:\n",
        "            converted_key = {\n",
        "                \"thesis\": int(key[\"thesis\"]) if key[\"thesis\"] else None,\n",
        "                \"figure\": int(key[\"figure\"]) if key[\"figure\"] else None,\n",
        "                \"table\": int(key[\"table\"]) if key[\"table\"] else None\n",
        "            }\n",
        "            converted_keys.append(converted_key)\n",
        "        return converted_keys\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decoding error: {e}\")\n",
        "        return []\n",
        "    except KeyError as e:\n",
        "        print(f\"Missing key in JSON data: {e}\")\n",
        "        return []\n",
        "    except ValueError as e:\n",
        "        print(f\"Value error: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_descriptions(df, keys):\n",
        "    \"\"\"\n",
        "    Extract and format descriptions from the dataframe based on the provided keys.\n",
        "\n",
        "    Args:\n",
        "    df (DataFrame): The dataframe containing thesis, figure, table, and description data.\n",
        "    keys (list): A list of dictionaries with 'thesis' as int, and 'figure' and 'table' as int or None.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of formatted descriptions corresponding to the provided keys.\n",
        "    \"\"\"\n",
        "    formatted_descriptions = []\n",
        "\n",
        "    for key in keys:\n",
        "        thesis_num = key[\"thesis\"]\n",
        "        figure_num = key[\"figure\"]\n",
        "        table_num = key[\"table\"]\n",
        "\n",
        "        if figure_num is not None:\n",
        "            description = df[(df[\"thesis_num\"] == thesis_num) & (df[\"figure_num\"] == figure_num)][\"description\"].values\n",
        "            prefix = f\"thesis{thesis_num} figure{figure_num} description: \"\n",
        "        elif table_num is not None:\n",
        "            description = df[(df[\"thesis_num\"] == thesis_num) & (df[\"table_num\"] == table_num)][\"description\"].values\n",
        "            prefix = f\"thesis{thesis_num} table{table_num} description: \"\n",
        "        else:\n",
        "            description = []\n",
        "            prefix = \"\"\n",
        "\n",
        "        if len(description) > 0:\n",
        "            formatted_descriptions.append(prefix + description[0])\n",
        "        else:\n",
        "            formatted_descriptions.append(prefix + \"Description not found\")\n",
        "\n",
        "    return formatted_descriptions\n",
        "\n",
        "def extract_thesis_numbers(converted_keys):\n",
        "    \"\"\"\n",
        "    Extracts the thesis numbers from a list of dictionaries.\n",
        "\n",
        "    Args:\n",
        "    converted_keys (list): A list of dictionaries with 'thesis', 'figure', and 'table' keys.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of thesis numbers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        thesis_numbers = [item['thesis'] for item in converted_keys]\n",
        "        return thesis_numbers\n",
        "    except Exception as e:\n",
        "        # print(f\"An error occurred while extracting thesis numbers: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_descriptions_for_thesis_summary(thesis_numbers, table_summary):\n",
        "    \"\"\"\n",
        "    Retrieves the descriptions for the given thesis numbers from the table_summary DataFrame.\n",
        "\n",
        "    Args:\n",
        "    thesis_numbers (list): A list of thesis numbers.\n",
        "    table_summary (pd.DataFrame): The DataFrame containing thesis numbers and their descriptions.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of descriptions corresponding to the thesis numbers, formatted to indicate which thesis each description belongs to.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = []\n",
        "        for thesis_num in thesis_numbers:\n",
        "            description = table_summary.loc[table_summary['thesis_num'] == thesis_num, 'description'].values[0]\n",
        "            result.append(f\"Summary description for thesis {thesis_num}: '{description}'\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        # print(f\"An error occurred while retrieving descriptions: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "gB67f2VkSPRo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompts(Instructions)\n",
        "\n",
        "instruction_hyde = \"\"\"\n",
        "### Instructions ###\n",
        "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" below seems relevant to \"Users' query\", please refer to it.\n",
        "\n",
        "### User’s query ###\n",
        "{USER_QUERY}\n",
        "\n",
        "### Context ###\n",
        "{CONTEXT_HYDE}\n",
        "\n",
        "### Output ###\n",
        "\"\"\"\n",
        "\n",
        "instruction_final = \"\"\"\n",
        "### Instructions ###\n",
        "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
        "If the information in the \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them.\n",
        "\"Text Context\" includes several chunks from different parts of an academic paper. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper.\n",
        "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response.\n",
        "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
        "If you determine that the previous conversation history is relevant, please also refer to that information to answer the user's query.\n",
        "If the conversation is continuing from the previous session and no additional information is needed, you may refer to the previous conversation history and might not need to use the contexts below. (e.g., User's query: Please make your response brief).\n",
        "If the contexts and the previous conversation history do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
        "\n",
        "##### User’s query #####\n",
        "{USER_QUERY}\n",
        "\n",
        "\n",
        "##### Figure/Table Context #####\n",
        "{CONTEXT_FIGURE_TABLE}\n",
        "\n",
        "##### Text Context #####\n",
        "{CONTEXT_RAG_HYDE}\n",
        "\n",
        "{CONTEXT_RAG_GENERAL}\n",
        "\n",
        "\n",
        "##### Output #####\n",
        "\"\"\"\n",
        "\n",
        "instruction_extract_query = \"\"\"\n",
        "### Instructions ###\n",
        "You are an NLP engineer. Your task is to extract the \"numbers\" from the user's query below.\n",
        "The \"numbers\" mean which academic paper the user is referring to, 2) which figure the user is referring to, and 3) which table the user is referring to.\n",
        "There may be cases where all, some, or none of these are specified. Enter the number only for the specified fields, and return an empty string \"\" for fields that are not specified. Interpret \"figure\" for terms such as \"Chart,\" \"Diagram,\" or \"Image.\"\n",
        "Please provide your response as a list of objects, each containing thesis, figure, and table.　Please provide your response strictly in the specified format, without including any additional text for formatting. I will use your response directly.\n",
        "If it is unclear which thesis, figure, or table is being referred to, it is okay to return an empty string. Please do not make any assumptions.\n",
        "\n",
        "### Output Format ###\n",
        "Format: a list of objects\n",
        "\n",
        "### Example user's query1 ###\n",
        "What is the main hypothesis or research question addressed in the first academic article?\n",
        "\n",
        "### Example Output1 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"1\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query2 ###\n",
        "Summarize the methodology used in the third academic article. Highlight any unique approaches or techniques employed.\n",
        "\n",
        "### Example Output2 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"3\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "\n",
        "### Example user's query3 ###\n",
        "Q. From the images and figures in the second article, describe the trend shown in Figure 2. What does it indicate about the research findings?\n",
        "\n",
        "### Example Output3 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"2\",\n",
        "  \"figure\": \"2\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query4 ###\n",
        "Q. What can be understood from Image 3 in the third paper?\n",
        "\n",
        "### Example Output4 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"3\",\n",
        "  \"figure\": \"3\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query4 ###\n",
        "Q. Please explain Figure 3 and Table 2 of the second academic paper. What do these indicate about the research findings?\n",
        "\n",
        "### Example Output4 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"2\",\n",
        "  \"figure\": \"3\",\n",
        "  \"table\": \"\"\n",
        "  },\n",
        "  {\n",
        "  \"thesis\": \"2\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"2\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query5 ###\n",
        "Q. Please compare table 3 and chart 4 from the second and third theses, respectively.\n",
        "\n",
        "### Example Output5 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"2\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"3\"\n",
        "  },\n",
        "  {\n",
        "  \"thesis\": \"3\",\n",
        "  \"figure\": \"4\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query6 ###\n",
        "Do you like an apple?\n",
        "\n",
        "### Example Output6 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "\n",
        "### User’s query ###\n",
        "{USER_QUERY}\n",
        "\n",
        "### Output ###\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "tGYKP0lnTIXe"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main - Prepare data/databases\n",
        "\n",
        "## Load PDF\n",
        "# Load Sample PDF　- Replace with your path\n",
        "pdf_path = \"attention.pdf\"\n",
        "# text to document\n",
        "text = extract_text(pdf_path)\n",
        "doc = Document(page_content=text)\n",
        "docs = [doc]\n",
        "\n",
        "## Chunk\n",
        "# Split the documents into chunks\n",
        "doc_splits = split_documents_into_chunks(docs)\n",
        "# Add chunk number to metadata\n",
        "doc_splits = add_chunk_numbers_to_metadata(doc_splits)\n",
        "\n",
        "## Embedding\n",
        "# SciBERT(Allen Institute for AI) - for academic(science) paper including computer science - maximum 512 tokens\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "## Vector Store\n",
        "# Configure FAISS as Vector Store\n",
        "# !!!!!!!!!!!!!!!!!!!! Need to keep the document number - test includes only 1 document !!!!!!!!!!!!!!!!!!!!!\n",
        "vector_db = configure_faiss_vector_store(doc_splits, embeddings)\n",
        " # Expose index to the retriever\n",
        "retriever = create_retriever(vector_db)\n",
        "\n",
        "## Load prepared tables\n",
        "# Load discription table(sample)\n",
        "table_figure_table = [\n",
        "    {\"thesis_num\": 1, \"figure_num\": 1, \"table_num\": None, \"description\": \"The Transformer model architecture consists of an Encoder and a Decoder. The Encoder includes Input Embedding to convert tokens into dense vectors, Positional Encoding to add position information, Multi-Head Attention to focus on different input parts, Add & Norm for residual connections and normalization, Feed Forward to apply a position-wise feed-forward neural network, and another Add & Norm. The Decoder involves Output Embedding to convert previous output tokens, Positional Encoding, Masked Multi-Head Attention to focus on previous outputs with masking, Add & Norm, Multi-Head Attention to attend to encoder outputs, another Add & Norm, Feed Forward, and another Add & Norm. The final layers include a Linear Layer to transform decoder output and Softmax to produce a probability distribution over output tokens. This architecture uses attention mechanisms to handle long-range dependencies in sequences, making it effective for tasks like machine translation and text summarization.\"},\n",
        "    {\"thesis_num\": 1, \"figure_num\": 2, \"table_num\": None, \"description\": \"This is description for figure2 in thesis 1\"},\n",
        "    {\"thesis_num\": 1, \"figure_num\": 3, \"table_num\": None, \"description\": \"This is description for figure3 in thesis 1\"},\n",
        "    {\"thesis_num\": 1, \"figure_num\": 4, \"table_num\": None, \"description\": \"This is description for figure4 in thesis 1\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": 1, \"table_num\": None, \"description\": \"This is description for figure1 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": 2, \"table_num\": None, \"description\": \"This is description for figure2 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": 3, \"table_num\": None, \"description\": \"This is description for figure3 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": None, \"table_num\": 1, \"description\": \"This is description for table1 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": None, \"table_num\": 2, \"description\": \"This is description for table2 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": None, \"table_num\": 3, \"description\": \"This is description for table3 in thesis 2\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": 1, \"table_num\": None, \"description\": \"This is description for figure1 in thesis 3\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": 2, \"table_num\": None, \"description\": \"This is description for figure2 in thesis 3\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": None, \"table_num\": 1, \"description\": \"This is description for table1 in thesis 3\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": None, \"table_num\": 2, \"description\": \"This is description for table2 in thesis 3\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": None, \"table_num\": 3, \"description\": \"This is description for table3 in thesis 3\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": 1, \"table_num\": None, \"description\": \"This is description for figure1 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": 2, \"table_num\": None, \"description\": \"This is description for figure2 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": 3, \"table_num\": None, \"description\": \"This is description for figure3 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": None, \"table_num\": 1, \"description\": \"This is description for table1 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": None, \"table_num\": 2, \"description\": \"This is description for table2 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": None, \"table_num\": 3, \"description\": \"This is description for table3 in thesis 4\"}\n",
        "]\n",
        "\n",
        "table_figure_table = pd.DataFrame(table_figure_table)\n",
        "\n",
        "# Load table with thesis_num and description\n",
        "table_summary = pd.DataFrame([\n",
        "    {\"thesis_num\": 1, \"description\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"},\n",
        "    {\"thesis_num\": 2, \"description\": \"This is summary description in thesis 2\"},\n",
        "    {\"thesis_num\": 3, \"description\": \"This is summary description in thesis 3\"},\n",
        "    {\"thesis_num\": 4, \"description\": \"This is summary description in thesis 4\"}\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCzy_vdYTdVT",
        "outputId": "c0243000-30d4-4161-e178-6fde69151772"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM for the main flow\n",
        "client_main = Groq(\n",
        "    api_key=\"YOUR_API_KEY_1\",\n",
        ")\n",
        "#  LLM for keys(thesis/figure/table) extaction\n",
        "client_extract = Groq(\n",
        "    api_key=\"YOUR_API_KEY_2\",\n",
        ")\n",
        "# LLM for HyDE\n",
        "client_hyde = Groq(\n",
        "    api_key=\"YOUR_API_KEY_3\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "17XIY37dcj1a"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main - Answering questions\n",
        "\n",
        "def chat_main(user_query):\n",
        "\n",
        "  ## 1. extract keys from user's query and find figure/table description and summary description ##\n",
        "  # generate prompt to extract keys from user's query\n",
        "  prompt_extract_query = generate_prompt_extract_query(instruction_extract_query, user_query)\n",
        "  # get keys from Extraction LLM\n",
        "  response_keys = get_groq_response(client_extract, prompt_extract_query)\n",
        "  # parse keys\n",
        "  keys = parse_and_convert_keys(response_keys[0])\n",
        "  # extract figure/table descriptions\n",
        "  descriptions_figure_table = extract_descriptions(table_figure_table, keys)\n",
        "\n",
        "  # extract thesis numbers from keys\n",
        "  keys_thesis = extract_thesis_numbers(keys)\n",
        "  # get summary descriptions\n",
        "  descriptions_summary = get_descriptions_for_thesis_summary(keys_thesis, table_summary)\n",
        "\n",
        "\n",
        "  ## 2. get context for HyDE and general RAG ##\n",
        "  # !!!!!!!!!!!!!!!!!!! filter vectorDB by using thesis number(keys_thesis) - process_query!!!!!!!!!!!!!!!!!!!\n",
        "  # ADD_CODE_HERE\n",
        "\n",
        "  # add summary of the thesis as a context for HyDE\n",
        "  context_hyde = descriptions_summary\n",
        "  # create prompt for HyDE\n",
        "  prompt_hyde = generate_prompt_hyde(instruction_hyde, user_query, str(context_hyde))\n",
        "  # get a hypothetical answer from HyDE LLM\n",
        "  response_hyde = get_groq_response(client_hyde, prompt_hyde)\n",
        "  # create contexts - RAG(Hyde)\n",
        "  context_rag_hyde = process_query(response_hyde[0], retriever)\n",
        "  # create contexts - RAG(General)\n",
        "  context_rag_general = process_query(user_query, retriever)\n",
        "\n",
        "\n",
        "  ## 3. get a final response ##\n",
        "  # create prompt for a final response\n",
        "  prompt_final = generate_prompt_final(instruction_final, user_query, str(descriptions_figure_table), context_rag_hyde, context_rag_general)\n",
        "  # get final response from main LLM\n",
        "  response_final = get_groq_response(client_main, prompt_final)\n",
        "\n",
        "  return response_final"
      ],
      "metadata": {
        "id": "-KOsa_idg_2_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get response - figure test\n",
        "user_query = 'Describe the Figure 1 in the first paper.'\n",
        "response = chat_main(user_query)\n",
        "print(response[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIkaZ0vlCnGQ",
        "outputId": "d0d31ee7-a27a-47e5-dfb0-3432e4105fdc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure 1 in the first paper illustrates the Transformer model architecture, which consists of an Encoder and a Decoder. The Encoder includes Input Embedding, Positional Encoding, Multi-Head Attention, Add & Norm, Feed Forward, and another Add & Norm. The Decoder involves Output Embedding, Positional Encoding, Masked Multi-Head Attention, Add & Norm, Multi-Head Attention, another Add & Norm, Feed Forward, and another Add & Norm. The final layers include a Linear Layer and Softmax to produce a probability distribution over output tokens. This architecture uses attention mechanisms to handle long-range dependencies in sequences, making it effective for tasks like machine translation and text summarization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get response - summary test\n",
        "user_query = 'Identify key findings from the first academic article'\n",
        "response = chat_main(user_query)\n",
        "print(response[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80oSsllTCvsl",
        "outputId": "d8fcc1c0-29f1-4eb1-af2e-8c59c9c8251e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided Text Context, the key findings from the first academic article are:\n",
            "\n",
            "1. The proposed Transformer model achieves a new state-of-the-art BLEU score of 28.4 on the WMT 2014 English-to-German translation task, outperforming existing models, including ensembles, by over 2 BLEU.\n",
            "2. The Transformer model also establishes a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task, after training for 3.5 days on eight GPUs, which is a small fraction of the training costs of the best models from the literature.\n",
            "3. The Transformer model generalizes well to other tasks, such as English constituency parsing, both with large and limited training data.\n",
            "4. The model can be trained significantly faster than architectures based on recurrent or convolutional layers, achieving a new state of the art on both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\n",
            "5. The attention mechanism in the Transformer model allows for more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get response - conversation history test\n",
        "user_query = 'Please make the findings brief'\n",
        "response = chat_main(user_query)\n",
        "print(response[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcfiy0sVHzNY",
        "outputId": "cc9605d2-8676-4c5d-85b8-16f190fb3fa4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the findings can be briefly summarized as follows:\n",
            "\n",
            "* The paper explores the use of self-attention mechanisms in sequence transduction models, highlighting their potential benefits in terms of interpretability and performance.\n",
            "* The authors demonstrate that self-attention layers can learn to perform different tasks and exhibit behavior related to the syntactic and semantic structure of sentences.\n",
            "* The paper also presents results from experiments comparing self-attention layers to recurrent and convolutional layers, showing that self-attention can be a viable alternative.\n",
            "* Additionally, the authors discuss the importance of considering the total computational complexity per layer and the amount of computation that can be parallelized.\n",
            "\n",
            "Overall, the paper presents a novel approach to sequence transduction using self-attention mechanisms and demonstrates its potential benefits and advantages.\n"
          ]
        }
      ]
    }
  ]
}