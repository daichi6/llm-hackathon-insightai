{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install packages"
      ],
      "metadata": {
        "id": "qfrNeFuj6XGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain-community\n",
        "# !pip install sentence-transformers\n",
        "# !pip install faiss-cpu\n",
        "# !pip install --upgrade langchain\n",
        "# !pip install fitz\n",
        "# !pip install PyMuPDF\n",
        "# !pip install groq"
      ],
      "metadata": {
        "id": "mw01kLe7RkCO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "import faiss\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "import fitz\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from groq import Groq"
      ],
      "metadata": {
        "id": "rt-p2YyLRqBJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Functions"
      ],
      "metadata": {
        "id": "f9g3BwfW6d9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from a single PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "def extract_texts_from_pdfs(pdf_paths):\n",
        "    \"\"\"\n",
        "    Extract text from each PDF file in the list and create Document objects.\n",
        "\n",
        "    Args:\n",
        "        pdf_paths (list of str): List of paths to PDF files.\n",
        "\n",
        "    Returns:\n",
        "        list of Document: List of Document objects containing the extracted text.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        text = extract_text(pdf_path)\n",
        "        doc = Document(page_content=text, metadata={\"source\": pdf_path})\n",
        "        docs.append(doc)\n",
        "    return docs\n",
        "\n",
        "def split_documents_into_chunks(docs, chunk_size=500, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Splits the given documents into chunks of specified size with overlap.\n",
        "\n",
        "    Args:\n",
        "        docs (list): List of documents to split.\n",
        "        chunk_size (int): Size of each chunk. Default is 500 characters.\n",
        "        chunk_overlap (int): Overlap size between chunks. Default is 100 characters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of lists containing split documents with chunks per original document.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "    doc_chunks = {}\n",
        "    for doc in docs:\n",
        "        doc_chunks[doc.metadata[\"source\"]] = text_splitter.split_documents([doc])\n",
        "    return doc_chunks\n",
        "\n",
        "def add_chunk_numbers_to_metadata(doc_chunks):\n",
        "    \"\"\"\n",
        "    Adds chunk numbers to the metadata of each split document.\n",
        "\n",
        "    Args:\n",
        "        doc_chunks (dict): Dictionary of lists containing split documents.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of lists containing split documents with updated metadata.\n",
        "    \"\"\"\n",
        "    for chunks in doc_chunks.values():\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            chunk.metadata[\"chunk\"] = idx\n",
        "    return doc_chunks\n",
        "\n",
        "def configure_faiss_vector_store(doc_splits, embeddings):\n",
        "    \"\"\"\n",
        "    Configures FAISS as the vector store using the provided document splits and embeddings.\n",
        "\n",
        "    Args:\n",
        "        doc_splits (dict): Dictionary of lists containing split documents.\n",
        "        embeddings (Embeddings): Embeddings to be used for FAISS.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of FAISS vector stores per document.\n",
        "    \"\"\"\n",
        "    vector_stores = {}\n",
        "    for doc_source, chunks in doc_splits.items():\n",
        "        vector_stores[doc_source] = FAISS.from_documents(chunks, embeddings)\n",
        "    return vector_stores\n",
        "\n",
        "def save_faiss_vector_store(vector_db, directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    for doc_source, vector_store in vector_db.items():\n",
        "        index_path = os.path.join(directory, f\"{doc_source}.index\")\n",
        "        faiss.write_index(vector_store.index, index_path)\n",
        "\n",
        "        # Save docstore and index_to_docstore_id\n",
        "        metadata_path = os.path.join(directory, f\"{doc_source}.metadata\")\n",
        "        with open(metadata_path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'docstore': vector_store.docstore,\n",
        "                'index_to_docstore_id': vector_store.index_to_docstore_id\n",
        "            }, f)\n",
        "\n",
        "def load_faiss_vector_store(directory, embedding):\n",
        "    vector_db = {}\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.index'):\n",
        "            doc_source = os.path.splitext(filename)[0]\n",
        "            index_path = os.path.join(directory, filename)\n",
        "            metadata_path = os.path.join(directory, f\"{doc_source}.metadata\")\n",
        "\n",
        "            # Load FAISS index\n",
        "            index = faiss.read_index(index_path)\n",
        "\n",
        "            # Load metadata\n",
        "            with open(metadata_path, 'rb') as f:\n",
        "                metadata = pickle.load(f)\n",
        "\n",
        "            # Reconstruct FAISS vector store\n",
        "            vector_store = FAISS(\n",
        "                embedding_function=embedding,\n",
        "                index=index,\n",
        "                docstore=metadata['docstore'],\n",
        "                index_to_docstore_id=metadata['index_to_docstore_id']\n",
        "            )\n",
        "\n",
        "            vector_db[doc_source] = vector_store\n",
        "\n",
        "    return vector_db\n",
        "\n",
        "def create_retrievers(vector_stores, search_type=\"similarity\", k=5):\n",
        "    \"\"\"\n",
        "    Exposes the vector store index to retrievers for multiple documents.\n",
        "\n",
        "    Args:\n",
        "        vector_stores (dict): Dictionary of FAISS vector stores per document.\n",
        "        search_type (str): The type of search to perform. Default is \"similarity\".\n",
        "        k (int): The number of documents to return. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of retrievers per document.\n",
        "    \"\"\"\n",
        "    retrievers = {}\n",
        "    for doc_source, vector_store in vector_stores.items():\n",
        "        retrievers[doc_source] = vector_store.as_retriever(\n",
        "            search_type=search_type, search_kwargs={\"k\": k}\n",
        "        )\n",
        "    return retrievers\n",
        "\n",
        "def process_query(query: str, retriever):\n",
        "    \"\"\"\n",
        "    Processes the query using the provided retriever to retrieve relevant document chunks.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string to search for relevant documents.\n",
        "        retriever: The retriever object configured to use the vector store for document retrieval.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the formatted content and metadata of the retrieved document chunks.\n",
        "    \"\"\"\n",
        "    # Retrieve chunks based on the query\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "    # Initialize an empty string to collect all outputs\n",
        "    full_output = \"\"\n",
        "\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        chunk_output = f\"-----Chunk {i}------\\n\"\n",
        "        chunk_output += f\"Content: {doc.page_content}...\\n\"\n",
        "        chunk_output += f\"Metadata {doc.metadata}\\n\\n\"\n",
        "\n",
        "        # Append the chunk output to the full output\n",
        "        full_output += chunk_output\n",
        "\n",
        "    return full_output\n",
        "\n",
        "def get_groq_response(client, prompt, model=\"llama3-70b-8192\", max_tokens=2048, temperature=0.0):\n",
        "    \"\"\"\n",
        "    Generates a response using the provided client, model, prompt, and specified parameters.\n",
        "\n",
        "    Args:\n",
        "        client: The client object to interact with the API.\n",
        "        prompt (str): The prompt to generate a response for.\n",
        "        model (str, optional): The model identifier to use for generating the response. Default is \"llama3-70b-8192\".\n",
        "        max_tokens (int, optional): The maximum number of tokens for the generated response. Default is 2048.\n",
        "        temperature (float, optional): The temperature setting for the response generation. Default is 0.0.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The generated response content and usage statistics.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ],\n",
        "            model=model,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content, chat_completion.usage\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def generate_prompt_hyde(instruction, user_query, context_hyde):\n",
        "    \"\"\"\n",
        "    Generates a prompt for HyDE by replacing placeholders in the instruction template with the user's query and context.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing placeholders.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "        context_hyde (str): The context for creating a hypothetical answer to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholders replaced by the user's query and context.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    instruction = instruction.replace(\"{CONTEXT_HYDE}\", context_hyde)\n",
        "    return instruction\n",
        "\n",
        "def generate_prompt_final(instruction, user_query, context_figure_table, context_rag_hyde, context_rag_general):\n",
        "    \"\"\"\n",
        "    Generates a final prompt by replacing placeholders in the instruction template with the user's query and various contexts.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing placeholders.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "        context_figure_table (str): The context(description) related to figure and table to be inserted into the instruction.\n",
        "        context_rag_hyde (str): The context retreived from RAG HyDE to be inserted into the instruction.\n",
        "        context_rag_general (str): The general context retreived from RAG to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholders replaced by the user's query and contexts.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    instruction = instruction.replace(\"{CONTEXT_FIGURE_TABLE}\", context_figure_table)\n",
        "    instruction = instruction.replace(\"{CONTEXT_RAG_HYDE}\", context_rag_hyde)\n",
        "    instruction = instruction.replace(\"{CONTEXT_RAG_GENERAL}\", context_rag_general)\n",
        "    return instruction\n",
        "\n",
        "def generate_prompt_extract_query(instruction, user_query):\n",
        "    \"\"\"\n",
        "    Generates a prompt for extracting keys from the user's query by replacing placeholders in the instruction template.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The template instruction containing a placeholder for the user's query.\n",
        "        user_query (str): The user's query to be inserted into the instruction.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated instruction with the placeholder replaced by the user's query.\n",
        "    \"\"\"\n",
        "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
        "    return instruction\n",
        "\n",
        "def parse_and_convert_keys(json_string):\n",
        "    \"\"\"\n",
        "    Parse the JSON string and convert the string values in the keys list to their appropriate types.\n",
        "\n",
        "    Args:\n",
        "    json_string (str): A JSON string representing a list of dictionaries with string values for 'thesis', 'figure', and 'table'.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of dictionaries with 'thesis' as int, and 'figure' and 'table' as int or None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        keys = json.loads(json_string)\n",
        "        if not keys:\n",
        "            return []\n",
        "\n",
        "        converted_keys = []\n",
        "        for key in keys:\n",
        "            converted_key = {\n",
        "                \"thesis\": int(key[\"thesis\"]) if key[\"thesis\"] else None,\n",
        "                \"figure\": int(key[\"figure\"]) if key[\"figure\"] else None,\n",
        "                \"table\": int(key[\"table\"]) if key[\"table\"] else None\n",
        "            }\n",
        "            converted_keys.append(converted_key)\n",
        "        return converted_keys\n",
        "    except json.JSONDecodeError as e:\n",
        "        # print(f\"JSON decoding error: {e}\")\n",
        "        return []\n",
        "    except KeyError as e:\n",
        "        # print(f\"Missing key in JSON data: {e}\")\n",
        "        return []\n",
        "    except ValueError as e:\n",
        "        # print(f\"Value error: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        # print(f\"An unexpected error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_descriptions(df, keys):\n",
        "    \"\"\"\n",
        "    Extract and format descriptions from the dataframe based on the provided keys.\n",
        "\n",
        "    Args:\n",
        "    df (DataFrame): The dataframe containing thesis, figure, table, and description data.\n",
        "    keys (list): A list of dictionaries with 'thesis' as int, and 'figure' and 'table' as int or None.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of formatted descriptions corresponding to the provided keys.\n",
        "    \"\"\"\n",
        "    formatted_descriptions = []\n",
        "\n",
        "    for key in keys:\n",
        "        thesis_num = key[\"thesis\"]\n",
        "        figure_num = key[\"figure\"]\n",
        "        table_num = key[\"table\"]\n",
        "\n",
        "        if figure_num is not None:\n",
        "            description = df[(df[\"thesis_num\"] == thesis_num) & (df[\"figure_num\"] == figure_num)][\"description\"].values\n",
        "            prefix = f\"thesis{thesis_num} figure{figure_num} description: \"\n",
        "        elif table_num is not None:\n",
        "            description = df[(df[\"thesis_num\"] == thesis_num) & (df[\"table_num\"] == table_num)][\"description\"].values\n",
        "            prefix = f\"thesis{thesis_num} table{table_num} description: \"\n",
        "        else:\n",
        "            description = []\n",
        "            prefix = \"\"\n",
        "\n",
        "        if len(description) > 0:\n",
        "            formatted_descriptions.append(prefix + description[0])\n",
        "        else:\n",
        "            formatted_descriptions.append(prefix + \"Description not found\")\n",
        "\n",
        "    return formatted_descriptions\n",
        "\n",
        "def extract_thesis_numbers(converted_keys):\n",
        "    \"\"\"\n",
        "    Extracts the thesis numbers from a list of dictionaries.\n",
        "\n",
        "    Args:\n",
        "    converted_keys (list): A list of dictionaries with 'thesis', 'figure', and 'table' keys.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of thesis numbers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        thesis_numbers = [item['thesis'] for item in converted_keys]\n",
        "        return thesis_numbers\n",
        "    except Exception as e:\n",
        "        # print(f\"An error occurred while extracting thesis numbers: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_descriptions_for_thesis_summary(thesis_numbers, table_summary):\n",
        "    \"\"\"\n",
        "    Retrieves the descriptions for the given thesis numbers from the table_summary DataFrame.\n",
        "\n",
        "    Args:\n",
        "    thesis_numbers (list): A list of thesis numbers.\n",
        "    table_summary (pd.DataFrame): The DataFrame containing thesis numbers and their descriptions.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of descriptions corresponding to the thesis numbers, formatted to indicate which thesis each description belongs to.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = []\n",
        "        for thesis_num in thesis_numbers:\n",
        "            description = table_summary.loc[table_summary['thesis_num'] == thesis_num, 'description'].values[0]\n",
        "            result.append(f\"Summary description for thesis {thesis_num}: '{description}'\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        # print(f\"An error occurred while retrieving descriptions: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "gB67f2VkSPRo"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prompts"
      ],
      "metadata": {
        "id": "qtJr7Mot6hU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompts(Instructions)\n",
        "\n",
        "instruction_hyde = \"\"\"\n",
        "### Instructions ###\n",
        "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" below seems relevant to \"Users' query\", please refer to it.\n",
        "\n",
        "### User’s query ###\n",
        "{USER_QUERY}\n",
        "\n",
        "### Context ###\n",
        "{CONTEXT_HYDE}\n",
        "\n",
        "### Output ###\n",
        "\"\"\"\n",
        "\n",
        "instruction_final = \"\"\"\n",
        "### Instructions ###\n",
        "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
        "If the information in the \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them.\n",
        "\"Text Context\" includes several chunks from different parts of an academic paper. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper.\n",
        "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response.\n",
        "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
        "If you determine that the previous conversation history is relevant, please also refer to that information to answer the user's query.　Especially when the the contexts below are empty, please answer the user's most recent query　based on the conversation history(the user's previous queries and your responses).\n",
        "If the conversation is continuing from the previous session and no additional information is needed, you may refer to the previous conversation history and might not need to use the contexts below. (e.g., User's query: Please make your response brief).\n",
        "If the contexts and the previous conversation history do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
        "\n",
        "##### User’s query #####\n",
        "{USER_QUERY}\n",
        "\n",
        "\n",
        "##### Figure/Table Context #####\n",
        "{CONTEXT_FIGURE_TABLE}\n",
        "\n",
        "##### Text Context #####\n",
        "{CONTEXT_RAG_HYDE}\n",
        "\n",
        "{CONTEXT_RAG_GENERAL}\n",
        "\n",
        "\n",
        "##### Output #####\n",
        "\"\"\"\n",
        "\n",
        "instruction_extract_query = \"\"\"\n",
        "### Instructions ###\n",
        "You are an NLP engineer. Your task is to extract the \"numbers\" from the user's query below.\n",
        "The \"numbers\" mean which academic paper the user is referring to, 2) which figure the user is referring to, and 3) which table the user is referring to.\n",
        "There may be cases where all, some, or none of these are specified. Enter the number only for the specified fields, and return an empty string \"\" for fields that are not specified.\n",
        "Interpret \"figure\" for terms such as \"Chart,\" \"Diagram,\" or \"Image.\" Interpret \"thesis\" for terms such as \"Academic Paper,\" \"Paper,\" or \"Document.\"\n",
        "Please provide your response as a list of objects, each containing thesis, figure, and table.　Please provide your response strictly in the specified format, without including any additional text for formatting. I will use your response directly.\n",
        "If it is unclear which thesis, figure, or table is being referred to, it is okay to return an empty string. Please do not make any assumptions.\n",
        "\n",
        "### Output Format ###\n",
        "Format: a list of objects\n",
        "\n",
        "### Example user's query1 ###\n",
        "What is the main hypothesis or research question addressed in the first academic article?\n",
        "\n",
        "### Example Output1 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"1\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query2 ###\n",
        "Summarize the methodology used in the third academic article. Highlight any unique approaches or techniques employed.\n",
        "\n",
        "### Example Output2 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"3\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "\n",
        "### Example user's query3 ###\n",
        "Q. From the images and figures in the second article, describe the trend shown in Figure 2. What does it indicate about the research findings?\n",
        "\n",
        "### Example Output3 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"2\",\n",
        "  \"figure\": \"2\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query4 ###\n",
        "Q. What can be understood from Image 3 in the third paper?\n",
        "\n",
        "### Example Output4 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"3\",\n",
        "  \"figure\": \"3\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query4 ###\n",
        "Q. Please explain Figure 3 and Table 2 of the second academic paper. What do these indicate about the research findings?\n",
        "\n",
        "### Example Output4 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"2\",\n",
        "  \"figure\": \"3\",\n",
        "  \"table\": \"\"\n",
        "  },\n",
        "  {\n",
        "  \"thesis\": \"2\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"2\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query5 ###\n",
        "Q. Please compare table 3 and chart 4 from the second and third theses, respectively.\n",
        "\n",
        "### Example Output5 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"2\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"3\"\n",
        "  },\n",
        "  {\n",
        "  \"thesis\": \"3\",\n",
        "  \"figure\": \"4\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query6 ###\n",
        "Do you like an apple?\n",
        "\n",
        "### Example Output6 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "### Example user's query7 ###\n",
        "Considering the previous conversations, please propose a new research direction or hypothesis.\n",
        "\n",
        "### Example Output7 ###\n",
        "[\n",
        "  {\n",
        "  \"thesis\": \"\",\n",
        "  \"figure\": \"\",\n",
        "  \"table\": \"\"\n",
        "  }\n",
        "]\n",
        "\n",
        "\n",
        "### User’s query ###\n",
        "{USER_QUERY}\n",
        "\n",
        "### Output ###\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "tGYKP0lnTIXe"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare data"
      ],
      "metadata": {
        "id": "l_D6noSE6pP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1. Creating VectorDB"
      ],
      "metadata": {
        "id": "hkq63-IQ7pIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating VectorDB\n",
        "# Load Sample PDF\n",
        "pdf_paths = [\"attention.pdf\", \"llm_review 2.pdf\", \"Multimodal.pdf\"] # Change it to YOUR PATH\n",
        "\n",
        "# Extract text from each PDF and create Document objects\n",
        "docs = extract_texts_from_pdfs(pdf_paths)\n",
        "\n",
        "## Chunk\n",
        "# Split the documents into chunks\n",
        "doc_splits = split_documents_into_chunks(docs)\n",
        "# Add chunk number to metadata\n",
        "doc_splits = add_chunk_numbers_to_metadata(doc_splits)\n",
        "\n",
        "## Embedding\n",
        "# SciBERT(Allen Institute for AI) - for academic(science) paper including computer science\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "## Vector Store\n",
        "# Configure FAISS as Vector Store\n",
        "vector_db = configure_faiss_vector_store(doc_splits, embeddings)\n",
        "\n",
        "# Save the vector_db\n",
        "vectordb_path = \"vectordb_faiss\" # Change it to YOUR PATH\n",
        "save_faiss_vector_store(vector_db, vectordb_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCzy_vdYTdVT",
        "outputId": "b5d1c7bc-e08e-4c7f-8786-04f11f11f019"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2. Creating Image/Table Desctiption Table"
      ],
      "metadata": {
        "id": "ZVygFqp272wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADD CODES HERE - Creating Figure/Table Descriptions\n",
        "# ADD CODES HERE - Extracting Figure/Table Numbers from Image by using LLM\n",
        "# ADD CODES HERE - Creating Final Table\n"
      ],
      "metadata": {
        "id": "y2uAfc-V8FvG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-3. Creating Summary Table"
      ],
      "metadata": {
        "id": "IiqnksVZ8L54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADD CODES HERE"
      ],
      "metadata": {
        "id": "0g7rHAbi8Sou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load prepared data"
      ],
      "metadata": {
        "id": "eIYnVSBb7AeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-1. Load VectorDB"
      ],
      "metadata": {
        "id": "Pv0eAE5o8cCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load prepared vectorDB\n",
        "# Load the vector_db\n",
        "vector_db = load_faiss_vector_store(vectordb_path, embeddings)\n",
        "# Create retrievers for each document and store them in a dictionary\n",
        "retrievers = create_retrievers(vector_db)"
      ],
      "metadata": {
        "id": "WhW-IrYM7ZQx"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-2. Load Image/Table Desctiption Table"
      ],
      "metadata": {
        "id": "HOdoAqLC8fuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load prepared tables\n",
        "# Load image/table discription table(sample)\n",
        "\n",
        "# REPLACE WITH REAL DATASET\n",
        "\n",
        "table_figure_table = [\n",
        "    {\"thesis_num\": 1, \"figure_num\": 1, \"table_num\": None, \"description\": \"The Transformer model architecture consists of an Encoder and a Decoder. The Encoder includes Input Embedding to convert tokens into dense vectors, Positional Encoding to add position information, Multi-Head Attention to focus on different input parts, Add & Norm for residual connections and normalization, Feed Forward to apply a position-wise feed-forward neural network, and another Add & Norm. The Decoder involves Output Embedding to convert previous output tokens, Positional Encoding, Masked Multi-Head Attention to focus on previous outputs with masking, Add & Norm, Multi-Head Attention to attend to encoder outputs, another Add & Norm, Feed Forward, and another Add & Norm. The final layers include a Linear Layer to transform decoder output and Softmax to produce a probability distribution over output tokens. This architecture uses attention mechanisms to handle long-range dependencies in sequences, making it effective for tasks like machine translation and text summarization.\"},\n",
        "    {\"thesis_num\": 1, \"figure_num\": 2, \"table_num\": None, \"description\": \"This is description for figure2 in thesis 1\"},\n",
        "    {\"thesis_num\": 1, \"figure_num\": 3, \"table_num\": None, \"description\": \"This is description for figure3 in thesis 1\"},\n",
        "    {\"thesis_num\": 1, \"figure_num\": 4, \"table_num\": None, \"description\": \"This is description for figure4 in thesis 1\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": 1, \"table_num\": None, \"description\": \"The image illustrates the LLaVA network architecture, which integrates vision and language processing to generate language responses from image and language instructions. The Vision Encoder processes the image to generate visual features. These visual features are then mapped to a new representation using a projection matrix. The Language Model takes the projected visual features and language instructions as inputs to produce a language response. The workflow shows how information flows from the image and language inputs through the network to produce a language response.\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": 2, \"table_num\": None, \"description\": \"This is description for figure2 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": 3, \"table_num\": None, \"description\": \"This is description for figure3 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": None, \"table_num\": 1, \"description\": \"This is description for table1 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": None, \"table_num\": 2, \"description\": \"This is description for table2 in thesis 2\"},\n",
        "    {\"thesis_num\": 2, \"figure_num\": None, \"table_num\": 3, \"description\": \"This is description for table3 in thesis 2\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": 1, \"table_num\": None, \"description\": \"This is description for figure1 in thesis 3\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": 2, \"table_num\": None, \"description\": \"This is description for figure2 in thesis 3\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": None, \"table_num\": 1, \"description\": \"This is description for table1 in thesis 3\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": None, \"table_num\": 2, \"description\": \"This is description for table2 in thesis 3\"},\n",
        "    {\"thesis_num\": 3, \"figure_num\": None, \"table_num\": 3, \"description\": \"This is description for table3 in thesis 3\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": 1, \"table_num\": None, \"description\": \"This is description for figure1 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": 2, \"table_num\": None, \"description\": \"This is description for figure2 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": 3, \"table_num\": None, \"description\": \"This is description for figure3 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": None, \"table_num\": 1, \"description\": \"This is description for table1 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": None, \"table_num\": 2, \"description\": \"This is description for table2 in thesis 4\"},\n",
        "    {\"thesis_num\": 4, \"figure_num\": None, \"table_num\": 3, \"description\": \"This is description for table3 in thesis 4\"}\n",
        "]\n",
        "\n",
        "table_figure_table = pd.DataFrame(table_figure_table)\n"
      ],
      "metadata": {
        "id": "Adq-rtgv7C6V"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-3. Load Summary Table"
      ],
      "metadata": {
        "id": "E9qvmZYX8jo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load table with thesis_num and description\n",
        "\n",
        "# REPLACE WITH REAL DATASET\n",
        "\n",
        "table_summary = pd.DataFrame([\n",
        "    {\"thesis_num\": 1, \"description\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"},\n",
        "    {\"thesis_num\": 2, \"description\": \"Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we in- troduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general- purpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demon- strates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela- tive score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available.\"},\n",
        "    {\"thesis_num\": 3, \"description\": \"This is summary description in thesis 3\"},\n",
        "    {\"thesis_num\": 4, \"description\": \"This is summary description in thesis 4\"}\n",
        "])"
      ],
      "metadata": {
        "id": "5d-WbgiD8kxi"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Main function"
      ],
      "metadata": {
        "id": "GdIrzw0j6snA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## User selection and Mapping\n",
        "# User thesis selection before asking questions\n",
        "\n",
        "# mapping for pdf_paths\n",
        "\n",
        "# mapping for vectordb(retreivers)\n",
        "\n",
        "# mapping for image/table description table\n",
        "\n",
        "# mapping for summary table"
      ],
      "metadata": {
        "id": "iPq1fnTHCORS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM for the main flow\n",
        "client_main = Groq(\n",
        "    api_key=\"YOUR_API_KEY_1\",\n",
        ")\n",
        "#  LLM for keys(thesis/figure/table) extaction\n",
        "client_extract = Groq(\n",
        "    api_key=\"YOUR_API_KEY_2\",\n",
        ")\n",
        "# LLM for HyDE\n",
        "client_hyde = Groq(\n",
        "    api_key=\"YOUR_API_KEY_3\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "17XIY37dcj1a"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "\n",
        "def chat_main(user_query):\n",
        "\n",
        "  ## 1. extract keys from user's query and find figure/table description and summary description ##\n",
        "  # generate prompt to extract keys from user's query\n",
        "  prompt_extract_query = generate_prompt_extract_query(instruction_extract_query, user_query)\n",
        "  # get keys from Extraction LLM\n",
        "  response_keys = get_groq_response(client_extract, prompt_extract_query)\n",
        "\n",
        "  # parse keys\n",
        "  keys = parse_and_convert_keys(response_keys[0])\n",
        "\n",
        "  # extract figure/table descriptions\n",
        "  descriptions_figure_table = extract_descriptions(table_figure_table, keys)\n",
        "\n",
        "  # extract thesis numbers from keys\n",
        "  keys_thesis = extract_thesis_numbers(keys)\n",
        "  # get summary descriptions\n",
        "  descriptions_summary = get_descriptions_for_thesis_summary(keys_thesis, table_summary)\n",
        "\n",
        "  ## 2. get context for HyDE and general RAG ##\n",
        "  # add summary of the thesis as a context for HyDE\n",
        "  context_hyde = descriptions_summary\n",
        "  # create prompt for HyDE\n",
        "  prompt_hyde = generate_prompt_hyde(instruction_hyde, user_query, str(context_hyde))\n",
        "  # get a hypothetical answer from HyDE LLM\n",
        "  response_hyde = get_groq_response(client_hyde, prompt_hyde)\n",
        "\n",
        "  # # create contexts\n",
        "  # initialize empty strings for contexts\n",
        "  context_rag_hyde = \"\"\n",
        "  context_rag_general = \"\"\n",
        "\n",
        "  # search for documents based on keys_thesis(thesis number extracted from user's query)\n",
        "  if keys_thesis and all(key is not None for key in keys_thesis):\n",
        "      for key in keys_thesis:\n",
        "          if isinstance(key, int):\n",
        "              adjusted_key = key - 1  # adjust the key by subtracting 1\n",
        "              doc_source = pdf_paths[adjusted_key]  # get the document source based on the adjusted key\n",
        "              retriever = retrievers[doc_source]  # get the corresponding retriever\n",
        "\n",
        "              # process query for RAG(Hyde)\n",
        "              result_hyde = process_query(response_hyde[0], retriever)\n",
        "              context_rag_hyde += f\"Document {key}:\\n{result_hyde}\\n\"\n",
        "\n",
        "              # process query for RAG(General)\n",
        "              result_general = process_query(user_query, retriever)\n",
        "              context_rag_general += f\"Document {key}:\\n{result_general}\\n\"\n",
        "  else:\n",
        "      context_rag_hyde = \"\"\n",
        "      context_rag_general = \"\"\n",
        "\n",
        "  ## 3. get a final response ##\n",
        "  # create prompt for a final response\n",
        "  prompt_final = generate_prompt_final(instruction_final, user_query, str(descriptions_figure_table), context_rag_hyde, context_rag_general)\n",
        "  # get final response from main LLM\n",
        "  response_final = get_groq_response(client_main, prompt_final)\n",
        "\n",
        "  return response_final"
      ],
      "metadata": {
        "id": "-KOsa_idg_2_"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Asking questions"
      ],
      "metadata": {
        "id": "LI_h3T7q64Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get response - figure test\n",
        "user_query = 'Describe the Figure 1 in the first paper.'\n",
        "response = chat_main(user_query)\n",
        "print(response[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIkaZ0vlCnGQ",
        "outputId": "1074d829-5b04-4795-a45e-4f7b3845402a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure 1 in the first paper illustrates the Transformer model architecture, which consists of an Encoder and a Decoder. The Encoder includes Input Embedding, Positional Encoding, Multi-Head Attention, Add & Norm, Feed Forward, and another Add & Norm. The Decoder involves Output Embedding, Positional Encoding, Masked Multi-Head Attention, Add & Norm, Multi-Head Attention, another Add & Norm, Feed Forward, and another Add & Norm. The final layers include a Linear Layer and Softmax to produce a probability distribution over output tokens. This architecture uses attention mechanisms to handle long-range dependencies in sequences, making it effective for tasks like machine translation and text summarization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get response - summary test\n",
        "user_query = 'Identify key findings from the first academic article'\n",
        "response = chat_main(user_query)\n",
        "print(response[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80oSsllTCvsl",
        "outputId": "1f5490db-6312-4dca-8e79-8014e956d8c8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided Text Context, the key findings from the first academic article are:\n",
            "\n",
            "1. The proposed Transformer model achieves state-of-the-art results in machine translation tasks, outperforming existing models and ensembles.\n",
            "2. The model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, improving over the existing best results by over 2 BLEU.\n",
            "3. The model establishes a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task.\n",
            "4. The Transformer model generalizes well to other tasks, such as English constituency parsing, both with large and limited training data.\n",
            "5. The model can be trained significantly faster than architectures based on recurrent or convolutional layers, achieving a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "6. The attention mechanism in the Transformer model allows for more parallelization and yields more interpretable models, with individual attention heads clearly learning to perform different tasks and exhibiting behavior related to the syntactic and semantic structure of the sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get response - conversation history test\n",
        "user_query = 'Can you explain more detail about it?'\n",
        "response = chat_main(user_query)\n",
        "print(response[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rH-3sOx0Y9L",
        "outputId": "20ed6580-2641-48af-d7d7-ba557c9ce3c6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm happy to help! However, I notice that the Text Context and Figure/Table Context are empty, and there is no previous conversation history provided. Could you please provide more context or clarify what you would like me to explain in more detail? I'll do my best to assist you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get response - multiple figures from different papers test\n",
        "user_query = 'Describe the Figure1 in the first paper and Figure1 in the second paper.'\n",
        "response = chat_main(user_query)\n",
        "print(response[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qxdLs6-zzwT",
        "outputId": "fdfc9fbd-26fe-46fd-be1a-3ee33bdb1e47"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided Figure/Table Context, I can describe Figure 1 in the first paper and Figure 1 in the second paper as follows:\n",
            "\n",
            "**Figure 1 in the first paper:**\n",
            "Figure 1 in the first paper illustrates the Transformer model architecture, which consists of an Encoder and a Decoder. The Encoder includes Input Embedding, Positional Encoding, Multi-Head Attention, Add & Norm, Feed Forward, and another Add & Norm. The Decoder involves Output Embedding, Positional Encoding, Masked Multi-Head Attention, Add & Norm, Multi-Head Attention, another Add & Norm, Feed Forward, and another Add & Norm. The final layers include a Linear Layer and Softmax to produce a probability distribution over output tokens.\n",
            "\n",
            "**Figure 1 in the second paper:**\n",
            "Figure 1 in the second paper illustrates the LLaVA network architecture, which integrates vision and language processing to generate language responses from image and language instructions. The Vision Encoder processes the image to generate visual features, which are then mapped to a new representation using a projection matrix. The Language Model takes the projected visual features and language instructions as inputs to produce a language response. The workflow shows how information flows from the image and language inputs through the network to produce a language response.\n"
          ]
        }
      ]
    }
  ]
}