{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfrNeFuj6XGt"
   },
   "source": [
    "## 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mw01kLe7RkCO"
   },
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community\n",
    "# !pip install sentence-transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install --upgrade langchain\n",
    "# !pip install fitz\n",
    "# !pip install PyMuPDF\n",
    "# !pip install groq\n",
    "# !pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rt-p2YyLRqBJ"
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "from groq import Groq\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9g3BwfW6d9k"
   },
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "gB67f2VkSPRo"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a single PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "def extract_texts_from_pdfs(pdf_paths):\n",
    "    \"\"\"\n",
    "    Extract text from each PDF file in the list and create Document objects.\n",
    "\n",
    "    Args:\n",
    "        pdf_paths (list of str): List of paths to PDF files.\n",
    "\n",
    "    Returns:\n",
    "        list of Document: List of Document objects containing the extracted text.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        text = extract_text(pdf_path)\n",
    "        doc = Document(page_content=text, metadata={\"source\": pdf_path})\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "def split_documents_into_chunks(docs, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Splits the given documents into chunks of specified size with overlap.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of documents to split.\n",
    "        chunk_size (int): Size of each chunk. Default is 500 characters.\n",
    "        chunk_overlap (int): Overlap size between chunks. Default is 100 characters.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of lists containing split documents with chunks per original document.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    doc_chunks = {}\n",
    "    for doc in docs:\n",
    "        doc_chunks[doc.metadata[\"source\"]] = text_splitter.split_documents([doc])\n",
    "    return doc_chunks\n",
    "\n",
    "def add_chunk_numbers_to_metadata(doc_chunks):\n",
    "    \"\"\"\n",
    "    Adds chunk numbers to the metadata of each split document.\n",
    "\n",
    "    Args:\n",
    "        doc_chunks (dict): Dictionary of lists containing split documents.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of lists containing split documents with updated metadata.\n",
    "    \"\"\"\n",
    "    for chunks in doc_chunks.values():\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"chunk\"] = idx\n",
    "    return doc_chunks\n",
    "\n",
    "def configure_faiss_vector_store(doc_splits, embeddings):\n",
    "    \"\"\"\n",
    "    Configures FAISS as the vector store using the provided document splits and embeddings.\n",
    "\n",
    "    Args:\n",
    "        doc_splits (dict): Dictionary of lists containing split documents.\n",
    "        embeddings (Embeddings): Embeddings to be used for FAISS.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of FAISS vector stores per document.\n",
    "    \"\"\"\n",
    "    vector_stores = {}\n",
    "    for doc_source, chunks in doc_splits.items():\n",
    "        vector_stores[doc_source] = FAISS.from_documents(chunks, embeddings)\n",
    "    return vector_stores\n",
    "\n",
    "def save_faiss_vector_store(vector_db, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for doc_source, vector_store in vector_db.items():\n",
    "        index_path = os.path.join(directory, f\"{doc_source}.index\")\n",
    "        faiss.write_index(vector_store.index, index_path)\n",
    "\n",
    "        # Save docstore and index_to_docstore_id\n",
    "        metadata_path = os.path.join(directory, f\"{doc_source}.metadata\")\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'docstore': vector_store.docstore,\n",
    "                'index_to_docstore_id': vector_store.index_to_docstore_id\n",
    "            }, f)\n",
    "\n",
    "def load_faiss_vector_store(directory, embedding):\n",
    "    vector_db = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.index'):\n",
    "            doc_source = os.path.splitext(filename)[0]\n",
    "            index_path = os.path.join(directory, filename)\n",
    "            metadata_path = os.path.join(directory, f\"{doc_source}.metadata\")\n",
    "\n",
    "            # Load FAISS index\n",
    "            index = faiss.read_index(index_path)\n",
    "\n",
    "            # Load metadata\n",
    "            with open(metadata_path, 'rb') as f:\n",
    "                metadata = pickle.load(f)\n",
    "\n",
    "            # Reconstruct FAISS vector store\n",
    "            vector_store = FAISS(\n",
    "                embedding_function=embedding,\n",
    "                index=index,\n",
    "                docstore=metadata['docstore'],\n",
    "                index_to_docstore_id=metadata['index_to_docstore_id']\n",
    "            )\n",
    "\n",
    "            vector_db[doc_source] = vector_store\n",
    "\n",
    "    return vector_db\n",
    "\n",
    "def create_retrievers(vector_stores, search_type=\"similarity\", k=5):\n",
    "    \"\"\"\n",
    "    Exposes the vector store index to retrievers for multiple documents.\n",
    "\n",
    "    Args:\n",
    "        vector_stores (dict): Dictionary of FAISS vector stores per document.\n",
    "        search_type (str): The type of search to perform. Default is \"similarity\".\n",
    "        k (int): The number of documents to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of retrievers per document.\n",
    "    \"\"\"\n",
    "    retrievers = {}\n",
    "    for doc_source, vector_store in vector_stores.items():\n",
    "        retrievers[doc_source] = vector_store.as_retriever(\n",
    "            search_type=search_type, search_kwargs={\"k\": k}\n",
    "        )\n",
    "    return retrievers\n",
    "\n",
    "def process_query(query: str, retriever):\n",
    "    \"\"\"\n",
    "    Processes the query using the provided retriever to retrieve relevant document chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for relevant documents.\n",
    "        retriever: The retriever object configured to use the vector store for document retrieval.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the formatted content and metadata of the retrieved document chunks.\n",
    "    \"\"\"\n",
    "    # Retrieve chunks based on the query\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Initialize an empty string to collect all outputs\n",
    "    full_output = \"\"\n",
    "\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        chunk_output = f\"-----Chunk {i}------\\n\"\n",
    "        chunk_output += f\"Content: {doc.page_content}...\\n\"\n",
    "        chunk_output += f\"Metadata {doc.metadata}\\n\\n\"\n",
    "\n",
    "        # Append the chunk output to the full output\n",
    "        full_output += chunk_output\n",
    "\n",
    "    return full_output\n",
    "\n",
    "def get_groq_response(client, prompt, model=\"llama3-70b-8192\", max_tokens=2048, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Generates a response using the provided client, model, prompt, and specified parameters.\n",
    "\n",
    "    Args:\n",
    "        client: The client object to interact with the API.\n",
    "        prompt (str): The prompt to generate a response for.\n",
    "        model (str, optional): The model identifier to use for generating the response. Default is \"llama3-70b-8192\".\n",
    "        max_tokens (int, optional): The maximum number of tokens for the generated response. Default is 2048.\n",
    "        temperature (float, optional): The temperature setting for the response generation. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The generated response content and usage statistics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content, chat_completion.usage\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def generate_prompt_hyde(instruction, user_query, context_hyde, table_summary):\n",
    "    \"\"\"\n",
    "    Generates a prompt for HyDE by replacing placeholders in the instruction template with the user's query and context.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The template instruction containing placeholders.\n",
    "        user_query (str): The user's query to be inserted into the instruction.\n",
    "        context_hyde (str): The context for creating a hypothetical answer to be inserted into the instruction.\n",
    "        table_insight(str): The summary table for all PDFs.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated instruction with the placeholders replaced by the user's query and context.\n",
    "    \"\"\"\n",
    "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
    "    instruction = instruction.replace(\"{CONTEXT_HYDE}\", context_hyde)\n",
    "    instruction = instruction.replace(\"{SUMMARY_TABLE}\", table_summary)\n",
    "\n",
    "    return instruction\n",
    "\n",
    "def generate_prompt_final(instruction, user_query, context_figure_table, context_rag_hyde, context_rag_general, table_insight):\n",
    "    \"\"\"\n",
    "    Generates a final prompt by replacing placeholders in the instruction template with the user's query and various contexts.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The template instruction containing placeholders.\n",
    "        user_query (str): The user's query to be inserted into the instruction.\n",
    "        context_figure_table (str): The context(description) related to figure and table to be inserted into the instruction.\n",
    "        context_rag_hyde (str): The context retreived from RAG HyDE to be inserted into the instruction.\n",
    "        context_rag_general (str): The general context retreived from RAG to be inserted into the instruction.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated instruction with the placeholders replaced by the user's query and contexts.\n",
    "    \"\"\"\n",
    "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
    "    instruction = instruction.replace(\"{CONTEXT_FIGURE_TABLE}\", context_figure_table)\n",
    "    instruction = instruction.replace(\"{CONTEXT_RAG_HYDE}\", context_rag_hyde)\n",
    "    instruction = instruction.replace(\"{CONTEXT_RAG_GENERAL}\", context_rag_general)\n",
    "    instruction = instruction.replace(\"{SUMMARY_TABLE}\", table_insight)\n",
    "    return instruction\n",
    "\n",
    "def generate_prompt_extract_query(instruction, user_query):\n",
    "    \"\"\"\n",
    "    Generates a prompt for extracting keys from the user's query by replacing placeholders in the instruction template.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The template instruction containing a placeholder for the user's query.\n",
    "        user_query (str): The user's query to be inserted into the instruction.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated instruction with the placeholder replaced by the user's query.\n",
    "    \"\"\"\n",
    "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
    "    return instruction\n",
    "\n",
    "def parse_and_convert_keys(json_string):\n",
    "    \"\"\"\n",
    "    Parse the JSON string and convert the string values in the keys list to their appropriate types.\n",
    "\n",
    "    Args:\n",
    "    json_string (str): A JSON string representing a list of dictionaries with string values for 'thesis', 'figure', and 'table'.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries with 'thesis' as int, and 'figure' and 'table' as int or None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        keys = json.loads(json_string)\n",
    "        if not keys:\n",
    "            return []\n",
    "\n",
    "        converted_keys = []\n",
    "        for key in keys:\n",
    "            converted_key = {\n",
    "                \"thesis\": int(key[\"thesis\"]) if key[\"thesis\"] else None,\n",
    "                \"figure\": int(key[\"figure\"]) if key[\"figure\"] else None,\n",
    "                \"table\": int(key[\"table\"]) if key[\"table\"] else None\n",
    "            }\n",
    "            converted_keys.append(converted_key)\n",
    "        return converted_keys\n",
    "    except json.JSONDecodeError as e:\n",
    "        # print(f\"JSON decoding error: {e}\")\n",
    "        return []\n",
    "    except KeyError as e:\n",
    "        # print(f\"Missing key in JSON data: {e}\")\n",
    "        return []\n",
    "    except ValueError as e:\n",
    "        # print(f\"Value error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_descriptions(df, keys):\n",
    "    \"\"\"\n",
    "    Extract and format descriptions from the dataframe based on the provided keys.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The dataframe containing thesis, figure, table, and description data.\n",
    "    keys (list): A list of dictionaries with 'thesis' as int, and 'figure' and 'table' as int or None.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of formatted descriptions corresponding to the provided keys.\n",
    "    \"\"\"\n",
    "    formatted_descriptions = []\n",
    "\n",
    "    for key in keys:\n",
    "        thesis_num = key[\"thesis\"]\n",
    "        figure_num = key[\"figure\"]\n",
    "        table_num = key[\"table\"]\n",
    "\n",
    "        if figure_num is not None:\n",
    "            description = df[(df[\"thesis_num\"] == thesis_num) & (df[\"figure_num\"] == figure_num)][\"description\"].values\n",
    "            prefix = f\"thesis{thesis_num} figure{figure_num} description: \"\n",
    "        elif table_num is not None:\n",
    "            description = df[(df[\"thesis_num\"] == thesis_num) & (df[\"table_num\"] == table_num)][\"description\"].values\n",
    "            prefix = f\"thesis{thesis_num} table{table_num} description: \"\n",
    "        else:\n",
    "            description = []\n",
    "            prefix = \"\"\n",
    "\n",
    "        if len(description) > 0:\n",
    "            formatted_descriptions.append(prefix + description[0])\n",
    "        else:\n",
    "            formatted_descriptions.append(prefix + \"Description not found\")\n",
    "\n",
    "    return formatted_descriptions\n",
    "\n",
    "def extract_thesis_numbers(converted_keys):\n",
    "    \"\"\"\n",
    "    Extracts the thesis numbers from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "    converted_keys (list): A list of dictionaries with 'thesis', 'figure', and 'table' keys.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of thesis numbers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        thesis_numbers = [item['thesis'] for item in converted_keys]\n",
    "        return thesis_numbers\n",
    "    except Exception as e:\n",
    "        # print(f\"An error occurred while extracting thesis numbers: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_descriptions_for_thesis_summary(thesis_numbers, table_summary):\n",
    "    \"\"\"\n",
    "    Retrieves the descriptions for the given thesis numbers from the table_summary DataFrame.\n",
    "\n",
    "    Args:\n",
    "    thesis_numbers (list): A list of thesis numbers.\n",
    "    table_summary (pd.DataFrame): The DataFrame containing thesis numbers and their descriptions.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of descriptions corresponding to the thesis numbers, formatted to indicate which thesis each description belongs to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = []\n",
    "        for thesis_num in thesis_numbers:\n",
    "            description = table_summary.loc[table_summary['thesis_num'] == thesis_num, 'description'].values[0]\n",
    "            result.append(f\"Summary description for thesis {thesis_num}: '{description}'\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # print(f\"An error occurred while retrieving descriptions: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_prompt_extract_thesis_numbers(instruction, user_query):\n",
    "    instruction = instruction.replace(\"{USER_QUERY}\", user_query)\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "cNW2QePg1rwn"
   },
   "outputs": [],
   "source": [
    "# Claude\n",
    "client = anthropic.Anthropic(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "def get_response_claude(prompt, max_tokens=4096, temperature=0):\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model= \"claude-3-5-sonnet-20240620\",\n",
    "            # model= \"claude-3-haiku-20240307\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return message.content[0].text, message.usage\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtJr7Mot6hU4"
   },
   "source": [
    "## 3. Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "tGYKP0lnTIXe"
   },
   "outputs": [],
   "source": [
    "# Prompts(Instructions)\n",
    "\n",
    "instruction_hyde = \"\"\"\n",
    "### Instructions ###\n",
    "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.　If the information in the \"Context\" or \"Summary Table\" below seem relevant to \"Users' query\", please refer to it.\n",
    "\n",
    "### User’s query ###\n",
    "{USER_QUERY}\n",
    "\n",
    "### Context ###\n",
    "{CONTEXT_HYDE}\n",
    "\n",
    "### Summary Table ###\n",
    "{SUMMARY_TABLE}\n",
    "\n",
    "### Output ###\n",
    "\"\"\"\n",
    "\n",
    "instruction_final = \"\"\"\n",
    "### Instructions ###\n",
    "You are an expert in scientific academic papers. Your task is to answer to \"Users' query\" below.\n",
    "\n",
    "If the information in the \"Summary Table\", \"Figure/Table Context\" and \"Text Context\" below seem relevant to \"Users' query\", please refer to them.\n",
    "The \"Summary Table\" summarizes the key points of all academic papers. \"Text Context\" includes several chunks from different parts of an academic paper.　Each chunk also includes the name of the PDF. \"Figure/Table Context\" includes the descriptions related to figures or tables in an academic paper.\n",
    "Please refer only to the relevant contexts for your response. There is no need to include unrelated context in your response. If you refer to the Text Context for your answer, please include the PDF name. There is no need to include the chunk number.\n",
    "If the user asks about a specific figure or table and the information is contained in the Figure/Table Context, please ensure that this information is included in your response.\n",
    "If you determine that the previous conversation history is relevant, please also refer to that information to answer the user's query, especially when the the contexts below are empty.\n",
    "If the contexts and the previous conversation history do not contain the necessary information and it is difficult to answer even with general knowledge and previous context, please respond with 'The information provided is insufficient to answer your question.　Could you please clarify your question?'.\n",
    "\n",
    "##### User’s query #####\n",
    "{USER_QUERY}\n",
    "\n",
    "\n",
    "##### Summary Table #####\n",
    "{SUMMARY_TABLE}\n",
    "\n",
    "##### Figure/Table Context #####\n",
    "{CONTEXT_FIGURE_TABLE}\n",
    "\n",
    "##### Text Context #####\n",
    "{CONTEXT_RAG_HYDE}\n",
    "\n",
    "{CONTEXT_RAG_GENERAL}\n",
    "\n",
    "\n",
    "##### Output #####\n",
    "\"\"\"\n",
    "\n",
    "instruction_extract_query = \"\"\"\n",
    "### Instructions ###\n",
    "You are an NLP engineer. Your task is to extract the \"numbers\" from the user's query below.\n",
    "The \"numbers\" mean which academic paper the user is referring to, 2) which figure the user is referring to, and 3) which table the user is referring to.\n",
    "There may be cases where all, some, or none of these are specified. Enter the number only for the specified fields, and return an empty string \"\" for fields that are not specified.\n",
    "Interpret \"figure\" for terms such as \"Chart,\" \"Diagram,\" or \"Image.\" Interpret \"thesis\" for terms such as \"Academic Paper,\" \"Paper,\" or \"Document.\"\n",
    "Please provide your response as a list of objects, each containing thesis, figure, and table.　Please provide your response strictly in the specified format, without including any additional text for formatting. I will use your response directly.\n",
    "If it is unclear which thesis, figure, or table is being referred to, it is okay to return an empty string. Please do not make any assumptions.\n",
    "\n",
    "### Output Format ###\n",
    "Format: a list of objects\n",
    "\n",
    "### Example user's query1 ###\n",
    "What is the main hypothesis or research question addressed in the first academic article?\n",
    "\n",
    "### Example Output1 ###\n",
    "[\n",
    "  {\n",
    "  \"thesis\": \"1\",\n",
    "  \"figure\": \"\",\n",
    "  \"table\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "### Example user's query2 ###\n",
    "Summarize the methodology used in the third academic article. Highlight any unique approaches or techniques employed.\n",
    "\n",
    "### Example Output2 ###\n",
    "[\n",
    "  {\n",
    "  \"thesis\": \"3\",\n",
    "  \"figure\": \"\",\n",
    "  \"table\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "### Example user's query3 ###\n",
    "Q. From the images and figures in the second article, describe the trend shown in Figure 2. What does it indicate about the research findings?\n",
    "\n",
    "### Example Output3 ###\n",
    "[\n",
    "  {\n",
    "  \"thesis\": \"2\",\n",
    "  \"figure\": \"2\",\n",
    "  \"table\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "### Example user's query4 ###\n",
    "Q. What can be understood from Image 3 in the third paper?\n",
    "\n",
    "### Example Output4 ###\n",
    "[\n",
    "  {\n",
    "  \"thesis\": \"3\",\n",
    "  \"figure\": \"3\",\n",
    "  \"table\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "### Example user's query4 ###\n",
    "Q. Please explain Figure 3 and Table 2 of the second academic paper. What do these indicate about the research findings?\n",
    "\n",
    "### Example Output4 ###\n",
    "[\n",
    "  {\n",
    "  \"thesis\": \"2\",\n",
    "  \"figure\": \"3\",\n",
    "  \"table\": \"\"\n",
    "  },\n",
    "  {\n",
    "  \"thesis\": \"2\",\n",
    "  \"figure\": \"\",\n",
    "  \"table\": \"2\"\n",
    "  }\n",
    "]\n",
    "\n",
    "### Example user's query5 ###\n",
    "Q. Please compare table 3 and chart 4 from the second and third theses, respectively.\n",
    "\n",
    "### Example Output5 ###\n",
    "[\n",
    "  {\n",
    "  \"thesis\": \"2\",\n",
    "  \"figure\": \"\",\n",
    "  \"table\": \"3\"\n",
    "  },\n",
    "  {\n",
    "  \"thesis\": \"3\",\n",
    "  \"figure\": \"4\",\n",
    "  \"table\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "### Example user's query6 ###\n",
    "Do you like an apple?\n",
    "\n",
    "### Example Output6 ###\n",
    "[\n",
    "  {\n",
    "  \"thesis\": \"\",\n",
    "  \"figure\": \"\",\n",
    "  \"table\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "### Example user's query7 ###\n",
    "Considering the previous conversations, please propose a new research direction or hypothesis.\n",
    "\n",
    "### Example Output7 ###\n",
    "[\n",
    "  {\n",
    "  \"thesis\": \"\",\n",
    "  \"figure\": \"\",\n",
    "  \"table\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "### User’s query ###\n",
    "{USER_QUERY}\n",
    "\n",
    "### Output ###\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "instruction_thesis_numbers_extract_query = \"\"\"\n",
    "\n",
    "### Instructions ###\n",
    "You are an NLP engineer. Your task is to extract the academic paper numbers from the user's query below.\n",
    "The \"numbers\" mean which academic papers the user is referring to. Interpret \"Academic Paper\" for terms such as \"Thesis,\" \"Paper,\" or \"Document.\"\n",
    "The figure or table numbers may be included in the user's query, but please ignore them.\n",
    "If the user’s query does not specify a particular academic paper, respond with [1,2,3,4,5,6,7,8,9].\n",
    "Please provide your response as a list format, without any additional text for formatting. I will use your response directly.\n",
    "\n",
    "\n",
    "### Output Format ###\n",
    "Format: a list\n",
    "\n",
    "### Example user's query1 ###\n",
    "What is the main hypothesis or research question addressed in the first academic article?\n",
    "\n",
    "### Example Output1 ###\n",
    "[1]\n",
    "\n",
    "### Example user's query2 ###\n",
    "Summarize the methodology used in the third academic article. Highlight any unique approaches or techniques employed.\n",
    "\n",
    "### Example Output2 ###\n",
    "[3]\n",
    "\n",
    "### Example user's query3 ###\n",
    "From the images and figures in the second article, describe the trend shown in Figure 3. What does it indicate about the research findings?\n",
    "\n",
    "### Example Output3 ###\n",
    "[2]\n",
    "\n",
    "### Example user's query4 ###\n",
    "Please compare table 3 and chart 4 from the second and third theses, respectively.\n",
    "\n",
    "### Example Output4 ###\n",
    "[2,3]\n",
    "\n",
    "### Example user's query5 ###\n",
    "What are the encoder and decoder mentioned in these papers?\n",
    "\n",
    "### Example Output5 ###\n",
    "[1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "### Example user's query6 ###\n",
    "Which paper explains Llama?\n",
    "\n",
    "### Example Output6 ###\n",
    "[1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "\n",
    "### User’s query ###\n",
    "{USER_QUERY}\n",
    "\n",
    "### Output ###\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_D6noSE6pP-"
   },
   "source": [
    "## 4. Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkq63-IQ7pIo"
   },
   "source": [
    "### 4-1. Creating VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCzy_vdYTdVT",
    "outputId": "b060cf6b-f542-4f19-e83c-6d9284f60072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n",
      "MuPDF error: syntax error: cannot find ExtGState resource 'a0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Creating VectorDB\n",
    "# Load PDFs\n",
    "pdf_paths =  [\"attention.pdf\", \"Multimodal.pdf\", \"Performance Evaluation.pdf\", \"RAG Agent Resource-1.pdf\", \"Continual_Pretraining.pdf\", \"Challenges LLM July 19_23.pdf\", \"llm_review 2.pdf\", \"cs224n-2023-lecture11-prompting-rlhf.pdf\", \"Mistral.pdf\"]\n",
    "\n",
    "# Extract text from each PDF and create Document objects\n",
    "docs = extract_texts_from_pdfs(pdf_paths)\n",
    "\n",
    "## Chunk\n",
    "# Split the documents into chunks\n",
    "doc_splits = split_documents_into_chunks(docs)\n",
    "# Add chunk number to metadata\n",
    "doc_splits = add_chunk_numbers_to_metadata(doc_splits)\n",
    "\n",
    "## Embedding\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"allenai/scibert_scivocab_uncased\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "## Vector Store\n",
    "# Configure FAISS as Vector Store\n",
    "vector_db = configure_faiss_vector_store(doc_splits, embeddings)\n",
    "\n",
    "# Save the vector_db\n",
    "vectordb_path = \"vectordb_faiss\" # Change it to YOUR PATH\n",
    "save_faiss_vector_store(vector_db, vectordb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVygFqp272wg"
   },
   "source": [
    "### 4-2. Creating Image/Table Desctiption Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6nz3sfIaDA0"
   },
   "source": [
    "https://github.com/daichi6/llm-hackathon-insightai/blob/main/notebooks/image_description_generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_U9k1oFaFnI"
   },
   "source": [
    "- Creating Figure/Table Descriptions\n",
    "- Extracting Figure/Table Numbers from Images using LLM\n",
    "- Combining them into a Final Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiqnksVZ8L54"
   },
   "source": [
    "### 4-3. Creating Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZzVDDd9affF"
   },
   "source": [
    "https://github.com/daichi6/llm-hackathon-insightai/blob/main/notebooks/summary_table_generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl2fIQ-Uahqi"
   },
   "source": [
    "- Creating summaries for each academic paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIYnVSBb7AeK"
   },
   "source": [
    "## 5. Load prepared data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv0eAE5o8cCq"
   },
   "source": [
    "### 5-1. Load VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "WhW-IrYM7ZQx"
   },
   "outputs": [],
   "source": [
    "## Load prepared vectorDB\n",
    "# Load the vector_db\n",
    "vector_db = load_faiss_vector_store(vectordb_path, embeddings)\n",
    "# Create retrievers for each document and store them in a dictionary\n",
    "retrievers = create_retrievers(vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOdoAqLC8fuy"
   },
   "source": [
    "### 5-2. Load Image/Table Desctiption Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Adq-rtgv7C6V",
    "outputId": "bbdb88d5-3dc6-46be-fb11-c6d726e7d103"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"table_figure_table\",\n  \"rows\": 195,\n  \"fields\": [\n    {\n      \"column\": \"thesis_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"figure_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.6139291458145304,\n        \"min\": 1.0,\n        \"max\": 15.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          14.0,\n          12.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"table_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.431665995258196,\n        \"min\": 1.0,\n        \"max\": 23.0,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          16.0,\n          10.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 178,\n        \"samples\": [\n          \"I apologize, but I'm not able to analyze the image you've described titled 'tmpcx6x_0wr.png' as it is not present in the actual image shared. The image I see contains two photographs with annotations and questions, which appears to be part of a research paper or dataset.\\n\\nInstead, I'll provide an analysis of the actual image content:\\n\\n1. Image Type and Overview:\\nThis is a composite image from a scientific paper, showing two photographs with detailed annotations and questions. It's laid out in a table format with rows for different aspects of the images.\\n\\n2. Key Scientific Concepts:\\nThe image illustrates concepts related to computer vision, natural language processing, and artificial intelligence. It demonstrates how detailed image annotations can be used to train and evaluate AI models for image understanding and question answering tasks.\\n\\n3. Data Representation:\\nThe data is represented through photographs, text annotations, and questions. Each image has a detailed description and specific questions related to its content.\\n\\n4. Methodology Illustration:\\nThe image showcases a methodology for creating a dataset for AI training and evaluation. It demonstrates how to pair images with detailed annotations and questions to test an AI model's ability to understand and reason about image content.\\n\\n5. Results and Findings:\\nWhile no specific results are shown, the image implies that this approach can be used to create challenging examples for AI models, testing their ability to extract details from high-resolution images and demonstrate broad knowledge coverage.\\n\\n6. Technical Details:\\nThe annotations include specific details about the images, such as identifying ingredients in food, describing the layout of items in a refrigerator, and naming specific brands.\\n\\n7. Visual Aids and Annotations:\\nEach image has a source label, a detailed annotation describing the image content, and two questions designed to test AI comprehension of the image.\\n\\n8. Interdisciplinary Connections:\\nThis research connects computer vision, natural language processing, and potentially fields like gastronomy (for the food image) and consumer behavior (for the refrigerator contents).\\n\\n9. Limitations and Considerations:\\nThe approach may be limited by the subjectivity of human annotations and the specificity of questions, which might not cover all aspects of image understanding.\\n\\n10. Summary of Scientific Significance:\\nThis image demonstrates an approach to creating challenging, highly detailed datasets for training and evaluating AI models in tasks involving image understanding and question answering.\\n\\n11. Summary:\\nThe image shows two photographs - one of a ramen dish and one of an open refrigerator - along with detailed annotations and specific questions about each image. It's part of a scientific paper discussing AI model evaluation techniques.\",\n          \"I apologize, but the image you've provided does not appear to be from a scientific paper. Instead, it seems to be a partial screenshot of a mobile app or user interface design. Given this, I'll adjust my analysis to focus on the visible elements and content of the image:\\n\\n1. Image Type and Overview:\\n   - This is a user interface design or app screenshot.\\n   - The layout shows a partial view of what appears to be a chat or information interface with an icon and text.\\n\\n2. Key Concepts:\\n   - The main concept illustrated is user guidance or information provision in a mobile app context.\\n   - This relates to fields like user experience (UX) design and mobile app development.\\n\\n3. Data Representation:\\n   - There is no numerical data representation in this image.\\n\\n4. Methodology Illustration:\\n   - The image doesn't depict a scientific methodology, but rather a design approach for presenting information to users.\\n\\n5. Results and Findings:\\n   - Not applicable in the traditional scientific sense.\\n\\n6. Technical Details:\\n   - The image shows a stylized volcano icon labeled \\\"LLaVA\\\".\\n   - There's a text bubble with safety or guidance information for visitors to a location.\\n\\n7. Visual Aids and Annotations:\\n   - The volcano icon serves as a visual identifier.\\n   - The text provides user instructions or warnings.\\n\\n8. Interdisciplinary Connections:\\n   - This design could relate to fields like tourism, outdoor recreation, and public safety communication.\\n\\n9. Limitations and Considerations:\\n   - The partial nature of the image limits full context understanding.\\n   - We can't see the full app interface or functionality.\\n\\n10. Summary of Significance:\\n    - This image illustrates how mobile apps can be used to provide important safety information to users in specific contexts.\\n\\n11. Summary:\\n    - The image shows part of a mobile app interface with a stylized volcano icon labeled \\\"LLaVA\\\" and a text bubble providing visitor guidance for a peaceful location, emphasizing safety and regulations.\\n\\nThis image is not from a scientific paper and doesn't contain traditional scientific data or methodology. It appears to be a user interface design focusing on providing information to app users, possibly in a tourism or outdoor recreation context.\",\n          \"I apologize, but I'm unable to provide a comprehensive analysis as requested because the image you've shared is only a small fragment of a larger scientific figure or chart. The visible portion is too limited to draw meaningful conclusions about the full content and context. Here's what I can discern from the available information:\\n\\n1. Image Type and Overview:\\n   - The image appears to be a small part of a bar chart or graph from a scientific paper.\\n   - Only a small section of one bar and an icon are visible.\\n\\n2. Key Scientific Concepts:\\n   - The visible icon suggests the graph relates to finance or economics, as it shows a rising chart symbol with a dollar sign.\\n\\n3. Data Representation:\\n   - A partial vertical bar is visible, suggesting this is likely a bar chart comparing different categories or time periods.\\n\\n4-10. Cannot be addressed due to limited information.\\n\\n11. Summary:\\n   - The image fragment shows part of a financial or economic-related graph or chart from a scientific paper. It includes a finance icon and part of a data bar, but without more context, it's impossible to provide a detailed analysis or draw significant conclusions about the research, methodology, or findings represented.\\n\\nTo provide a more thorough and accurate analysis, I would need to see the complete image or figure from the scientific paper.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "table_figure_table"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-354e3a4d-5d48-439b-b9dc-1ff9abb66935\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thesis_num</th>\n",
       "      <th>figure_num</th>\n",
       "      <th>table_num</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I apologize, but the image you've provided doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I apologize, but the image you've provided doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1. Image Type and Overview:\\nThis image is a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1. Image Type and Overview:\\nThis image is a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1. Image Type and Overview:\\nThis image contai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-354e3a4d-5d48-439b-b9dc-1ff9abb66935')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-354e3a4d-5d48-439b-b9dc-1ff9abb66935 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-354e3a4d-5d48-439b-b9dc-1ff9abb66935');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-0a82bb33-7431-4e34-8391-8eb962303f05\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a82bb33-7431-4e34-8391-8eb962303f05')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-0a82bb33-7431-4e34-8391-8eb962303f05 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   thesis_num  figure_num  table_num  \\\n",
       "0           1         NaN        NaN   \n",
       "1           1         NaN        NaN   \n",
       "2           1         NaN        1.0   \n",
       "3           1         NaN        2.0   \n",
       "4           1         NaN        3.0   \n",
       "\n",
       "                                         description  \n",
       "0  I apologize, but the image you've provided doe...  \n",
       "1  I apologize, but the image you've provided doe...  \n",
       "2  1. Image Type and Overview:\\nThis image is a s...  \n",
       "3  1. Image Type and Overview:\\nThis image is a s...  \n",
       "4  1. Image Type and Overview:\\nThis image contai...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load prepared tables\n",
    "table_figure_table = pd.read_csv(\"image_analysis_results.csv\") # Change it to YOUR PATH\n",
    "table_figure_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9qvmZYX8jo3"
   },
   "source": [
    "### 5-3. Load Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5d-WbgiD8kxi",
    "outputId": "c3c5c1a8-e800-43e6-8a9f-b4b720ada53d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"table_summary\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"thesis_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 9,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          8,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"This lecture covers techniques for transforming large language models (LMs) into multitask AI assistants capable of following instructions and helping with a wide variety of tasks. It traces the evolution from basic next-word prediction models to more capable assistants:\\n\\n1. Zero-shot and few-shot prompting: This involves providing examples or instructions in the prompt to get LMs to perform new tasks without fine-tuning. Techniques like chain-of-thought prompting can improve performance on reasoning tasks. However, there are limits to what can fit in the context window.\\n\\n2. Instruction finetuning: This involves finetuning LMs on many examples of instructions and corresponding outputs across diverse tasks. Models like Flan-T5 show strong generalization to unseen tasks after instruction finetuning. However, collecting demonstrations for many tasks is expensive, and there's still a mismatch between the language modeling objective and human preferences.\\n\\n3. Reinforcement Learning from Human Feedback (RLHF): This uses reinforcement learning to directly optimize LMs for human preferences, rather than just predicting the next word. It involves training a separate reward model on human preference data, then using RL to optimize the LM to maximize the predicted reward. RLHF has been used to create assistants like InstructGPT and ChatGPT.\\n\\nThe lecture discusses the strengths and limitations of each approach. RLHF can produce more helpful and aligned assistants, but has challenges like reward hacking and over-optimization. There are also concerns about AI systems optimizing for unhelpful human preferences.\\n\\nThe lecture concludes by touching on very recent work aiming to reduce the data requirements of RLHF, like using AI feedback instead of human feedback. However, many core limitations of large language models (like tendency to hallucinate) may not be solvable just with RLHF. Overall, this is still a very active and fast-moving area of research in natural language processing and AI.\",\n          \"This text describes the development and capabilities of LLaVA (Large Language and Vision Assistant), a new multimodal AI model that combines visual and language understanding. The key points are:\\n\\n1. Background: The text begins by noting that instruction tuning has been shown to improve zero-shot capabilities of large language models (LLMs) on new tasks, but this approach has been less explored for multimodal models combining vision and language.\\n\\n2. Introduction of LLaVA: The authors present LLaVA as the first attempt to use a language-only model (GPT-4) to generate multimodal language-image instruction-following data. This data is then used to train an end-to-end multimodal model that connects a vision encoder with an LLM.\\n\\n3. Model Architecture: LLaVA combines a vision encoder (likely based on a pre-trained vision model) with a large language model for general-purpose visual and language understanding. \\n\\n4. Training Data: The model is trained on multimodal instruction-following data generated by GPT-4. This data includes image-text pairs with associated instructions and responses.\\n\\n5. Evaluation: The authors constructed two evaluation benchmarks with diverse and challenging application-oriented tasks to assess visual instruction following capabilities.\\n\\n6. Performance: LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting behaviors similar to multimodal GPT-4 on unseen images/instructions. It achieves an 85.1% relative score compared to GPT-4 on a synthetic multimodal instruction-following dataset.\\n\\n7. Fine-tuning Results: When fine-tuned on Science QA, the combination of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.\\n\\n8. Open-source Contribution: The authors make their GPT-4 generated visual instruction tuning data, model, and code publicly available to facilitate further research in this area.\\n\\n9. Methodology: The paper describes the process of using language-only GPT-4 to generate multimodal instruction-following data, which is then used to train LLaVA. This approach allows for the creation of diverse and complex multimodal training data without requiring manual annotation.\\n\\n10. Model Components: While not explicitly stated, it can be inferred that LLaVA likely uses a pre-trained vision model (such as CLIP or a similar architecture) as its vision encoder, combined with a large language model (possibly based on GPT or a similar architecture).\\n\\n11. Potential Applications: The capabilities demonstrated by LLaVA suggest its potential use in a wide range of applications involving visual understanding and language generation, such as image captioning, visual question answering, and multimodal dialogue systems.\\n\\n12. Limitations and Future Work: While not explicitly mentioned in this abstract, it's likely that the full paper discusses limitations of the current approach and potential directions for future research and improvements.\\n\\n13. Broader Impact: The development of models like LLaVA has significant implications for the field of AI, potentially leading to more advanced and capable AI assistants that can understand and communicate about visual information in natural language.\\n\\nThis research represents a significant step forward in multimodal AI, demonstrating the potential of using large language models to generate training data for multimodal tasks and showcasing the impressive capabilities that can result from combining advanced vision and language models.\",\n          \"This paper provides a comprehensive overview of the current state of large language models (LLMs), focusing on two key areas: challenges and applications.\\n\\nIn terms of challenges, the paper covers a wide range of issues:\\n\\n1. Dataset quality and transparency: The authors highlight problems with the massive datasets used to train LLMs, including near-duplicates, benchmark contamination, and the inclusion of personally identifiable information. They also note the difficulty in understanding the composition of these large datasets.\\n\\n2. Tokenization limitations: The paper discusses how current tokenization methods can lead to unfairness across languages and other issues.\\n\\n3. Computational costs: The authors detail the enormous computational resources required for training and inference with LLMs, as well as various techniques being developed to address this, such as efficient attention mechanisms and model compression methods.\\n\\n4. Limited context length: The paper explores the challenges LLMs face in handling long inputs and various approaches to extend context length.\\n\\n5. Prompt engineering and brittleness: The authors discuss the sensitivity of LLMs to prompt wording and format, and various prompting techniques developed to improve performance.\\n\\n6. Alignment and safety: The paper covers issues related to ensuring LLMs behave in accordance with human values and expectations, including techniques like reinforcement learning from human feedback (RLHF).\\n\\n7. Hallucinations and factuality: The authors discuss the tendency of LLMs to generate false or unsupported information and methods to mitigate this.\\n\\n8. Outdated knowledge and model updating: The paper explores the challenge of keeping LLMs up-to-date and methods for efficient model editing.\\n\\n9. Evaluation challenges: The authors highlight issues with current evaluation methods for LLMs and the need for more robust benchmarks.\\n\\nIn terms of applications, the paper provides an extensive survey of how LLMs are being used across various domains:\\n\\n1. Chatbots and dialogue systems\\n2. Computational biology, including protein structure prediction\\n3. Computer programming and code generation\\n4. Creative work, such as story and script writing\\n5. Knowledge work in various professional fields\\n6. Legal applications, including question answering and case prediction\\n7. Medical applications, such as medical question answering and information extraction\\n8. Reasoning tasks, including mathematical and causal reasoning\\n9. Robotics and embodied agents\\n10. Social sciences and psychology research\\n11. Synthetic data generation for training smaller models\\n\\nFor each application area, the authors discuss current approaches, successes, and limitations. They also highlight specific challenges relevant to each domain, such as the need for factual accuracy in medical applications or the difficulty of long-term planning in robotics.\\n\\nThe paper concludes by emphasizing the rapid pace of development in the field of LLMs and the ongoing need for research to address the challenges identified while responsibly advancing the capabilities and applications of these powerful models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "table_summary"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0907eed0-9f73-4953-b382-aebc54a37fc1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thesis_num</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The paper \"Attention Is All You Need\" introduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This text describes the development and capabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>This paper presents a comprehensive evaluation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The provided text is a comprehensive guide cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This paper explores how to effectively adapt l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0907eed0-9f73-4953-b382-aebc54a37fc1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0907eed0-9f73-4953-b382-aebc54a37fc1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0907eed0-9f73-4953-b382-aebc54a37fc1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-728ad391-5162-4f26-b17e-521bc5e21b0c\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-728ad391-5162-4f26-b17e-521bc5e21b0c')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-728ad391-5162-4f26-b17e-521bc5e21b0c button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   thesis_num                                        description\n",
       "0           1  The paper \"Attention Is All You Need\" introduc...\n",
       "1           2  This text describes the development and capabi...\n",
       "2           3  This paper presents a comprehensive evaluation...\n",
       "3           4  The provided text is a comprehensive guide cov...\n",
       "4           5  This paper explores how to effectively adapt l..."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load table with thesis_num and description\n",
    "table_summary = pd.read_csv(\"summaries.csv\") # Change it to YOUR PATH\n",
    "table_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-5zra1ztRbB"
   },
   "source": [
    "### 5-4. Load Insight Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DNZXsKkitWlp",
    "outputId": "c51f3c0c-17e6-4ac0-ed91-07ba4bcf4d2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"table_insight\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"PDF Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"cs224n-2023-lecture11-prompting-rlhf.pdf\",\n          \"Multimodal.pdf\",\n          \"Challenges LLM July 19_23.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Paper Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Natural Language Processing\",\n          \"Visual Instruction Tuning\",\n          \"Challenges and Applications of Large Language Models\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Author Names\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"with Deep Learning\",\n          \"Haotian Liu1\\u2217, Chunyuan Li2\\u2217, Qingyang Wu3, Yong Jae Lee1\",\n          \"Jean Kaddour\\u03b1,\\u2020,\\u2217, Joshua Harris\\u03b2,\\u2217, Maximilian Mozes\\u03b1,\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Short Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Natural Language Processing with Deep Learning CS224N/Ling284 Jesse Mu Lecture 11: Prompting, Instruction Finetuning, and RLHF Reminders \\u2022Project proposals (both custom and final) due a few minutes ago! \\u2022We\\u2019re in the process of assigning mentors to projects and will aim to give feedback on project proposals with a quick turnaround \\u2022A5 due Friday 11:59PM! \\u2022We still recommend using Colab for the assignments; in case you run into trouble (e.g. you have exceeded Colab quota), instructions for connecting to a Kaggle notebook have been posted on Ed 2 Larger and larger models 3 https:// www.economist.com /interactive/briefing/2022/06/11/huge -foundation -models -are-turbo -charging -ai-progress...\",\n          \"Visual Instruction Tuning Haotian Liu1\\u2217, Chunyuan Li2\\u2217, Qingyang Wu3, Yong Jae Lee1 1University of Wisconsin\\u2013Madison2Microsoft Research3Columbia University https://llava-vl.github.io Abstract Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we in- troduce LLaV A: Large Language andVision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general- purpose visual and language...\",\n          \"Challenges and Applications of Large Language Models Jean Kaddour\\u03b1,\\u2020,\\u2217, Joshua Harris\\u03b2,\\u2217, Maximilian Mozes\\u03b1, Herbie Bradley\\u03b3,\\u03b4,\\u03f5, Roberta Raileanu\\u03b6, and Robert McHardy\\u03b7,\\u2217 \\u03b1University College London\\u03b2UK Health Security Agency\\u03b3EleutherAI \\u03b4University of Cambridge\\u03f5Stability AI\\u03b6Meta AI Research\\u03b7InstaDeep Abstract Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learn- ing discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to es- tablish a systematic set of open problems and application successes so that ML researchers can comprehend the field\\u2019s current state...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "table_insight"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-adfa4258-d697-4746-85bd-6f69e701ec45\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF Name</th>\n",
       "      <th>Paper Name</th>\n",
       "      <th>Author Names</th>\n",
       "      <th>Short Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>Ashish Vaswani\u0003</td>\n",
       "      <td>Attention Is All You Need Ashish Vaswani\u0003 Goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multimodal.pdf</td>\n",
       "      <td>Visual Instruction Tuning</td>\n",
       "      <td>Haotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yo...</td>\n",
       "      <td>Visual Instruction Tuning Haotian Liu1∗, Chuny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Performance Evaluation.pdf</td>\n",
       "      <td>A Multitask, Multilingual, Multimodal Evaluati...</td>\n",
       "      <td>on Reasoning, Hallucination, and Interactivity</td>\n",
       "      <td>A Multitask, Multilingual, Multimodal Evaluati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAG Agent Resource-1.pdf</td>\n",
       "      <td>Part-I: What is an Agent?</td>\n",
       "      <td>Short Answer: Text-to-Task An LLM agent is an ...</td>\n",
       "      <td>Part-I: What is an Agent? Short Answer: Text-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Continual_Pretraining.pdf</td>\n",
       "      <td>ADAPTING LARGE LANGUAGE MODELS VIA</td>\n",
       "      <td>READING COMPREHENSION</td>\n",
       "      <td>ADAPTING LARGE LANGUAGE MODELS VIA READING COM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adfa4258-d697-4746-85bd-6f69e701ec45')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-adfa4258-d697-4746-85bd-6f69e701ec45 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-adfa4258-d697-4746-85bd-6f69e701ec45');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-79ebde03-152c-4939-ae00-7b1c22827af7\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79ebde03-152c-4939-ae00-7b1c22827af7')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-79ebde03-152c-4939-ae00-7b1c22827af7 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                     PDF Name  \\\n",
       "0               attention.pdf   \n",
       "1              Multimodal.pdf   \n",
       "2  Performance Evaluation.pdf   \n",
       "3    RAG Agent Resource-1.pdf   \n",
       "4   Continual_Pretraining.pdf   \n",
       "\n",
       "                                          Paper Name  \\\n",
       "0                          Attention Is All You Need   \n",
       "1                          Visual Instruction Tuning   \n",
       "2  A Multitask, Multilingual, Multimodal Evaluati...   \n",
       "3                          Part-I: What is an Agent?   \n",
       "4                 ADAPTING LARGE LANGUAGE MODELS VIA   \n",
       "\n",
       "                                        Author Names  \\\n",
       "0                                    Ashish Vaswani\u0003   \n",
       "1  Haotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yo...   \n",
       "2     on Reasoning, Hallucination, and Interactivity   \n",
       "3  Short Answer: Text-to-Task An LLM agent is an ...   \n",
       "4                              READING COMPREHENSION   \n",
       "\n",
       "                                   Short Description  \n",
       "0  Attention Is All You Need Ashish Vaswani\u0003 Goog...  \n",
       "1  Visual Instruction Tuning Haotian Liu1∗, Chuny...  \n",
       "2  A Multitask, Multilingual, Multimodal Evaluati...  \n",
       "3  Part-I: What is an Agent? Short Answer: Text-t...  \n",
       "4  ADAPTING LARGE LANGUAGE MODELS VIA READING COM...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load insight table\n",
    "table_insight = pd.read_csv(\"insights.csv\") # Change it to YOUR PATH\n",
    "table_insight.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdIrzw0j6snA"
   },
   "source": [
    "## 6. Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "iPq1fnTHCORS"
   },
   "outputs": [],
   "source": [
    "## User selection\n",
    "# User thesis selection before asking questions\n",
    "pdf_paths_user_selected =  [\"attention.pdf\", \"Multimodal.pdf\", \"Performance Evaluation.pdf\", \"RAG Agent Resource-1.pdf\", \"Continual_Pretraining.pdf\", \"Challenges LLM July 19_23.pdf\", \"llm_review 2.pdf\", \"cs224n-2023-lecture11-prompting-rlhf.pdf\", \"Mistral.pdf\"] # Change it to YOUR PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "17XIY37dcj1a"
   },
   "outputs": [],
   "source": [
    "# LLM for the main flow\n",
    "client_main = Groq(\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    ")\n",
    "#  LLM for keys(thesis/figure/table) extaction\n",
    "client_extract = Groq(\n",
    "    api_key=\"YOUR_API_KEY\"\n",
    ")\n",
    "# LLM for HyDE\n",
    "client_hyde = Groq(\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "-KOsa_idg_2_"
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "def chat_main(user_query):\n",
    "\n",
    "  ## 1. extract keys from user's query and find figure/table description and summary description ##\n",
    "  # generate prompt to extract keys from user's query\n",
    "  prompt_extract_query = generate_prompt_extract_query(instruction_extract_query, user_query)\n",
    "  # get keys from Extraction LLM\n",
    "  response_keys = get_groq_response(client_extract, prompt_extract_query)\n",
    "\n",
    "  # parse keys\n",
    "  keys = parse_and_convert_keys(response_keys[0])\n",
    "\n",
    "  # extract figure/table descriptions\n",
    "  descriptions_figure_table = extract_descriptions(table_figure_table, keys)\n",
    "\n",
    "  # extract thesis numbers from keys\n",
    "  keys_thesis = extract_thesis_numbers(keys)\n",
    "  # get summary descriptions\n",
    "  descriptions_summary = get_descriptions_for_thesis_summary(keys_thesis, table_summary)\n",
    "\n",
    "  ## 2. get context for HyDE and general RAG ##\n",
    "  # add summary of the thesis as a context for HyDE\n",
    "  context_hyde = descriptions_summary\n",
    "\n",
    "  # create prompt for HyDE - ADDED summary table as a context for HyDE(not insight table as summary is more detai than insight)\n",
    "  # prompt_hyde = generate_prompt_hyde(instruction_hyde, user_query, str(context_hyde))\n",
    "  prompt_hyde = generate_prompt_hyde(instruction_hyde, user_query, str(context_hyde), str(table_summary))\n",
    "  # get a hypothetical answer from HyDE LLM\n",
    "  response_hyde = get_groq_response(client_hyde, prompt_hyde)\n",
    "\n",
    "  ## 2. create contexts\n",
    "  # initialize empty strings for contexts\n",
    "  context_rag_hyde = \"\"\n",
    "  context_rag_general = \"\"\n",
    "\n",
    "  # thesis number lists(if user does not specify, then all thesis[1,....,9]) ONLY FOR RAG(NOT for figure/table descriptions)\n",
    "  prompt_extract_thesis_numbers = generate_prompt_extract_thesis_numbers(instruction_thesis_numbers_extract_query, user_query)\n",
    "  response_thesis_numbers = get_groq_response(client_extract, prompt_extract_thesis_numbers)\n",
    "  keys_thesis_case2 = ast.literal_eval(response_thesis_numbers[0])\n",
    "\n",
    "  # search for documents based on keys_thesis_cases2(thesis number extracted from user's query or all pdfs if user did not specify)\n",
    "  # need to add error handling when len(keys_thesis_case2) > the number of PDFs\n",
    "  if keys_thesis_case2 and all(key is not None for key in keys_thesis_case2):\n",
    "      for key in keys_thesis_case2:\n",
    "          if isinstance(key, int):\n",
    "              adjusted_key = key - 1  # adjust the key by subtracting 1\n",
    "              doc_source = pdf_paths_user_selected[adjusted_key]  # get the document source(FROM USER'S SELECTED LISTS) based on the adjusted key\n",
    "              retriever = retrievers[doc_source]  # get the corresponding retriever\n",
    "\n",
    "              # process query for RAG(Hyde)\n",
    "              result_hyde = process_query(response_hyde[0], retriever)\n",
    "              context_rag_hyde += f\"Document {key}:\\n{result_hyde}\\n\"\n",
    "\n",
    "              # process query for RAG(General)\n",
    "              result_general = process_query(user_query, retriever)\n",
    "              context_rag_general += f\"Document {key}:\\n{result_general}\\n\"\n",
    "  else:\n",
    "      context_rag_hyde = \"\"\n",
    "      context_rag_general = \"\"\n",
    "\n",
    "  ## 3. get a final response ##\n",
    "  # create prompt for a final response\n",
    "  prompt_final = generate_prompt_final(instruction_final, user_query, str(descriptions_figure_table), context_rag_hyde, context_rag_general, str(table_insight))\n",
    "  # get final response from main LLM\n",
    "  # response_final = get_groq_response(client_main, prompt_final)\n",
    "  response_final = get_response_claude(prompt = prompt_final)\n",
    "\n",
    "  return response_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI_h3T7q64Td"
   },
   "source": [
    "## 7. Test questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgIMCtwd3R7t",
    "outputId": "53e0c9d3-bec4-4354-baa9-4716cfffa118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the term \"object index\" is not explicitly defined or discussed in detail. However, I can provide some relevant information that may be related to the concept of object indexing in the context of language models and information retrieval:\n",
      "\n",
      "1. In the context of retrieval-augmented language models, there is mention of using search indexes to store and retrieve relevant information. For example, from the \"Challenges LLM July 19_23.pdf\":\n",
      "\n",
      "\"We can decouple (i) memory storage of knowledge (e.g., databases or search indexes) and (ii) processing of the knowledge to arrive at a more modular architecture. For (i), a retriever module retrieves the top-k relevant documents (or passages) for a query from a large corpus of text.\"\n",
      "\n",
      "2. In the context of visual tasks, there is mention of using API functions to locate and interact with objects. From the \"Challenges LLM July 19_23.pdf\":\n",
      "\n",
      "\"The Codex model is prompted with the query text and an API specification to do this. The human-generated API specification provides functions designed to deal with low-level visual tasks (e.g., find(object)) that the LLM can then use to generate solution code.\"\n",
      "\n",
      "3. In the context of attention mechanisms in language models, there is discussion of attending to different positions or objects in a sequence. From the \"attention.pdf\":\n",
      "\n",
      "\"Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\"\n",
      "\n",
      "While these concepts are related to object indexing in various ways, they don't provide a direct definition of \"object index\". If you're looking for a specific definition or usage of the term \"object index\", you may need to provide more context or clarify the specific domain or application you're interested in.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'What is object index?'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gwxng9c26cQF",
    "outputId": "53e5276b-3c67-4fd7-caf1-c76664143615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the context, LLaVA (Large Language and Vision Assistant) demonstrates strong performance across multiple image domains and subjects:\n",
      "\n",
      "1. Out-of-domain performance: LLaVA is able to understand scenes and follow question instructions to provide reasonable responses even for images that are out of its training domain. This suggests good generalization capabilities across different visual domains (Multimodal.pdf).\n",
      "\n",
      "2. Quantitative evaluation: A systematic evaluation was conducted to assess LLaVA's performance across various aspects, including accuracy, concept coverage, reasoning ability, and creativity (Multimodal.pdf).\n",
      "\n",
      "3. Benchmark performance: LLaVA significantly outperforms other models like BLIP-2 and OpenFlamingo on visual instruction following tasks (Multimodal.pdf).\n",
      "\n",
      "4. Emergent behavior: LLaVA demonstrates the ability to understand visual contents not covered in its training data. For example, it can recognize individuals like Elon Musk in different contexts, even though such images were not part of its training (Multimodal.pdf).\n",
      "\n",
      "5. Multimodal capabilities: LLaVA performs well on tasks that require both language and visual understanding, showing its effectiveness as a general-purpose visual assistant (Multimodal.pdf).\n",
      "\n",
      "6. Evaluation challenges: The paper notes that assessing LLaVA's performance is complex due to the involvement of both language and visual tasks. While their evaluation benchmark covers several aspects, they acknowledge that additional considerations like visual content hallucination and fine-grained visual understanding need further investigation (Multimodal.pdf).\n",
      "\n",
      "Overall, LLaVA appears to demonstrate strong and versatile performance across various image domains and subjects, with capabilities in understanding, reasoning, and following instructions for diverse visual inputs. However, the authors also note the challenges in comprehensively evaluating such multimodal models and suggest areas for further assessment.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'What is the performance of LLaVa across across multiple image domains / subjects?'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tqb9ONRo6ol3",
    "outputId": "dc12476e-1819-4d94-fe2b-d4908ae440d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the Summary Table, the following papers have been processed:\n",
      "\n",
      "1. attention.pdf - \"Attention Is All You Need\" by Ashish Vaswani\n",
      "2. Multimodal.pdf - \"Visual Instruction Tuning\" by Haotian Liu et al.\n",
      "3. Performance Evaluation.pdf - \"A Multitask, Multilingual, Multimodal Evaluation on Reasoning, Hallucination, and Interactivity\"\n",
      "4. RAG Agent Resource-1.pdf - \"Part-I: What is an Agent?\"\n",
      "5. Continual_Pretraining.pdf - \"ADAPTING LARGE LANGUAGE MODELS VIA READING COMPREHENSION\"\n",
      "6. Challenges LLM July 19_23.pdf - \"Challenges and Applications of Large Language Models\" by Jean Kaddour et al.\n",
      "7. llm_review 2.pdf - \"ANOVERVIEW ON LANGUAGE MODELS : RECENT DEVELOPMENTS AND OUTLOOK\"\n",
      "8. cs224n-2023-lecture11-prompting-rlhf.pdf - \"Natural Language Processing with Deep Learning\"\n",
      "9. Mistral.pdf - \"Mistral 7B\" by Albert Q. Jiang et al.\n",
      "\n",
      "These 9 papers cover a range of topics related to language models, including attention mechanisms, visual instruction tuning, performance evaluation, RAG agents, continual pretraining, challenges and applications of LLMs, and specific models like Mistral 7B.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'Can you provide a list of papers that have been processed?'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xu0DPDr36v_8",
    "outputId": "92bbe075-44f8-4062-a0dc-8b247ccf9043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the context, the largest GPT model mentioned is GPT-3 with 175 billion parameters. This is cited in multiple documents:\n",
      "\n",
      "1. From the \"Performance Evaluation.pdf\":\n",
      "\"Large Language Models (LLMs) are language models with parameter sizes over a hundred billion, beginning with the introduction of GPT-3. Examples of LLMs include, but are not limited to, GPT-3, Gopher (Rae et al., 2021b), Megatron (Shoeybi et al., 2019), GPT-Jurassic (Lieber et al., 2021), OPT-175B Zhang et al. (2022).\"\n",
      "\n",
      "2. From \"llm_review 2.pdf\":\n",
      "\"GPT-3 2020 175B 45TB of text data $12 million\"\n",
      "\n",
      "3. From \"cs224n-2023-lecture11-prompting-rlhf.pdf\":\n",
      "\"GPT-3 (175B parameters; Brown et al., 2020)\"\n",
      "\n",
      "These sources consistently mention GPT-3 as having 175 billion parameters, which is the largest GPT model size provided in the given context.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'What is the largest GPT model provided in the papers? please cite.'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJ0WPPhu4lkn",
    "outputId": "65ee2f75-31cb-4732-c3a4-32a140b6b5f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the paper that most directly covers how to process multiple data for RAG (Retrieval-Augmented Generation) is the \"RAG Agent Resource-1.pdf\". This document discusses advanced RAG techniques for handling multiple documents. Specifically:\n",
      "\n",
      "1. It mentions \"Advanced RAG – Multi Documents Agent with LlamaIndex\" which is directly relevant to processing multiple data sources for RAG.\n",
      "\n",
      "2. The document provides a sample implementation link for multi-document RAG using LlamaIndex: https://github.com/sugarforever/Advanced-RAG/blob/main/03_llama_index_multi_doc_agent.ipynb\n",
      "\n",
      "3. It discusses the limitations of standard RAG when dealing with multiple documents, stating that RAG can't answer questions that require \"Scanning, Comparing, and Reasoning across all documents in your knowledge base simultaneously.\"\n",
      "\n",
      "4. The paper proposes an architecture to overcome these limitations:\n",
      "   - Set up a \"document agent\" for each document that can do QA/summarization within its doc\n",
      "   - Set up a top-level agent over this set of document agents\n",
      "   - Use tool retrieval and chain-of-thought reasoning over the set of tools to answer questions\n",
      "\n",
      "5. It mentions additional features like comparing summaries between different documents.\n",
      "\n",
      "This paper provides a comprehensive overview of how to process and reason across multiple data sources in an advanced RAG setup, making it the most relevant to your query about processing multiple data for RAG.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'Which paper covers how to process multiple data for RAG?'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2rqfE7j7Z9t",
    "outputId": "a82e41e9-ac38-45eb-89e4-2f02a758e385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can integrate the findings from efficient attention, continual pretraining, and performance evaluation, and list the relevant authors:\n",
      "\n",
      "1. Efficient Attention:\n",
      "- Ashish Vaswani et al. (authors of \"Attention Is All You Need\")\n",
      "- Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré (authors of \"FlashAttention\")\n",
      "\n",
      "2. Continual Pretraining:\n",
      "- The specific authors are not mentioned, but the concept is discussed in the Continual_Pretraining.pdf document. The approach involves converting raw texts into reading comprehension tasks and including general instructions to improve domain-specific knowledge while preserving prompting performance.\n",
      "\n",
      "3. Performance Evaluation:\n",
      "- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (authors of \"Measuring massive multitask language understanding\")\n",
      "- The Performance Evaluation.pdf document discusses evaluations of ChatGPT, but specific author names are not provided in the given context.\n",
      "\n",
      "It's worth noting that while these authors have made significant contributions to their respective areas, the integration of these concepts is not explicitly attributed to a single set of authors in the provided context. The integration seems to be a synthesis of ideas from various research efforts in the field of large language models.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'Integrate the findings from efficient attention, continual pretraining, and performance evaluation. List the names of the authors'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fObNLX727rdI",
    "outputId": "508c588b-7b97-4db3-919f-4e22dd1d767a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the paper that focuses on continued pretraining is \"Continual_Pretraining.pdf\". This paper investigates the concept of continued pre-training for large language models.\n",
      "\n",
      "Continued pretraining, also referred to as domain-adaptive pretraining, involves further training a pre-trained language model on domain-specific corpora. The goal is to adapt the model to specific domains while leveraging its general abilities acquired during initial pre-training.\n",
      "\n",
      "Key points about continued pretraining from this paper include:\n",
      "\n",
      "1. It's an approach to adapt large language models to specific domains like biomedicine, finance, and law.\n",
      "\n",
      "2. The authors found that continued training on domain-specific raw corpora can endow the model with domain knowledge, but it can also hurt the model's prompting ability.\n",
      "\n",
      "3. To address this issue, they propose a method of converting raw corpora into reading comprehension texts for continued pre-training, which helps preserve prompting performance while gaining domain knowledge.\n",
      "\n",
      "4. Their experiments showed that this approach consistently improved model performance across the three tested domains (biomedicine, finance, and law).\n",
      "\n",
      "The authors note that continued pre-training has been proven effective in adapting natural language understanding models in previous studies, which motivated their investigation into whether it could also benefit large-scale generative models.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'What is continued pretraining and which paper focuses on this topic?'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaCEwNJl8SKW"
   },
   "source": [
    "## 8. Sample questions(on Canvas) and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9X23uKJ8673",
    "outputId": "8b4e0162-a73d-4b0c-8ade-f1bfe3cf5c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the Text Context from the \"attention.pdf\" document, the main hypothesis or research question addressed in the first academic article is:\n",
      "\n",
      "The paper proposes a new network architecture called the Transformer, which is based solely on attention mechanisms, without using recurrence or convolutions for sequence transduction tasks.\n",
      "\n",
      "This can be inferred from several key points in the context:\n",
      "\n",
      "1. From the abstract (attention.pdf, chunk 1):\n",
      "\"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\"\n",
      "\n",
      "2. Further elaboration (attention.pdf, chunk 3):\n",
      "\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\"\n",
      "\n",
      "3. The novelty of the approach is highlighted (attention.pdf, chunk 4):\n",
      "\"To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\"\n",
      "\n",
      "The researchers aim to demonstrate that this new architecture can perform well on sequence transduction tasks, particularly machine translation, while being more efficient to train than previous architectures based on recurrent or convolutional neural networks.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'What is the main hypothesis or research question addressed in the first academic article?'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNbJr62o9CP8",
    "outputId": "aa5efec3-f4a1-400a-8e31-5c0410f181e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the Text Context from the Multimodal.pdf, one key finding from this academic article is:\n",
      "\n",
      "LLaVA (Large Language and Vision Assistant) demonstrates impressive multimodal chat abilities and yields a 85.1% relative score compared to GPT-4 on a synthetic multimodal instruction-following benchmark.\n",
      "\n",
      "This finding is significant because it shows that LLaVA, the model developed in this study, performs well in tasks that combine visual and language understanding. The high relative score compared to GPT-4, which is considered a state-of-the-art model, indicates that LLaVA is capable of handling complex multimodal tasks effectively.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'Identify one key finding from the second academic article.'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lw1c7cnW9GDD",
    "outputId": "47c8ce5f-b8f7-4c3a-a4f5-5a3e34e2e452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the Text Context from the \"Performance Evaluation.pdf\", I can summarize the methodology used in this academic article as follows:\n",
      "\n",
      "1. Multitask Evaluation: The researchers conducted experiments on ChatGPT using samples from standard public test sets across various NLP tasks. These tasks included question answering, reasoning, summarization, machine translation, automatic post-editing, sentiment analysis, and language identification.\n",
      "\n",
      "2. Multilingual Evaluation: The study assessed ChatGPT's performance across multiple languages, though specific details about this aspect are not provided in the given context.\n",
      "\n",
      "3. Multimodal Evaluation: While mentioned in the title, specific details about the multimodal evaluation are not present in the provided context.\n",
      "\n",
      "4. Interactive Evaluation: The researchers explored the impact of multi-turn interactivity on performance in various NLP tasks. This approach showed significant improvements in some areas, such as an 8% increase in ROUGE-1 scores for summarization and a 2% improvement in ChrF++ scores for low-resource machine translation.\n",
      "\n",
      "5. Comparative Analysis: The study likely compared ChatGPT's performance against human experts and possibly other AI models, though specific details are limited in the given context.\n",
      "\n",
      "6. Quantitative Framework: The paper proposes a framework for quantitatively evaluating interactive Large Language Models (LLMs) like ChatGPT, focusing on reasoning, hallucination, and interactivity.\n",
      "\n",
      "Unique approaches or techniques employed:\n",
      "\n",
      "1. Multi-turn Interactivity: The researchers utilized a process similar to prompt engineering with feedback from the system, which is a novel approach to improving performance on standard NLP tasks.\n",
      "\n",
      "2. Comprehensive Task Coverage: The study covers a wide range of NLP tasks, providing a broad evaluation of ChatGPT's capabilities.\n",
      "\n",
      "3. Focus on Emergent Abilities: The research aimed to analyze the specific emergent abilities and strengths of ChatGPT from a technical perspective, going beyond general use cases.\n",
      "\n",
      "4. Analysis of Failure Cases: Despite ChatGPT's overall good performance, the researchers also identified and analyzed failure cases in various tasks, highlighting areas for improvement.\n",
      "\n",
      "This methodology provides a comprehensive evaluation of ChatGPT's capabilities across multiple dimensions, including task variety, language diversity, and interactive performance improvement.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'Summarize the methodology used in the third academic article. Highlight any unique approaches or techniques employed.'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkudXqp39Lmo",
    "outputId": "71b06c00-834f-456e-9194-8b4c940c36c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the Figure/Table Context provided, I can describe the trend shown in Figure 2 of the paper \"Attention Is All You Need\" (attention.pdf). \n",
      "\n",
      "Figure 2 illustrates two key components of the Transformer architecture: Scaled Dot-Product Attention and Multi-Head Attention.\n",
      "\n",
      "The left side of Figure 2 shows the Scaled Dot-Product Attention mechanism. The trend it indicates is a sequential flow of operations:\n",
      "\n",
      "1. It starts with input matrices Q (Query), K (Key), and V (Value).\n",
      "2. These inputs go through a series of operations:\n",
      "   - Matrix multiplication (MatMul) of Q and K\n",
      "   - Scaling\n",
      "   - Optional masking\n",
      "   - Softmax operation\n",
      "   - Another matrix multiplication with V\n",
      "\n",
      "The right side of Figure 2 depicts Multi-Head Attention, which shows multiple attention mechanisms operating in parallel.\n",
      "\n",
      "This figure indicates several key research findings:\n",
      "\n",
      "1. The Transformer model uses a novel attention mechanism that doesn't rely on recurrent or convolutional neural networks.\n",
      "\n",
      "2. The attention mechanism can process input data in parallel, potentially leading to faster training times compared to sequential models.\n",
      "\n",
      "3. The use of multiple attention heads allows the model to attend to different aspects of the input simultaneously, potentially capturing more complex relationships in the data.\n",
      "\n",
      "4. The scaled dot-product attention addresses potential issues with very large input dimensions by including a scaling factor.\n",
      "\n",
      "These trends shown in Figure 2 represent a significant innovation in neural network architecture for sequence transduction tasks, particularly in natural language processing. The figure illustrates how the Transformer model achieves its high performance through these attention mechanisms, which form the core of its design.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'From the images and figures in the first article, describe the trend shown in Figure 2. What does it indicate about the research findings?'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1CHhEd89OpJ",
    "outputId": "3ed94168-6747-42d2-e6b4-af4b63d65212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the text context from attention.pdf, I can critically evaluate the statistical methods used in the article \"Attention Is All You Need\" by Ashish Vaswani et al. Here are the key points:\n",
      "\n",
      "1. Performance Metric:\n",
      "The primary statistical method used to evaluate the model's performance is the BLEU score. This is a standard metric in machine translation tasks, which allows for comparison with other models. However, it's worth noting that while BLEU is widely used, it has limitations in capturing the full quality of translations.\n",
      "\n",
      "Strength: Using a standardized metric allows for direct comparison with other models in the field.\n",
      "Limitation: BLEU scores alone may not capture all aspects of translation quality, such as fluency or semantic accuracy.\n",
      "\n",
      "2. Comparative Analysis:\n",
      "The authors compare their model's performance against previous state-of-the-art models, including both single models and ensembles. This provides a clear benchmark for evaluating the Transformer's effectiveness.\n",
      "\n",
      "Strength: Comprehensive comparison against multiple existing models demonstrates the robustness of their results.\n",
      "\n",
      "3. Ablation Studies:\n",
      "The researchers conducted model variations to evaluate the importance of different components of the Transformer. This is evident from the statement: \"To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.\"\n",
      "\n",
      "Strength: Ablation studies provide insights into which components of the model contribute most to its performance.\n",
      "\n",
      "4. Multiple Datasets:\n",
      "The model was tested on both English-to-German and English-to-French translation tasks from WMT 2014, which demonstrates its effectiveness across different language pairs.\n",
      "\n",
      "Strength: Testing on multiple datasets increases the generalizability of the results.\n",
      "\n",
      "5. Training Efficiency:\n",
      "The authors report the training time and computational resources used (e.g., \"Training took 3.5 days on 8 P100 GPUs\"). They also estimate the computational cost in terms of floating-point operations (FLOPs).\n",
      "\n",
      "Strength: Providing details on computational efficiency allows for a more comprehensive evaluation of the model's practicality and scalability.\n",
      "\n",
      "6. Model Averaging:\n",
      "For the final results, the authors used model averaging: \"For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints.\"\n",
      "\n",
      "Strength: Model averaging can help reduce variance and improve generalization.\n",
      "\n",
      "7. Hyperparameter Tuning:\n",
      "The paper mentions adjusting hyperparameters like dropout rate and beam search parameters, indicating attention to model optimization.\n",
      "\n",
      "Limitation: The paper doesn't provide a comprehensive description of the hyperparameter tuning process, which could be seen as a limitation in terms of reproducibility.\n",
      "\n",
      "In conclusion, the statistical methods used in this paper are generally robust and well-suited to the task of evaluating a new machine translation model. The use of standardized metrics, comparative analysis, and ablation studies strengthens the validity of their findings. However, more detailed reporting on hyperparameter tuning and potential limitations of the BLEU score could have further enhanced the statistical rigor of the study.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'Critically evaluate the statistical methods used in the first article. Are there any limitations or strengths worth noting?'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nse2_hLf9TCB",
    "outputId": "8b33252c-7e91-480c-e86d-94a799b22470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the findings from the three articles, I propose a new research direction that integrates multimodal learning, attention mechanisms, and advanced reasoning capabilities in large language models. This proposal aims to develop more versatile and intelligent AI systems capable of handling complex tasks across different modalities while maintaining interpretability and efficiency. Here's the justification based on the evidence provided:\n",
      "\n",
      "1. Multimodal Integration:\n",
      "The \"Visual Instruction Tuning\" paper (Multimodal.pdf) demonstrates the potential of combining visual and language models to create more capable AI systems. They showed that instruction tuning using multimodal data can improve zero-shot capabilities on new tasks. Building on this, we can explore ways to seamlessly integrate multiple modalities (text, image, audio, video) into a single model architecture.\n",
      "\n",
      "2. Advanced Attention Mechanisms:\n",
      "The \"Attention Is All You Need\" paper (attention.pdf) introduced the transformer architecture, which has become the foundation for many state-of-the-art language models. The authors noted that self-attention mechanisms could yield more interpretable models, with individual attention heads learning to perform different tasks related to syntactic and semantic structure. We can further investigate and enhance these attention mechanisms to improve both performance and interpretability across multiple modalities.\n",
      "\n",
      "3. Reasoning Capabilities:\n",
      "The \"Performance Evaluation\" paper discusses various reasoning tasks for large language models, including logical reasoning, inductive, deductive, and abductive reasoning. Integrating these advanced reasoning capabilities into multimodal models could lead to AI systems that can not only process different types of input but also perform complex reasoning tasks across modalities.\n",
      "\n",
      "Proposed Research Direction:\n",
      "Develop a \"Multimodal Reasoning Transformer\" (MRT) that combines:\n",
      "\n",
      "a) The multimodal integration techniques from visual instruction tuning\n",
      "b) Enhanced attention mechanisms that work across different modalities\n",
      "c) Explicit training for various reasoning tasks (logical, inductive, deductive, abductive)\n",
      "\n",
      "This MRT would aim to:\n",
      "\n",
      "1. Process inputs from multiple modalities simultaneously (text, image, audio, video)\n",
      "2. Utilize cross-modal attention mechanisms to identify relevant information across different input types\n",
      "3. Perform complex reasoning tasks that require integrating information from multiple modalities\n",
      "4. Maintain interpretability by allowing inspection of attention patterns and reasoning steps\n",
      "5. Generalize to new tasks and domains through instruction tuning and few-shot learning\n",
      "\n",
      "Justification:\n",
      "1. The success of visual instruction tuning (Multimodal.pdf) suggests that this approach can be extended to other modalities and more complex tasks.\n",
      "2. The interpretability of attention mechanisms (attention.pdf) provides a foundation for understanding how the model processes and integrates information from different modalities.\n",
      "3. The evaluation of reasoning capabilities in large language models (Performance Evaluation.pdf) offers a framework for incorporating and assessing advanced reasoning in multimodal contexts.\n",
      "\n",
      "By combining these elements, we can push the boundaries of AI capabilities, creating systems that can understand, reason about, and generate content across multiple modalities in a more human-like manner. This research direction has the potential to lead to more versatile and intelligent AI assistants, improved decision-making systems, and advanced tools for creative and analytical tasks that span different types of data and reasoning processes.\n"
     ]
    }
   ],
   "source": [
    "user_query = 'Integrate the findings from the three articles to propose a new research direction or hypothesis. Justify your proposal based on the evidence provided in the articles.'\n",
    "response = chat_main(user_query)\n",
    "print(response[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
